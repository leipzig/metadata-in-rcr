% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@INCOLLECTION{Leisch2002-oo,
  title     = "Sweave: Dynamic Generation of Statistical Reports Using Literate
               Data Analysis",
  booktitle = "Compstat",
  author    = "Leisch, Friedrich",
  editor    = "H{\"a}rdle, Professor Dr Wolfgang and R{\"o}nz, Professor Dr
               Bernd",
  publisher = "Physica-Verlag HD",
  pages     = "575--580",
  year      =  2002,
  url       = "http://link.springer.com/chapter/10.1007/978-3-642-57489-4_89",
  language  = "en",
  isbn      = "9783790815177, 9783642574894",
  doi       = "10.1007/978-3-642-57489-4\_89"
}

@INPROCEEDINGS{Gehani2016-eq,
  title     = "Scaling {SPADE} to`` Big Provenance''",
  booktitle = "{TaPP}",
  author    = "Gehani, Ashish and Kazmi, Hasanat and Irshad, Hassaan",
  year      =  2016,
  url       = "https://www.usenix.org/conference/tapp16/workshop-program/presentation/gehani"
}

@INPROCEEDINGS{Esteves2015-na,
  title     = "{MEX} vocabulary: a lightweight interchange format for machine
               learning experiments",
  booktitle = "Proceedings of the 11th International Conference on Semantic
               Systems",
  author    = "Esteves, Diego and Moussallem, Diego and Neto, Ciro Baron and
               Soru, Tommaso and Usbeck, Ricardo and Ackermann, Markus and
               Lehmann, Jens",
  publisher = "Association for Computing Machinery",
  pages     = "169--176",
  series    = "SEMANTICS '15",
  month     =  sep,
  year      =  2015,
  url       = "https://doi.org/10.1145/2814864.2814883",
  address   = "New York, NY, USA",
  keywords  = "data provenance, interchange format, machine learning
               experiments, vocabulary",
  location  = "Vienna, Austria",
  isbn      = "9781450334624",
  doi       = "10.1145/2814864.2814883"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cormier2020-sj,
  title     = "Go Get Data ({GGD)}: simple, reproducible access to scientific
               data",
  author    = "Cormier, M and Belyeu, J and Pedersen, B S and Brown, J and
               Koster, J and {others}",
  abstract  = "… Since these complexities waste time, inhibit reproducibility,
               and curtail research creativity, we developed Go Get Data (GGD;
               https:// gogetdata .github.io/) as a fast, reproducible approach
               to installing standardized data recipes …",
  journal   = "bioRxiv",
  publisher = "biorxiv.org",
  year      =  2020,
  url       = "https://www.biorxiv.org/content/10.1101/2020.09.10.291377v2.abstract"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Moreews2015-ip,
  title       = "{BioShaDock}: a community driven bioinformatics shared
                 Docker-based tools registry",
  author      = "Moreews, Fran{\c c}ois and Sallou, Olivier and M{\'e}nager,
                 Herv{\'e} and Le Bras, Yvan and Monjeaud, Cyril and Blanchet,
                 Christophe and Collin, Olivier",
  affiliation = "Genscale team, IRISA, Rennes, France. Genouest Bioinformatics
                 Facility, University of Rennes 1/IRISA, Rennes, France. Centre
                 d'Informatique pour la Biologie, C3BI, Institut Pasteur,
                 Paris, France. Genouest Bioinformatics Facility, University of
                 Rennes 1/IRISA, Rennes, France. Genouest Bioinformatics
                 Facility, University of Rennes 1/IRISA, Rennes, France.
                 Genouest Bioinformatics Facility, University of Rennes
                 1/IRISA, Rennes, France. French Institute of Bioinformatics,
                 CNRS IFB-Core, Gif-sur-Yvette, France.",
  abstract    = "Linux container technologies, as represented by Docker,
                 provide an alternative to complex and time-consuming
                 installation processes needed for scientiﬁc software. The ease
                 of deployment and the process isolation they enable, as well
                 as the reproducibility they permit across environments and
                 versions, are among the qualities that make them interesting
                 candidates for the construction of bioinformatic
                 infrastructures, at any scale from single workstations to high
                 throughput computing architectures. The Docker Hub is a public
                 registry which can be used to distribute bioinformatic
                 software as Docker images. However, its lack of curation and
                 its genericity make it difﬁcult for a bioinformatics user to
                 ﬁnd the most appropriate images needed. BioShaDock is a
                 bioinformatics-focused Docker registry, which provides a local
                 and fully controlled environment to build and publish
                 bioinformatic software as portable Docker images. It provides
                 a number of improvements over the base Docker registry on
                 authentication and permissions management, that enable its
                 integration in existing bioinformatic infrastructures such as
                 computing platforms. The metadata associated with the
                 registered images are domain-centric, including for instance
                 concepts deﬁned in the EDAM ontology, a shared and structured
                 vocabulary of commonly used terms in bioinformatics. The
                 registry also includes user deﬁned tags to facilitate its
                 discovery, as well as a link to the tool description in the
                 ELIXIR registry if it already exists. If it does not, the
                 BioShaDock registry will synchronize with the registry to
                 create a new description in the Elixir registry, based on the
                 BioShaDock entry metadata. This link will help users get more
                 information on the tool such as its EDAM operations, input and
                 output types. This allows integration with the ELIXIR Tools
                 and Data Services Registry, thus providing the appropriate
                 visibility of such images to the bioinformatics community.",
  journal     = "F1000Research",
  volume      =  4,
  pages       = "1443",
  month       =  dec,
  year        =  2015,
  url         = "http://dx.doi.org/10.12688/f1000research.7536.1",
  keywords    = "bioinformatics; community driven registry; container;
                 deployment; docker; interoperability; maintainability",
  language    = "en",
  issn        = "2046-1402",
  pmid        = "26913191",
  doi         = "10.12688/f1000research.7536.1",
  pmc         = "PMC4743153"
}

@ARTICLE{Faith2008-xf,
  title       = "Many Microbe Microarrays Database: uniformly normalized
                 Affymetrix compendia with structured experimental metadata",
  author      = "Faith, Jeremiah J and Driscoll, Michael E and Fusaro, Vincent
                 A and Cosgrove, Elissa J and Hayete, Boris and Juhn, Frank S
                 and Schneider, Stephen J and Gardner, Timothy S",
  affiliation = "Program in Bioinformatics, Boston University, 24 Cummington
                 St. and Department of Biomedical Engineering, Boston
                 University, 44 Cummington St., Boston, Massachusetts, 02215,
                 USA.",
  abstract    = "Many Microbe Microarrays Database (M3D) is designed to
                 facilitate the analysis and visualization of expression data
                 in compendia compiled from multiple laboratories. M3D contains
                 over a thousand Affymetrix microarrays for Escherichia coli,
                 Saccharomyces cerevisiae and Shewanella oneidensis. The
                 expression data is uniformly normalized to make the data
                 generated by different laboratories and researchers more
                 comparable. To facilitate computational analyses, M3D provides
                 raw data (CEL file) and normalized data downloads of each
                 compendium. In addition, web-based construction, visualization
                 and download of custom datasets are provided to facilitate
                 efficient interrogation of the compendium for more focused
                 analyses. The experimental condition metadata in M3D is human
                 curated with each chemical and growth attribute stored as a
                 structured and computable set of experimental features with
                 consistent naming conventions and units. All versions of the
                 normalized compendia constructed for each species are
                 maintained and accessible in perpetuity to facilitate the
                 future interpretation and comparison of results published on
                 M3D data. M3D is accessible at http://m3d.bu.edu/.",
  journal     = "Nucleic acids research",
  volume      =  36,
  number      = "Database issue",
  pages       = "D866--70",
  month       =  jan,
  year        =  2008,
  url         = "http://dx.doi.org/10.1093/nar/gkm815",
  language    = "en",
  issn        = "0305-1048, 1362-4962",
  pmid        = "17932051",
  doi         = "10.1093/nar/gkm815",
  pmc         = "PMC2238822"
}

@MISC{Kunze2018-pn,
  title  = "The {BagIt} File Packaging Format (V1.0)",
  author = "Kunze, J and Littman, J and Madden, E and Scancella, J and Adams, C",
  year   =  2018,
  url    = "http://dx.doi.org/10.17487/rfc8493",
  doi    = "10.17487/rfc8493"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Pezoa2016-tb,
  title     = "Foundations of {JSON} Schema",
  booktitle = "Proceedings of the 25th International Conference on World Wide
               Web",
  author    = "Pezoa, Felipe and Reutter, Juan L and Suarez, Fernando and
               Ugarte, Mart{\'\i}n and Vrgo{\v c}, Domagoj",
  abstract  = "… This example illustrates the simplicity and readabil- ity of
               JSON , which partially explains its fast adoption. With the
               popularity of JSON it was soon noted that in many scenarios one
               can benefit from a declarative way of specifying a schema for
               JSON documents …",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "263--273",
  series    = "WWW '16",
  month     =  apr,
  year      =  2016,
  url       = "https://doi.org/10.1145/2872427.2883029",
  address   = "Republic and Canton of Geneva, CHE",
  keywords  = "expressiveness of schema languages, JSON, JSON validation, JSON
               schema",
  location  = "Montr{\'e}al, Qu{\'e}bec, Canada",
  isbn      = "9781450341431",
  doi       = "10.1145/2872427.2883029"
}

@INPROCEEDINGS{Page2012-rw,
  title     = "From workflows to Research Objects: an architecture for
               preserving the semantics of science",
  booktitle = "Proceedings of the 2nd International Workshop on Linked Science",
  author    = "Page, Kevin and Palma, Ra{\'u}l and Holubowicz, Piotr and Klyne,
               Graham and Soiland-Reyes, Stian and Cruickshank, Don and Cabero,
               Rafael Gonz{\'a}lez and Cuesta, Esteban Garc{\'\i}a and De
               Roure, David and Zhao, Jun",
  abstract  = "Abstract Research Objects (ROs) provide a flexible model to
               collate and describe the semantic context of science. In this
               position paper we describe how ROs can also provide a foundation
               for interoperability within RESTful architecture design,
               enabling the development",
  publisher = "pdfs.semanticscholar.org",
  year      =  2012,
  url       = "https://pdfs.semanticscholar.org/a1c9/e4c4a7c6a552075c08fa6efd26d33c664096.pdf",
  keywords  = "pipelines \& provenance"
}

@ARTICLE{Fedorov2012-ri,
  title       = "{3D} Slicer as an image computing platform for the
                 Quantitative Imaging Network",
  author      = "Fedorov, Andriy and Beichel, Reinhard and Kalpathy-Cramer,
                 Jayashree and Finet, Julien and Fillion-Robin, Jean-Christophe
                 and Pujol, Sonia and Bauer, Christian and Jennings, Dominique
                 and Fennessy, Fiona and Sonka, Milan and Buatti, John and
                 Aylward, Stephen and Miller, James V and Pieper, Steve and
                 Kikinis, Ron",
  affiliation = "Brigham and Women's Hospital, Harvard Medical School, Boston,
                 MA 02115, USA. fedorov@bwh.harvard.edu",
  abstract    = "Quantitative analysis has tremendous but mostly unrealized
                 potential in healthcare to support objective and accurate
                 interpretation of the clinical imaging. In 2008, the National
                 Cancer Institute began building the Quantitative Imaging
                 Network (QIN) initiative with the goal of advancing
                 quantitative imaging in the context of personalized therapy
                 and evaluation of treatment response. Computerized analysis is
                 an important component contributing to reproducibility and
                 efficiency of the quantitative imaging techniques. The success
                 of quantitative imaging is contingent on robust analysis
                 methods and software tools to bring these methods from bench
                 to bedside. 3D Slicer is a free open-source software
                 application for medical image computing. As a clinical
                 research tool, 3D Slicer is similar to a radiology workstation
                 that supports versatile visualizations but also provides
                 advanced functionality such as automated segmentation and
                 registration for a variety of application domains. Unlike a
                 typical radiology workstation, 3D Slicer is free and is not
                 tied to specific hardware. As a programming platform, 3D
                 Slicer facilitates translation and evaluation of the new
                 quantitative methods by allowing the biomedical researcher to
                 focus on the implementation of the algorithm and providing
                 abstractions for the common tasks of data communication,
                 visualization and user interface development. Compared to
                 other tools that provide aspects of this functionality, 3D
                 Slicer is fully open source and can be readily extended and
                 redistributed. In addition, 3D Slicer is designed to
                 facilitate the development of new functionality in the form of
                 3D Slicer extensions. In this paper, we present an overview of
                 3D Slicer as a platform for prototyping, development and
                 evaluation of image analysis tools for clinical research
                 applications. To illustrate the utility of the platform in the
                 scope of QIN, we discuss several use cases of 3D Slicer by the
                 existing QIN teams, and we elaborate on the future directions
                 that can further facilitate development and validation of
                 imaging biomarkers using 3D Slicer.",
  journal     = "Magnetic resonance imaging",
  volume      =  30,
  number      =  9,
  pages       = "1323--1341",
  month       =  nov,
  year        =  2012,
  url         = "http://dx.doi.org/10.1016/j.mri.2012.05.001",
  language    = "en",
  issn        = "0730-725X, 1873-5894",
  pmid        = "22770690",
  doi         = "10.1016/j.mri.2012.05.001",
  pmc         = "PMC3466397"
}

@ARTICLE{Demir2010-qs,
  title    = "{The {BioPAX} community standard for pathway data sharing}",
  author   = "Demir, E and Cary, M P and Paley, S and Fukuda, K and Lemer, C
              and Vastrik, I and {Others}",
  journal  = "Nature biotechnology",
  volume   =  28,
  pages    = "1308",
  year     =  2010,
  keywords = "charlie\_rcr.bib",
  issn     = "1087-0156"
}

@ARTICLE{Lenhardt2014-sl,
  title     = "Data management lifecycle and software lifecycle management in
               the context of conducting science",
  author    = "Lenhardt, W and Ahalt, Stanley and Blanton, Brian and
               Christopherson, Laura and Idaszak, Ray",
  journal   = "Journal of Open Research Software",
  publisher = "Ubiquity Press",
  volume    =  2,
  number    =  1,
  year      =  2014,
  url       = "https://openresearchsoftware.metajnl.com/articles/10.5334/jors.ax/?toggle_hypothesis=on"
}

@ARTICLE{Santana-Perez2017-by,
  title    = "Reproducibility of execution environments in computational
              science using Semantics and Clouds",
  author   = "Santana-Perez, Idafen and Ferreira da Silva, Rafael and Rynge,
              Mats and Deelman, Ewa and P{\'e}rez-Hern{\'a}ndez, Mar{\'\i}a S
              and Corcho, Oscar",
  abstract = "Abstract In the past decades, one of the most common forms of
              addressing reproducibility in scientific workflow-based
              computational science has consisted of tracking the provenance of
              the produced and published results. Such provenance allows
              inspecting intermediate and final results, improves
              understanding, and permits replaying a workflow execution.
              Nevertheless, this approach does not provide any means for
              capturing and sharing the very valuable knowledge about the
              experimental equipment of a computational experiment, i.e., the
              execution environment in which the experiments are conducted. In
              this work, we propose a novel approach based on semantic
              vocabularies that describes the execution environment of
              scientific workflows, so as to conserve it. We define a process
              for documenting the workflow application and its related
              management system, as well as their dependencies. Then we apply
              this approach over three different real workflow applications
              running in three distinct scenarios, using public, private, and
              local Cloud platforms. In particular, we study one astronomy
              workflow and two life science workflows for genomic information
              analysis. Experimental results show that our approach can
              reproduce an equivalent execution environment of a predefined
              virtual machine image on all evaluated computing platforms.",
  journal  = "Future generations computer systems: FGCS",
  volume   =  67,
  pages    = "354--367",
  year     =  2017,
  url      = "http://www.sciencedirect.com/science/article/pii/S0167739X16000029",
  keywords = "Semantic metadata; Scientific workflow; Reproducibility; Life
              sciences;APIs \& Semantic ontologies",
  issn     = "0167-739X"
}

@ARTICLE{Aranguren2015-cr,
  title    = "Enhanced reproducibility of {SADI} web service workflows with
              Galaxy and Docker",
  author   = "Aranguren, Mikel Ega{\~n}a and Wilkinson, Mark D",
  abstract = "Semantic Web technologies have been widely applied in the life
              sciences, for example by data providers such as OpenLifeData and
              through web services frameworks such as SADI. The recently
              reported OpenLifeData2SADI project offers access to the vast
              OpenLifeData data store through SADI services.",
  journal  = "GigaScience",
  volume   =  4,
  number   =  1,
  pages    = "59",
  year     =  2015,
  url      = "http://dx.doi.org/10.1186/s13742-015-0092-3",
  issn     = "2047-217X",
  doi      = "10.1186/s13742-015-0092-3"
}

@ARTICLE{Cheney2012-aw,
  title    = "Prov-o: The prov ontology",
  author   = "Cheney, J and Corsar, D and Garijo, D and Soiland-Reyes, S and
              {others}",
  journal  = "W3C Working Draft",
  year     =  2012,
  keywords = "APIs \& Semantic ontologies"
}

@ARTICLE{Madan2016-ql,
  title    = "{The {BEL} information extraction workflow ({BELIEF)}: evaluation
              in the {BioCreative} {V} {BEL} and {IAT} track.}",
  author   = "Madan, Sumit and Hodapp, Sven and Senger, Philipp and Ansari, Sam
              and Szostak, Justyna and Hoeng, Julia and Peitsch, Manuel and
              Fluck, Juliane",
  abstract = "Network-based approaches have become extremely important in
              systems biology to achieve a better understanding of biological
              mechanisms. For network representation, the Biological Expression
              Language (BEL) is well designed to collate findings from the
              scientific literature into biological network models. To
              facilitate encoding and biocuration of such findings in BEL, a
              BEL Information Extraction Workflow (BELIEF) was developed.
              BELIEF provides a web-based curation interface, the BELIEF
              Dashboard, that incorporates text mining techniques to support
              the biocurator in the generation of BEL networks. The underlying
              UIMA-based text mining pipeline (BELIEF Pipeline) uses several
              named entity recognition processes and relationship extraction
              methods to detect concepts and BEL relationships in literature.
              The BELIEF Dashboard allows easy curation of the automatically
              generated BEL statements and their context annotations. Resulting
              BEL statements and their context annotations can be syntactically
              and semantically verified to ensure consistency in the BEL
              network. In summary, the workflow supports experts in different
              stages of systems biology network building. Based on the
              BioCreative V BEL track evaluation, we show that the BELIEF
              Pipeline automatically extracts relationships with an F-score of
              36.4\% and fully correct statements can be obtained with an
              F-score of 30.8\%. Participation in the BioCreative V Interactive
              task (IAT) track with BELIEF revealed a systems usability scale
              (SUS) of 67. Considering the complexity of the task for new
              users-learning BEL, working with a completely new interface, and
              performing complex curation-a score so close to the overall SUS
              average highlights the usability of BELIEF.Database URL: BELIEF
              is available at http://www.scaiview.com/belief/.",
  journal  = "Database: the journal of biological databases and curation",
  volume   =  2016,
  number   = "September 2017",
  pages    = "1--17",
  year     =  2016,
  url      = "http://dx.doi.org/10.1093/database/baw136",
  keywords = "charlie\_rcr.bib",
  issn     = "1758-0463",
  pmid     = "27694210",
  doi      = "10.1093/database/baw136"
}

@MISC{Nust2021-xm,
  title     = "Code Execution in Peer Review",
  author    = "N{\"u}st, Daniel",
  abstract  = "Research idea: survey the practices of code execution during
               peer review across all disciplines, journals, and conferences.
               More information in https://osf.io/zasmf/",
  publisher = "Open Science Framework",
  month     =  jun,
  year      =  2021,
  url       = "https://osf.io/gur8m/",
  doi       = "10.17605/OSF.IO/GUR8M"
}

@ARTICLE{Perignon2019-xw,
  title    = "Certify reproducibility with confidential data",
  author   = "P{\'e}rignon, Christophe and Gadouche, Kamel and Hurlin,
              Christophe and Silberman, Roxane and Debonnel, Eric",
  journal  = "Science",
  volume   =  365,
  number   =  6449,
  pages    = "127--128",
  month    =  jul,
  year     =  2019,
  url      = "http://dx.doi.org/10.1126/science.aaw2825",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "31296759",
  doi      = "10.1126/science.aaw2825"
}

@ARTICLE{Bedo2019-zr,
  title       = "{BioShake}: a Haskell {EDSL} for bioinformatics workflows",
  author      = "Bed{\H o}, Justin",
  affiliation = "Bioinformatics Division, The Walter and Eliza Hall Institute,
                 Parkville, VIC, Australia. Department of Computing and
                 Information Systems, The University of Melbourne, Parkville,
                 VIC, Australia.",
  abstract    = "Typical bioinformatics analyses comprise of long running
                 computational workflows. An important part of reproducible
                 research is the management and execution of these workflows to
                 allow robust execution and to minimise errors. BioShake is an
                 embedded domain specific language in Haskell for specifying
                 and executing computational workflows for bioinformatics that
                 significantly reduces the possibility of errors occurring.
                 Unlike other workflow frameworks, BioShake raises many
                 properties to the type level allowing the correctness of a
                 workflow to be statically checked during compilation, catching
                 errors before any lengthy execution process. BioShake builds
                 on the Shake build tool to provide robust dependency tracking,
                 parallel execution, reporting, and resumption capabilities.
                 Finally, BioShake abstracts execution so that jobs can either
                 be executed directly or submitted to a cluster. BioShake is
                 available at http://github.com/PapenfussLab/bioshake.",
  journal     = "PeerJ",
  volume      =  7,
  pages       = "e7223",
  month       =  jul,
  year        =  2019,
  url         = "http://dx.doi.org/10.7717/peerj.7223",
  keywords    = "Bioinformatics; EDSL; Haskell; Reproducibility; Workflow",
  language    = "en",
  issn        = "2167-8359",
  pmid        = "31328031",
  doi         = "10.7717/peerj.7223",
  pmc         = "PMC6625497"
}

@INPROCEEDINGS{Pimentel2019-ok,
  title     = "A {Large-Scale} Study About Quality and Reproducibility of
               Jupyter Notebooks",
  booktitle = "2019 {IEEE/ACM} 16th International Conference on Mining Software
               Repositories ({MSR})",
  author    = "Pimentel, J F and Murta, L and Braganholo, V and Freire, J",
  abstract  = "Jupyter Notebooks have been widely adopted by many different
               communities, both in science and industry. They support the
               creation of literate programming documents that combine code,
               text, and execution results with visualizations and all sorts of
               rich media. The self-documenting aspects and the ability to
               reproduce results have been touted as significant benefits of
               notebooks. At the same time, there has been growing criticism
               that the way notebooks are being used leads to unexpected
               behavior, encourage poor coding practices, and that their
               results can be hard to reproduce. To understand good and bad
               practices used in the development of real notebooks, we studied
               1.4 million notebooks from GitHub. We present a detailed
               analysis of their characteristics that impact reproducibility.
               We also propose a set of best practices that can improve the
               rate of reproducibility and discuss open challenges that require
               further research and development.",
  pages     = "507--517",
  month     =  may,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/MSR.2019.00077",
  keywords  = "data analysis;data visualisation;document
               handling;Internet;notebook computers;public domain
               software;software quality;self-documenting
               aspects;visualizations;GitHub;data analysis;Jupyter notebook
               quality;interactive literate programming document;jupyter
               notebook;github;reproducibility",
  issn      = "2574-3864",
  doi       = "10.1109/MSR.2019.00077"
}

@UNPUBLISHED{Obels2019-sy,
  title    = "Analysis of Open Data and Computational Reproducibility in
              Registered Reports in Psychology",
  author   = "Obels, Pepijn and Lakens, Daniel and Coles, Nicholas A and
              Gottfried, Jaroslav and Green, Seth A",
  abstract = "Ongoing technological developments have made it easier than ever
              before for scientists to share their data, materials, and
              analysis code. Sharing data and analysis code makes it easier for
              other researchers to re-use or check published research. However,
              these benefits will only emerge if researchers can reproduce the
              analysis reported in published articles and if data is annotated
              well enough so that it is clear what all variables mean. Because
              most researchers are not trained in computational
              reproducibility, it is important to evaluate current practices to
              identify practices that can be improved. We examined data and
              code sharing for Registered Reports published in the
              psychological literature between 2014 and 2018, and attempted to
              independently computationally reproduce the main results in each
              article. Of the main results from 62 articles that met our
              inclusion criteria, data were available for 41 articles, and
              analysis scripts for 37 articles. For the main results in 36
              articles that shared both data and code we could run the scripts
              for 31 analyses, and reproduce the main results for 21 articles.
              Although the articles that shared both data and code (36 out of
              62, or 58\%) and articles for which main results could be
              computationally reproduced (21 out of 36, or 58\%) was relatively
              high compared to other studies, there is clear room for
              improvement. We provide practical recommendations based on our
              observations and link to examples of good research practices in
              the papers we reproduced.",
  month    =  may,
  year     =  2019,
  url      = "psyarxiv.com/fk8vh",
  keywords = "data; open data; Psychology; reproducibility; RR",
  doi      = "10.31234/osf.io/fk8vh",
  note     = {PsyArXiv Preprint}
}

@ARTICLE{Ciccarese2013-lm,
  title       = "{PAV} ontology: provenance, authoring and versioning",
  author      = "Ciccarese, Paolo and Soiland-Reyes, Stian and Belhajjame,
                 Khalid and Gray, Alasdair Jg and Goble, Carole and Clark, Tim",
  affiliation = "Department of Neurology, Massachusetts General Hospital, 55
                 Fruit Street, Boston, MA 02114, USA.
                 paolo.ciccarese@gmail.com.",
  abstract    = "BACKGROUND: Provenance is a critical ingredient for
                 establishing trust of published scientific content. This is
                 true whether we are considering a data set, a computational
                 workflow, a peer-reviewed publication or a simple scientific
                 claim with supportive evidence. Existing vocabularies such as
                 Dublin Core Terms (DC Terms) and the W3C Provenance Ontology
                 (PROV-O) are domain-independent and general-purpose and they
                 allow and encourage for extensions to cover more specific
                 needs. In particular, to track authoring and versioning
                 information of web resources, PROV-O provides a basic
                 methodology but not any specific classes and properties for
                 identifying or distinguishing between the various roles
                 assumed by agents manipulating digital artifacts, such as
                 author, contributor and curator. RESULTS: We present the
                 Provenance, Authoring and Versioning ontology (PAV, namespace
                 http://purl.org/pav/): a lightweight ontology for capturing
                 ``just enough'' descriptions essential for tracking the
                 provenance, authoring and versioning of web resources. We
                 argue that such descriptions are essential for digital
                 scientific content. PAV distinguishes between contributors,
                 authors and curators of content and creators of
                 representations in addition to the provenance of originating
                 resources that have been accessed, transformed and consumed.
                 We explore five projects (and communities) that have adopted
                 PAV illustrating their usage through concrete examples.
                 Moreover, we present mappings that show how PAV extends the
                 W3C PROV-O ontology to support broader interoperability.
                 METHOD: The initial design of the PAV ontology was driven by
                 requirements from the AlzSWAN project with further
                 requirements incorporated later from other projects detailed
                 in this paper. The authors strived to keep PAV lightweight and
                 compact by including only those terms that have demonstrated
                 to be pragmatically useful in existing applications, and by
                 recommending terms from existing ontologies when plausible.
                 DISCUSSION: We analyze and compare PAV with related
                 approaches, namely Provenance Vocabulary (PRV), DC Terms and
                 BIBFRAME. We identify similarities and analyze differences
                 between those vocabularies and PAV, outlining strengths and
                 weaknesses of our proposed model. We specify SKOS mappings
                 that align PAV with DC Terms. We conclude the paper with
                 general remarks on the applicability of PAV.",
  journal     = "Journal of biomedical semantics",
  publisher   = "jbiomedsem.biomedcentral.com",
  volume      =  4,
  number      =  1,
  pages       = "37",
  month       =  nov,
  year        =  2013,
  url         = "http://dx.doi.org/10.1186/2041-1480-4-37",
  keywords    = "printed",
  language    = "en",
  issn        = "2041-1480",
  pmid        = "24267948",
  doi         = "10.1186/2041-1480-4-37",
  pmc         = "PMC4177195"
}

@ARTICLE{noauthor_2020-zo,
  title     = "Welcome to a new {ERA} of reproducible publishing",
  author    = {Emmy Tsang and Giuliano Maciocci},
  abstract  = "New open-source technology lets eLife authors publish Executable
               Research Articles that treat live code and data as first-class
               citizens.",
  journal   = "elifesciences.org",
  publisher = "eLife Sciences Publications Limited",
  month     =  aug,
  year      =  2020,
  url       = "https://elifesciences.org/labs/dc5acbde/welcome-to-a-new-era-of-reproducible-publishing",
  language  = "en"
}

@ARTICLE{Baggerly2010-qy,
  title    = "Disclose all data in publications",
  author   = "Baggerly, Keith",
  journal  = "Nature",
  volume   =  467,
  number   =  7314,
  pages    = "401",
  month    =  sep,
  year     =  2010,
  url      = "http://dx.doi.org/10.1038/467401b",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "20864982",
  doi      = "10.1038/467401b"
}

@ARTICLE{Peter2016-xn,
  title    = "Common Workflow Language, v1.0",
  author   = "Peter, Amstutz and Michael R., Crusoe and Neboj{\v s}a,
              Tijani{\'c} and Brad, Chapman and John, Chilton and Michael,
              Heuer and Andrey, Kartashov and Dan, Leehr and Herv{\'e},
              M{\'e}nager and Maya, Nedeljkovich and Matt, Scales and Stian,
              Soiland-Reyes and Luka, Stojanovic",
  abstract = "The Common Workflow Language (CWL) is an informal, multi-vendor
              working group consisting of various organizations and individuals
              that have an interest in portability of data analysis workflows.
              Our goal is to create specifications that enable data scientists
              to describe analysis tools and workflows that are powerful, easy
              to use, portable, and support reproducibility.CWL builds on
              technologies such as JSON-LD and Avro for data modeling and
              Docker for portable runtime environments. CWL is designed to
              express workflows for data-intensive science, such as
              Bioinformatics, Medical Imaging, Chemistry, Physics, and
              Astronomy.This is v1.0 of the CWL tool and workflow
              specification, released on 2016-07-08.The specification, in HTML
              format, is in the draft-3/docs folder.",
  journal  = "figshare",
  month    =  jul,
  year     =  2016,
  url      = "https://figshare.com/articles/Common_Workflow_Language_draft_3/3115156/2",
  doi      = "10.6084/m9.figshare.3115156.v2"
}

@ARTICLE{Collberg2014-cj,
  title   = "Measuring reproducibility in computer systems research",
  author  = "Collberg, Christian and Proebsting, Todd and Moraila, Gina and
             Shankaran, Akash and Shi, Zuoming and Warren, Alex M",
  journal = "Department of Computer Science, University of Arizona, Tech. Rep",
  year    =  2014,
  url     = "http://reproducibility.cs.arizona.edu/tr.pdf"
}

@ARTICLE{Alterovitz2018-gj,
  title       = "Enabling precision medicine via standard communication of
                 {HTS} provenance, analysis, and results",
  author      = "Alterovitz, Gil and Dean, Dennis and Goble, Carole and Crusoe,
                 Michael R and Soiland-Reyes, Stian and Bell, Amanda and Hayes,
                 Anais and Suresh, Anita and Purkayastha, Anjan and King,
                 Charles H and Taylor, Dan and Johanson, Elaine and Thompson,
                 Elaine E and Donaldson, Eric and Morizono, Hiroki and Tsang,
                 Hsinyi and Vora, Jeet K and Goecks, Jeremy and Yao, Jianchao
                 and Almeida, Jonas S and Keeney, Jonathon and Addepalli,
                 Kanakadurga and Krampis, Konstantinos and Smith, Krista M and
                 Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo,
                 Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel
                 and Gottipati, Srikanth and Rodwell, Timothy C and Bloom, Toby
                 and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja",
  affiliation = "Harvard/MIT Division of Health Sciences and Technology,
                 Harvard Medical School, Boston, Massachusetts, United States
                 of America. Computational Health Informatics Program, Boston
                 Children's Hospital, Boston, Massachusetts, United States of
                 America. Electrical Engineering and Computer Science,
                 Massachusetts Institute of Technology, Boston, Massachusetts,
                 United States of America. Seven Bridges, Cambridge,
                 Massachusetts, United States of America. School of Computer
                 Science, The University of Manchester, Manchester, United
                 Kingdom. Common Workflow Language Project, Vilnius, Lithuania.
                 The Department of Biochemistry \& Molecular Medicine, The
                 George Washington University Medical Center, Washington, DC,
                 United States of America. Foundation for Innovative New
                 Diagnostics (FIND), Geneva, Switzerland. OpenBox Bio, Vienna,
                 Virgnia, United States of America. The McCormick Genomic and
                 Proteomic Center, The George Washington University,
                 Washington, DC, United States of America. Internet 2,
                 Washington, DC, United States of America. US Food and Drug
                 Administration, Silver Spring, Maryland, United States of
                 America. Center for Genetic Medicine, Children's National
                 Medical Center, Washington, DC, United States of America. The
                 Department of Genomics and Precision Medicine, The George
                 Washington University, School of Medicine and Health Sciences,
                 Washington, DC, United States of America. Center for
                 Biomedical Informatics and Information Technology, National
                 Cancer Institute, National Institutes of Health, Gaithersburg,
                 Maryland, United States of America. Attain, McClean, Virginia,
                 United States of America. Computational Biology Program,
                 Oregon Health \& Science University, Portland Oregon, United
                 States of America. MRL IT, Merck \& Co., Boston,
                 Massachusetts, United States of America. Stony Brook
                 University, School of Medicine and College of Engineering and
                 Applied Sciences, Stony Brook, New York, United States of
                 America. Department of Biological Sciences, Hunter College of
                 The City University of New York, New York, New York, United
                 States of America. Institute for Computational Biomedicine,
                 Weill Cornell Medical College, New York, New York, United
                 States of America. Wellesley College, Wellesley,
                 Massachusetts, United States of America. Critical Path
                 Institute, Tucson, Arizona, United States of America. DDL
                 Diagnostic Laboratory, Rijswijk, Netherlands. NSilico Life
                 Science, Nova Center, Belfield Innovation Park, University
                 College Dublin, Dublin Ireland. OTSUKA Pharmaceutical
                 Development \& Commercialization, Princeton, New Jersey,
                 United States of America. New York Genome Center, New York,
                 New York, United States of America.",
  abstract    = "A personalized approach based on a patient's or pathogen's
                 unique genomic sequence is the foundation of precision
                 medicine. Genomic findings must be robust and reproducible,
                 and experimental data capture should adhere to findable,
                 accessible, interoperable, and reusable (FAIR) guiding
                 principles. Moreover, effective precision medicine requires
                 standardized reporting that extends beyond wet-lab procedures
                 to computational methods. The BioCompute framework
                 (https://w3id.org/biocompute/1.3.0) enables standardized
                 reporting of genomic sequence data provenance, including
                 provenance domain, usability domain, execution domain,
                 verification kit, and error domain. This framework facilitates
                 communication and promotes interoperability. Bioinformatics
                 computation instances that employ the BioCompute framework are
                 easily relayed, repeated if needed, and compared by
                 scientists, regulators, test developers, and clinicians.
                 Easing the burden of performing the aforementioned tasks
                 greatly extends the range of practical application. Large
                 clinical trials, precision medicine, and regulatory
                 submissions require a set of agreed upon standards that
                 ensures efficient communication and documentation of genomic
                 analyses. The BioCompute paradigm and the resulting BioCompute
                 Objects (BCOs) offer that standard and are freely accessible
                 as a GitHub organization
                 (https://github.com/biocompute-objects) following the
                 ``Open-Stand.org principles for collaborative open standards
                 development.'' With high-throughput sequencing (HTS) studies
                 communicated using a BCO, regulatory agencies (e.g., Food and
                 Drug Administration [FDA]), diagnostic test developers,
                 researchers, and clinicians can expand collaboration to drive
                 innovation in precision medicine, potentially decreasing the
                 time and cost associated with next-generation sequencing
                 workflow exchange, reporting, and regulatory reviews.",
  journal     = "PLoS biology",
  volume      =  16,
  number      =  12,
  pages       = "e3000099",
  month       =  dec,
  year        =  2018,
  url         = "http://dx.doi.org/10.1371/journal.pbio.3000099",
  language    = "en",
  issn        = "1544-9173, 1545-7885",
  pmid        = "30596645",
  doi         = "10.1371/journal.pbio.3000099",
  pmc         = "PMC6338479"
}

@ARTICLE{Li2020-mw,
  title       = "Data objects and documenting scientific processes: An analysis
                 of data events in biodiversity data papers",
  author      = "Li, Kai and Greenberg, Jane and Dunic, Jillian",
  affiliation = "College of Computing and InformaticsDrexel University
                 Philadelphia PA; Department of Biological SciencesSimon Fraser
                 University Burnaby BC Canada",
  abstract    = "The data paper, an emerging scholarly genre, describes
                 research data sets and is intended to bridge the gap between
                 the publication of research data and scientific articles.
                 Research examining how data papers report data events, such as
                 data transactions and manipulations, is limited. The research
                 reported on in this article addresses this limitation and
                 investigated how data events are inscribed in data papers. A
                 content analysis was conducted examining the full texts of 82
                 data papers, drawn from the curated list of data papers
                 connected to the Global Biodiversity Information Facility.
                 Data events recorded for each paper were organized into a set
                 of 17 categories. Many of these categories are described
                 together in the same sentence, which indicates the messiness
                 of data events in the laboratory space. The findings challenge
                 the degrees to which data papers are a distinct genre compared
                 to research articles and they describe data-centric research
                 processes in a through way. This article also discusses how
                 our results could inform a better data publication ecosystem
                 in the future.",
  journal     = "Journal of the Association for Information Science and
                 Technology",
  publisher   = "Wiley",
  volume      =  71,
  number      =  2,
  pages       = "172--182",
  month       =  feb,
  year        =  2020,
  url         = "https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24226",
  copyright   = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language    = "en",
  issn        = "2330-1635, 2330-1643",
  doi         = "10.1002/asi.24226"
}

@INPROCEEDINGS{De_Roure2011-bk,
  title     = "Towards the preservation of scientific workflows",
  booktitle = "Procs. of the 8th International Conference on Preservation of
               Digital Objects ({iPRES} 2011). {ACM}",
  author    = "De Roure, David and Belhajjame, Khalid and Missier, Paolo and
               G{\'o}mez-P{\'e}rez, Jos{\'e} Manuel and Palma, Ra{\'u}l and
               Ruiz, Jos{\'e} Enrique and Hettne, Kristina and Roos, Marco and
               Klyne, Graham and Goble, Carole",
  abstract  = "ABSTRACT Some of the shared digital artefacts of digital
               research are executable in the sense that they describe an
               automated process which generates results. One example is the
               computational scientific workflow which is used to conduct
               automated data analysis,",
  publisher = "amiga.iaa.csic.es",
  year      =  2011,
  url       = "http://www.amiga.iaa.csic.es/FCKeditor/UserFiles/File/wfpreservev.pdf",
  keywords  = "printed"
}

@ARTICLE{Fabregat2018-fo,
  title    = "{The Reactome pathway knowledgebase}",
  author   = "Fabregat, A and Jupe, S and Matthews, L and Sidiropoulos, K and
              Gillespie, M and Garapati, P and {Others}",
  journal  = "Nucleic acids research",
  volume   =  46,
  pages    = "D649",
  year     =  2018,
  keywords = "charlie\_rcr.bib",
  issn     = "0305-1048"
}

@INPROCEEDINGS{Robinson2017-cx,
  title     = "{CWL} Viewer: The Common Workflow Language Viewer",
  booktitle = "Bioinformatics Open Source Conference ({BOSC}) 2017",
  author    = "Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R
               and Goble, Carole",
  year      =  2017,
  url       = "https://www.research.manchester.ac.uk/portal/files/57032407/BOSC_2017_paper_27.pdf"
}

@ARTICLE{Landau2021-sk,
  title     = "The targets {R} package: a dynamic Make-like function-oriented
               pipeline toolkit for reproducibility and high-performance
               computing",
  author    = "Landau, William",
  journal   = "Journal of open source software",
  publisher = "The Open Journal",
  volume    =  6,
  number    =  57,
  pages     = "2959",
  month     =  jan,
  year      =  2021,
  url       = "https://joss.theoj.org/papers/10.21105/joss.02959",
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  issn      = "2475-9066",
  doi       = "10.21105/joss.02959"
}

@MISC{Angelino2010-cs,
  title   = "{StarFlow}: A {Script-Centric} Data Analysis Environment",
  author  = "Angelino, Elaine and Yamins, Daniel and Seltzer, Margo",
  journal = "Lecture Notes in Computer Science",
  pages   = "236--250",
  year    =  2010,
  url     = "http://dx.doi.org/10.1007/978-3-642-17819-1_27",
  doi     = "10.1007/978-3-642-17819-1\_27"
}

@INCOLLECTION{Richardson2016-fu,
  title     = "Geospatial Semantic Web",
  author    = {Krzysztof Janowicz and Pascal Hitzler},
  booktitle = "International Encyclopedia of Geography: People, the Earth,
               Environment and Technology",
  editor    = "Richardson, Douglas and Castree, Noel and Goodchild, Michael F
               and Kobayashi, Audrey and Liu, Weidong and Marston, Richard A",
  abstract  = "The Semantic Web is a highly interdisciplinary research field
               that aims at fostering the publication, retrieval, reuse, and
               integration of data in a distributed, web-scale environment in
               which multithematic and multiperspective information has to be
               handled by humans and machines alike. This raises many
               interesting challenges that attract researchers from disciplines
               such as computer science, cognitive science, geographic
               information science, bioinformatics, digital humanities, and the
               life sciences. Over the years this led to a multitude of
               subcommunities that drive today's Semantic Web research. As one
               of these communities, the Geospatial Semantic Web is concerned
               with the semantics of geospatial data, next-generation
               cyberinfrastructures, formal vocabularies for the geodomain, and
               geographic information retrieval, as well as new methods that
               foster semantic interoperability without restricting
               heterogeneity.",
  publisher = "John Wiley \& Sons, Ltd",
  volume    =  284,
  pages     = "1--6",
  month     =  dec,
  year      =  2016,
  url       = "http://doi.wiley.com/10.1002/9781118786352.wbieg1158",
  address   = "Oxford, UK",
  isbn      = "9780470659632, 9781118786352",
  doi       = "10.1002/9781118786352.wbieg1158"
}

@BOOK{Qin2016-qe,
  title     = "Metadata",
  author    = "Qin, Jian and Zeng, Marcia",
  publisher = "ALA-Neal Schuman",
  year      =  2016,
  url       = "https://experts.syr.edu/en/publications/metadata"
}

@ARTICLE{Lehrer2010-pm,
  title   = "The truth wears off",
  author  = "Lehrer, Jonah",
  journal = "New Yorker",
  volume  =  13,
  number  =  52,
  pages   = "229",
  year    =  2010,
  url     = "http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off",
  issn    = "0028-792X"
}

@ARTICLE{Schulz2010-bp,
  title       = "{CONSORT} 2010 Statement: updated guidelines for reporting
                 parallel group randomised trials",
  author      = "Schulz, Kenneth F and Altman, Douglas G and Moher, David and
                 {CONSORT Group}",
  affiliation = "Family Health International, Research Triangle Park, NC 27709,
                 USA. kschulz@fhi.org",
  abstract    = "The CONSORT statement is used worldwide to improve the
                 reporting of randomised controlled trials. Kenneth Schulz and
                 colleagues describe the latest version, CONSORT 2010, which
                 updates the reporting guideline based on new methodological
                 evidence and accumulating experience.To encourage
                 dissemination of the CONSORT 2010 Statement, this article is
                 freely accessible on bmj.com and will also be published in the
                 Lancet, Obstetrics and Gynecology, PLoS Medicine, Annals of
                 Internal Medicine, Open Medicine, Journal of Clinical
                 Epidemiology, BMC Medicine, and Trials.",
  journal     = "BMC medicine",
  volume      =  8,
  pages       = "18",
  month       =  mar,
  year        =  2010,
  url         = "http://dx.doi.org/10.1186/1741-7015-8-18",
  language    = "en",
  issn        = "1741-7015",
  pmid        = "20334633",
  doi         = "10.1186/1741-7015-8-18",
  pmc         = "PMC2860339"
}

@ARTICLE{Alter2020-hy,
  title       = "The Data Tags Suite ({DATS}) model for discovering data access
                 and use requirements",
  author      = "Alter, George and Gonzalez-Beltran, Alejandra and
                 Ohno-Machado, Lucila and Rocca-Serra, Philippe",
  affiliation = "University of Michigan, ICPSR 330 Packard Street, Ann Arbor MI
                 48104, USA. Science and Technology Facilities Council,
                 Scientific Computing Department, Rutherford Appleton
                 Laboratory, Harwell Campus, Didcot, 0X11 0QX, United Kingdom.
                 University of California, San Diego, Division of Biomedical
                 Informatics, 9500 Gilman Dr. MC 0728, La Jolla CA 92093-0728,
                 USA. Oxford e-Research Centre University of Oxford 7 Keble
                 Road, Oxford, OX1 3QG United Kingdom.",
  abstract    = "BACKGROUND: Data reuse is often controlled to protect the
                 privacy of subjects and patients. Data discovery tools need
                 ways to inform researchers about restrictions on data access
                 and re-use. RESULTS: We present elements in the Data Tags
                 Suite (DATS) metadata schema describing data access, data use
                 conditions, and consent information. DATS metadata are
                 explained in terms of the administrative, legal, and technical
                 systems used to protect confidential data. CONCLUSIONS: The
                 access and use metadata items in DATS are designed from the
                 perspective of a researcher who wants to find and re-use
                 existing data. We call for standard ways of describing
                 informed consent and data use agreements that will enable
                 automated systems for managing research data.",
  journal     = "GigaScience",
  publisher   = "ora.ox.ac.uk",
  volume      =  9,
  number      =  2,
  month       =  feb,
  year        =  2020,
  url         = "http://dx.doi.org/10.1093/gigascience/giz165",
  keywords    = "confidential data; data access; data discovery; data use;
                 metadata",
  language    = "en",
  issn        = "2047-217X",
  pmid        = "32031623",
  doi         = "10.1093/gigascience/giz165",
  pmc         = "PMC7006671"
}

@ARTICLE{Kumar2021-aq,
  title       = "Tool recommender system in Galaxy using deep learning",
  author      = "Kumar, Anup and Rasche, Helena and Gr{\"u}ning, Bj{\"o}rn and
                 Backofen, Rolf",
  affiliation = "Bioinformatics Group, Department of Computer Science,
                 University of Freiburg, Georges-Koehler-Allee 106, 79110
                 Freiburg, Germany. Signalling Research Centres BIOSS and
                 CIBSS, University of Freiburg, Schaenzlestr. 18, 79104
                 Freiburg, Germany.",
  abstract    = "BACKGROUND: Galaxy is a web-based and open-source scientific
                 data-processing platform. Researchers compose pipelines in
                 Galaxy to analyse scientific data. These pipelines, also known
                 as workflows, can be complex and difficult to create from
                 thousands of tools, especially for researchers new to Galaxy.
                 To help researchers with creating workflows, a system is
                 developed to recommend tools that can facilitate further data
                 analysis. FINDINGS: A model is developed to recommend tools
                 using a deep learning approach by analysing workflows composed
                 by researchers on the European Galaxy server. The higher-order
                 dependencies in workflows, represented as directed acyclic
                 graphs, are learned by training a gated recurrent units neural
                 network, a variant of a recurrent neural network. In the
                 neural network training, the weights of tools used are derived
                 from their usage frequencies over time and the sequences of
                 tools are uniformly sampled from training data.
                 Hyperparameters of the neural network are optimized using
                 Bayesian optimization. Mean accuracy of 98\% in recommending
                 tools is achieved for the top-1 metric. CONCLUSIONS: The model
                 is accessed by a Galaxy API to provide researchers with
                 recommended tools in an interactive manner using multiple user
                 interface integrations on the European Galaxy server.
                 High-quality and highly used tools are shown at the top of the
                 recommendations. The scripts and data to create the
                 recommendation system are available under MIT license at
                 https://github.com/anuprulez/galaxy\_tool\_recommendation.",
  journal     = "GigaScience",
  volume      =  10,
  number      =  1,
  month       =  jan,
  year        =  2021,
  url         = "http://dx.doi.org/10.1093/gigascience/giaa152",
  keywords    = "Galaxy; deep learning; gated recurrent units; neural networks;
                 recommender system; workflows",
  language    = "en",
  issn        = "2047-217X",
  pmid        = "33404053",
  doi         = "10.1093/gigascience/giaa152",
  pmc         = "PMC7786169"
}

@INPROCEEDINGS{Di_Cosmo2017-ii,
  title     = "Software heritage: Why and how to preserve software source code",
  booktitle = "{iPRES} 2017-14th International Conference on Digital
               Preservation",
  author    = "Di Cosmo, Roberto and Zacchiroli, Stefano",
  pages     = "1--10",
  year      =  2017,
  url       = "https://hal.archives-ouvertes.fr/hal-01590958/"
}

@ARTICLE{Xie2014-gx,
  title   = "knitr: a comprehensive tool for reproducible research in {R}",
  author  = "Xie, Yihui",
  journal = "Implement Reprod Res",
  volume  =  1,
  pages   = "20",
  year    =  2014,
  url     = "https://books.google.com/books?hl=en&lr=&id=WVTSBQAAQBAJ&oi=fnd&pg=PA3&dq=metadata%2Breproducible%2Bresearch&ots=qRBy9aJfRW&sig=N0FdAsTOoQn1RhZVCWT96MiUYSM"
}

@ARTICLE{Belmann2015-tt,
  title       = "Bioboxes: standardised containers for interchangeable
                 bioinformatics software",
  author      = "Belmann, Peter and Dr{\"o}ge, Johannes and Bremges, Andreas
                 and McHardy, Alice C and Sczyrba, Alexander and Barton,
                 Michael D",
  affiliation = "Faculty of Technology and Center for Biotechnology, Bielefeld
                 University, 33615 Bielefeld, Germany. Computational Biology of
                 Infection Research, Helmholtz Centre for Infection Research,
                 38124 Braunschweig, Germany. Faculty of Technology and Center
                 for Biotechnology, Bielefeld University, 33615 Bielefeld,
                 Germany ; Computational Biology of Infection Research,
                 Helmholtz Centre for Infection Research, 38124 Braunschweig,
                 Germany. Computational Biology of Infection Research,
                 Helmholtz Centre for Infection Research, 38124 Braunschweig,
                 Germany. Faculty of Technology and Center for Biotechnology,
                 Bielefeld University, 33615 Bielefeld, Germany. DOE Joint
                 Genome Institute, Walnut Creek, CA 94598 USA.",
  abstract    = "Software is now both central and essential to modern biology,
                 yet lack of availability, difficult installations, and complex
                 user interfaces make software hard to obtain and use.
                 Containerisation, as exemplified by the Docker platform, has
                 the potential to solve the problems associated with sharing
                 software. We propose bioboxes: containers with standardised
                 interfaces to make bioinformatics software interchangeable.",
  journal     = "GigaScience",
  volume      =  4,
  pages       = "47",
  month       =  oct,
  year        =  2015,
  url         = "http://dx.doi.org/10.1186/s13742-015-0087-0",
  keywords    = "Bioinformatics; Docker; Reproducibility; Software; Standards;
                 Usability",
  language    = "en",
  issn        = "2047-217X",
  pmid        = "26473029",
  doi         = "10.1186/s13742-015-0087-0",
  pmc         = "PMC4607242"
}

@ARTICLE{Moreau_undated-lo,
  title   = "Open Provenance Model ({OPM}) {OWL} Specification, October 2010",
  author  = "Moreau, Luc and Ding, Li and Futrelle, Joe and Garijo, D and
             Groth, Paul and Jewell, Mike and Miles, Simon and Missier, Paolo
             and Pan, Jeff and Zhao, Jun",
  journal = "URL http://openprovenance.org/model/opmo. (Cited on page 139. )"
}

@ARTICLE{Janowicz2014-wz,
  title     = "Five stars of linked data vocabulary use",
  author    = "Janowicz, Krzysztof and Hitzler, Pascal and Adams, Benjamin and
               Kolas, Dave and Vardeman, I I and {Others}",
  abstract  = "Abstract In 2010 Tim Berners-Lee introduced a 5 star rating to
               his Linked Data design issues page to encourage data publishers
               along the road to good Linked Data. What makes the star rating
               so effective is its simplicity, clarity, and a pinch of
               psychology--is your data 5 star?",
  journal   = "Semantic Web",
  publisher = "IOS Press",
  volume    =  5,
  number    =  3,
  pages     = "173--176",
  year      =  2014,
  url       = "http://content.iospress.com/articles/semantic-web/sw135",
  keywords  = "open data open source big data FAIR"
}

@MISC{noauthor_undated-kc,
  title        = "{STATO}: an Ontology of Statistical Methods",
  url          = "http://stato-ontology.org/",
  howpublished = "\url{http://stato-ontology.org/}",
  note         = "Accessed: 2020-5-21"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Warden2018-ak,
  title        = "The Machine Learning Reproducibility Crisis",
  booktitle    = "Pete Warden's blog",
  author       = "Warden, Pete",
  abstract     = "Gosper Glider Gun I was recently chatting to a friend whose
                  startup's machine learning models were so disorganized it was
                  causing serious problems as his team tried to build on each
                  other's work and share it with clients. Even the original
                  author sometimes couldn't train the same model and get
                  similar results! He was hoping…",
  month        =  mar,
  year         =  2018,
  url          = "https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/",
  howpublished = "\url{https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/}",
  note         = "Accessed: 2018-3-19",
  keywords     = "prospectus I"
}

@ARTICLE{Tierney2020-ue,
  title         = "A Realistic Guide to Making Data Available Alongside Code to
                   Improve Reproducibility",
  author        = "Tierney, Nicholas J and Ram, Karthik",
  abstract      = "Data makes science possible. Sharing data improves
                   visibility, and makes the research process transparent. This
                   increases trust in the work, and allows for independent
                   reproduction of results. However, a large proportion of data
                   from published research is often only available to the
                   original authors. Despite the obvious benefits of sharing
                   data, and scientists' advocating for the importance of
                   sharing data, most advice on sharing data discusses its
                   broader benefits, rather than the practical considerations
                   of sharing. This paper provides practical, actionable advice
                   on how to actually share data alongside research. The key
                   message is sharing data falls on a continuum, and entering
                   it should come with minimal barriers.",
  month         =  feb,
  year          =  2020,
  url           = "http://arxiv.org/abs/2002.11626",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "2002.11626",
  primaryClass  = "cs.DL",
  arxivid       = "2002.11626"
}

@ARTICLE{Bernstein2016-bs,
  title     = "Specification of the Crystallographic Information File format,
               version 2.0",
  author    = "Bernstein, H J and Bollinger, J C and Brown, I D and Gra{\v
               z}ulis, S and Hester, J R and McMahon, B and Spadaccini, N and
               Westbrook, J D and Westrip, S P",
  abstract  = "Version 2.0 of the CIF format incorporates novel features
               implemented in STAR 2.0. Among these are an expanded character
               repertoire, new and more flexible forms for quoted data values,
               and new compound data types. The CIF 2.0 format is compared with
               both CIF 1.1 and STAR 2.0, and a formal syntax specification is
               provided.",
  journal   = "Journal of applied crystallography",
  publisher = "International Union of Crystallography",
  volume    =  49,
  number    =  1,
  pages     = "277--284",
  month     =  feb,
  year      =  2016,
  url       = "https://scripts.iucr.org/cgi-bin/paper?S1600576715021871",
  keywords  = "CIF; CIF 2.0",
  language  = "en",
  issn      = "0021-8898",
  doi       = "10.1107/S1600576715021871"
}

@MISC{noauthor_undated-pu,
  title        = "Defining metadata (meta.yaml) --- conda-build
                  3.19.3+29.gba6cf7ab.dirty documentation",
  url          = "https://docs.conda.io/projects/conda-build/en/latest/resources/define-metadata.html",
  howpublished = "\url{https://docs.conda.io/projects/conda-build/en/latest/resources/define-metadata.html}",
  note         = "Accessed: 2020-5-21"
}

@MISC{FitzJohn_undated-bq,
  title  = "Reproducible research is still a challenge. {rOpenSci}. 2014",
  author = "FitzJohn, R and Pennell, M and Zanne, A and Cornwell, W"
}

@ARTICLE{Simera2010-cd,
  title       = "Transparent and accurate reporting increases reliability,
                 utility, and impact of your research: reporting guidelines and
                 the {EQUATOR} Network",
  author      = "Simera, Iveta and Moher, David and Hirst, Allison and Hoey,
                 John and Schulz, Kenneth F and Altman, Douglas G",
  affiliation = "Centre for Statistics in Medicine, University of Oxford,
                 Oxford, UK. iveta.simera@csm.ox.ac.uk",
  abstract    = "Although current electronic methods of scientific publishing
                 offer increased opportunities for publishing all research
                 studies and describing them in sufficient detail, health
                 research literature still suffers from many shortcomings.
                 These shortcomings seriously undermine the value and utility
                 of the literature and waste scarce resources invested in the
                 research. In recent years there have been several positive
                 steps aimed at improving this situation, such as a
                 strengthening of journals' policies on research publication
                 and the wide requirement to register clinical trials.The
                 EQUATOR (Enhancing the QUAlity and Transparency Of health
                 Research) Network is an international initiative set up to
                 advance high quality reporting of health research studies; it
                 promotes good reporting practices including the wider
                 implementation of reporting guidelines. EQUATOR provides free
                 online resources http://www.equator-network.org supported by
                 education and training activities and assists in the
                 development of robust reporting guidelines. This paper
                 outlines EQUATOR's goals and activities and offers suggestions
                 for organizations and individuals involved in health research
                 on how to strengthen research reporting.",
  journal     = "BMC medicine",
  volume      =  8,
  pages       = "24",
  month       =  apr,
  year        =  2010,
  url         = "http://dx.doi.org/10.1186/1741-7015-8-24",
  language    = "en",
  issn        = "1741-7015",
  pmid        = "20420659",
  doi         = "10.1186/1741-7015-8-24",
  pmc         = "PMC2874506"
}

@ARTICLE{Courtot2019-zc,
  title       = "{BioSamples} database: an updated sample metadata hub",
  author      = "Courtot, M{\'e}lanie and Cherubin, Luca and Faulconbridge,
                 Adam and Vaughan, Daniel and Green, Matthew and Richardson,
                 David and Harrison, Peter and Whetzel, Patricia L and
                 Parkinson, Helen and Burdett, Tony",
  affiliation = "EMBL-EBI, Wellcome Genome Campus, Hinxton CB10 1SD, UK.",
  abstract    = "The BioSamples database at EMBL-EBI provides a central hub for
                 sample metadata storage and linkage to other EMBL-EBI
                 resources. BioSamples has recently undergone major changes,
                 both in terms of data content and supporting infrastructure.
                 The data content has more than doubled from around 2 million
                 samples in 2014 to just over 5 million samples in 2018. Fast,
                 reciprocal data exchange was fully established between sister
                 Biosample databases and other INSDC partners, enabling a
                 worldwide common representation and centralization of sample
                 metadata. The BioSamples platform has been upgraded to
                 accommodate anticipated increases in the number of submissions
                 via GA4GH driver projects such as the Human Cell Atlas and the
                 EGA, as well as from mirroring of NCBI dbGaP data. The
                 BioSamples database is now the authoritative repository for
                 all INSDC sample metadata, an ELIXIR Deposition Database for
                 Biomolecular Data and the EMBL-EBI sample metadata hub. To
                 support faster turnaround for sample submission, and to
                 increase scalability and resilience, we have upgraded the
                 BioSamples database backend storage, APIs and user interface.
                 Finally, the website has been redesigned to allow search and
                 retrieval of records based on specific filters, such as
                 'disease' or 'organism'. These changes are targeted at
                 answering current use cases as well as providing
                 functionalities for future emerging and anticipated
                 developments. Availability: The BioSamples database is freely
                 available at http://www.ebi.ac.uk/biosamples. Content is
                 distributed under the EMBL-EBI Terms of Use available at
                 https://www.ebi.ac.uk/about/terms-of-use.",
  journal     = "Nucleic acids research",
  volume      =  47,
  number      = "D1",
  pages       = "D1172--D1178",
  month       =  jan,
  year        =  2019,
  url         = "http://dx.doi.org/10.1093/nar/gky1061",
  language    = "en",
  issn        = "0305-1048, 1362-4962",
  pmid        = "30407529",
  doi         = "10.1093/nar/gky1061",
  pmc         = "PMC6323949"
}

@INPROCEEDINGS{Chirigati2016-aw,
  title     = "{ReproZip}: Computational Reproducibility With Ease",
  booktitle = "Proceedings of the 2016 International Conference on Management
               of Data",
  author    = "Chirigati, Fernando and Rampin, R{\'e}mi and Shasha, Dennis and
               Freire, Juliana",
  publisher = "ACM",
  pages     = "2085--2088",
  series    = "SIGMOD '16",
  year      =  2016,
  url       = "http://doi.acm.org/10.1145/2882903.2899401",
  address   = "New York, NY, USA",
  keywords  = "computational reproducibility, provenance, reprozip",
  location  = "San Francisco, California, USA",
  isbn      = "9781450335317",
  doi       = "10.1145/2882903.2899401"
}

@ARTICLE{Riley2017-uy,
  title   = "Understanding metadata",
  author  = "Riley, Jenn",
  journal = "Washington DC, United States: National Information Standards
             Organization (http://www. niso.
             org/publications/press/UnderstandingMetadata. pdf)",
  pages   = "23",
  year    =  2017,
  url     = "http://www.fidgeo.de/wp-content/uploads/2016/07/2017_01-NISO-understanding-metadata.pdf"
}

@ARTICLE{Gruning2018-ed,
  title       = "Bioconda: sustainable and comprehensive software distribution
                 for the life sciences",
  author      = "Gr{\"u}ning, Bj{\"o}rn and Dale, Ryan and Sj{\"o}din, Andreas
                 and Chapman, Brad A and Rowe, Jillian and Tomkins-Tinch,
                 Christopher H and Valieris, Renan and K{\"o}ster, Johannes and
                 {Bioconda Team}",
  affiliation = "Bioinformatics Group, Department of Computer Science,
                 University of Freiburg, Freiburg, Germany. Laboratory of
                 Cellular and Developmental Biology, National Institute of
                 Diabetes and Digestive and Kidney Diseases, US National
                 Institutes of Health, Bethesda, MD, USA. Division of CBRN
                 Security and Defence, FOI-Swedish Defence Research Agency,
                 Ume{\aa}, Sweden. Department of Chemistry, Computational Life
                 Science Cluster (CLiC), Ume{\aa} University, Ume{\aa}, Sweden.
                 Harvard T.H. Chan School of Public Health, Boston, MA, USA.
                 Center for Genomics and Systems Biology, Genomics Core,, NYU
                 Abu Dhabi,, Abu Dhabi,, United Arab Emirates. Department of
                 Organismic and Evolutionary Biology, Harvard University,
                 Cambridge, MA, USA. Broad Institute of MIT and Harvard,
                 Cambridge, MA, USA. Laboratory of Bioinformatics and
                 Computational Biology, A. C. Camargo Cancer Center, S{\~a}o
                 Paulo, Brazil. Algorithms for Reproducible Bioinformatics,
                 Genome Informatics, Institute of Human Genetics, University
                 Hospital Essen, University of Duisburg-Essen, Essen, Germany.
                 johannes.koester@uni-due.de. Medical Oncology, Dana Farber
                 Cancer Institute, Harvard Medical School, Boston, MA, USA.
                 johannes.koester@uni-due.de.",
  journal     = "Nature methods",
  volume      =  15,
  number      =  7,
  pages       = "475--476",
  month       =  jul,
  year        =  2018,
  url         = "http://dx.doi.org/10.1038/s41592-018-0046-7",
  language    = "en",
  issn        = "1548-7091, 1548-7105",
  pmid        = "29967506",
  doi         = "10.1038/s41592-018-0046-7"
}

@ARTICLE{Beaulieu-Jones2017-ky,
  title     = "Reproducibility of computational workflows is automated using
               continuous analysis",
  author    = "Beaulieu-Jones, Brett K and Greene, Casey S",
  abstract  = "The application of continuous integration, an approach common in
               software development, enables the automatic reproduction of
               computational analyses.",
  journal   = "Nature biotechnology",
  publisher = "Nature Research",
  volume    =  35,
  number    =  4,
  pages     = "342--346",
  month     =  mar,
  year      =  2017,
  url       = "http://dx.doi.org/10.1038/nbt.3780",
  keywords  = "citedinwf",
  language  = "en",
  issn      = "1087-0156, 1546-1696",
  doi       = "10.1038/nbt.3780"
}

@ARTICLE{Himmelstein2019-dt,
  title       = "Open collaborative writing with Manubot",
  author      = "Himmelstein, Daniel S and Rubinetti, Vincent and Slochower,
                 David R and Hu, Dongbo and Malladi, Venkat S and Greene, Casey
                 S and Gitter, Anthony",
  affiliation = "Department of Systems Pharmacology and Translational
                 Therapeutics, University of Pennsylvania, Philadelphia,
                 Pennsylvania, United States of America. Skaggs School of
                 Pharmacy and Pharmaceutical Sciences, University of
                 California, San Diego, San Diego, California, United States of
                 America. Department of Bioinformatics, University of Texas
                 Southwestern Medical Center, Dallas, Texas, United States of
                 America. Bioinformatics Core Facility, University of Texas
                 Southwestern Medical Center, Dallas, Texas, United States of
                 America. Department of Biostatistics and Medical Informatics,
                 University of Wisconsin-Madison, Madison, Wisconsin, United
                 States of America. Morgridge Institute for Research, Madison,
                 Wisconsin, United States of America.",
  abstract    = "Open, collaborative research is a powerful paradigm that can
                 immensely strengthen the scientific process by integrating
                 broad and diverse expertise. However, traditional research and
                 multi-author writing processes break down at scale. We present
                 new software named Manubot, available at https://manubot.org,
                 to address the challenges of open scholarly writing. Manubot
                 adopts the contribution workflow used by many large-scale open
                 source software projects to enable collaborative authoring of
                 scholarly manuscripts. With Manubot, manuscripts are written
                 in Markdown and stored in a Git repository to precisely track
                 changes over time. By hosting manuscript repositories
                 publicly, such as on GitHub, multiple authors can
                 simultaneously propose and review changes. A cloud service
                 automatically evaluates proposed changes to catch errors.
                 Publication with Manubot is continuous: When a manuscript's
                 source changes, the rendered outputs are rebuilt and
                 republished to a web page. Manubot automates bibliographic
                 tasks by implementing citation by identifier, where users cite
                 persistent identifiers (e.g. DOIs, PubMed IDs, ISBNs, URLs),
                 whose metadata is then retrieved and converted to a
                 user-specified style. Manubot modernizes publishing to align
                 with the ideals of open science by making it transparent,
                 reproducible, immediate, versioned, collaborative, and free of
                 charge.",
  journal     = "PLoS computational biology",
  volume      =  15,
  number      =  6,
  pages       = "e1007128",
  month       =  jun,
  year        =  2019,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1007128",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "31233491",
  doi         = "10.1371/journal.pcbi.1007128",
  pmc         = "PMC6611653"
}

@ARTICLE{Clarke2019-ub,
  title       = "{FAIRshake}: Toolkit to Evaluate the {FAIRness} of Research
                 Digital Resources",
  author      = "Clarke, Daniel J B and Wang, Lily and Jones, Alex and
                 Wojciechowicz, Megan L and Torre, Denis and Jagodnik, Kathleen
                 M and Jenkins, Sherry L and McQuilton, Peter and Flamholz,
                 Zachary and Silverstein, Moshe C and Schilder, Brian M and
                 Robasky, Kimberly and Castillo, Claris and Idaszak, Ray and
                 Ahalt, Stanley C and Williams, Jason and Schurer, Stephan and
                 Cooper, Daniel J and de Miranda Azevedo, Ricardo and Klenk,
                 Juergen A and Haendel, Melissa A and Nedzel, Jared and
                 Avillach, Paul and Shimoyama, Mary E and Harris, Rayna M and
                 Gamble, Meredith and Poten, Rudy and Charbonneau, Amanda L and
                 Larkin, Jennie and Brown, C Titus and Bonazzi, Vivien R and
                 Dumontier, Michel J and Sansone, Susanna-Assunta and Ma'ayan,
                 Avi",
  affiliation = "Department of Pharmacological Sciences, Mount Sinai Center for
                 Bioinformatics, Icahn School of Medicine at Mount Sinai, New
                 York, NY 10029, USA. Deloitte Consulting, 1919 N Lynn St.,
                 Arlington, VA 22209, USA. Oxford e-Research Centre, Department
                 of Engineering Science, University of Oxford, Oxford OX1 3QG,
                 UK. Renaissance Computing Institute, University of North
                 Carolina, Chapel Hill, USA. Cold Spring Harbor Laboratory, 1
                 Bungtown Rd., Cold Spring Harbor, NY 11724, USA. University of
                 Miami, Pharmacology, 1600 NW 10th Ave., Miami, FL 33136, USA.
                 Institute of Data Science Maastricht University,
                 Universiteitssingel 60, 6229 ER Maastricht, the Netherlands.
                 Oregon State University, 307 Linus Pauling Science Center,
                 2900 SW Campus Way, Corvallis, OR 9733, USA. The Broad
                 Institute, 415 Main St., Cambridge, MA 02142, USA. Harvard
                 Medical School, Department of Biomedical Informatics, 10
                 Shattuck St., Boston, MA 02115, USA. Medical College of
                 Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226,
                 USA. Department of Population Health and Reproduction, School
                 of Veterinary Medicine, UC Davis, Davis, CA 95616, USA.
                 Curoverse, Somerville, MA 02144, USA. National Institute of
                 Diabetes, Digestive, and Kidney Diseases (NIDDK), National
                 Institutes of Health (NIH), 6707 Democracy Blvd., Bethesda, MD
                 20817, USA. Office of the Director, National Institutes of
                 Health, 8600 Rockville Pike, Rm. 2S-20, Bethesda, MD, 20894,
                 USA. Department of Pharmacological Sciences, Mount Sinai
                 Center for Bioinformatics, Icahn School of Medicine at Mount
                 Sinai, New York, NY 10029, USA. Electronic address:
                 avi.maayan@mssm.edu.",
  abstract    = "As more digital resources are produced by the research
                 community, it is becoming increasingly important to harmonize
                 and organize them for synergistic utilization. The findable,
                 accessible, interoperable, and reusable (FAIR) guiding
                 principles have prompted many stakeholders to consider
                 strategies for tackling this challenge. The FAIRshake toolkit
                 was developed to enable the establishment of community-driven
                 FAIR metrics and rubrics paired with manual and automated FAIR
                 assessments. FAIR assessments are visualized as an insignia
                 that can be embedded within digital-resources-hosting
                 websites. Using FAIRshake, a variety of biomedical digital
                 resources were manually and automatically evaluated for their
                 level of FAIRness.",
  journal     = "Cell systems",
  volume      =  9,
  number      =  5,
  pages       = "417--421",
  month       =  nov,
  year        =  2019,
  url         = "http://dx.doi.org/10.1016/j.cels.2019.09.011",
  language    = "en",
  issn        = "2405-4720, 2405-4712",
  pmid        = "31677972",
  doi         = "10.1016/j.cels.2019.09.011",
  pmc         = "PMC7316196"
}

@ARTICLE{Da_Veiga_Leprevost2017-jl,
  title       = "{BioContainers}: an open-source and community-driven framework
                 for software standardization",
  author      = "da Veiga Leprevost, Felipe and Gr{\"u}ning, Bj{\"o}rn A and
                 Alves Aflitos, Saulo and R{\"o}st, Hannes L and Uszkoreit,
                 Julian and Barsnes, Harald and Vaudel, Marc and Moreno, Pablo
                 and Gatto, Laurent and Weber, Jonas and Bai, Mingze and
                 Jimenez, Rafael C and Sachsenberg, Timo and Pfeuffer, Julianus
                 and Vera Alvarez, Roberto and Griss, Johannes and Nesvizhskii,
                 Alexey I and Perez-Riverol, Yasset",
  affiliation = "Department of Pathology, University of Michigan, Ann Arbor, MI
                 48109, USA. Bioinformatics Group, Department of Computer
                 Science, Albert-Ludwigs-University Freiburg, Freiburg,
                 Germany. Albert-Ludwigs-University, Department of Computer
                 Science, Bioinformatics Group, Freiburg,
                 Baden-W{\"u}rttemberg, 79110 Freiburg, Freiburg. Wageningen
                 Plant Research, Cluster Bioinformatics, Wageningen, 6700 AD,
                 Gelderland, Netherlands. Department of Genetics, Stanford
                 University, USA. Medizinisches Proteom-Center, Ruhr-University
                 Bochum, Germany, 44801. Proteomics Unit (PROBE), Department of
                 Biomedicine, University of Bergen, Bergen, Norway.
                 Computational Biology Unit (CBU), Department of Informatics,
                 University of Bergen, Bergen, Norway. KG Jebsen Center for
                 Diabetes Research, Department of Clinical Science, University
                 of Bergen, Norway. (I) KG Jebsen Center for Diabetes Research,
                 Department of Clinical Science, University of Bergen, Bergen,
                 Norway, 5020; (II) Center for Medical Genetics and Molecular
                 Medicine, Haukeland University Hospital, Bergen, Norway, 5020.
                 EMBL Outstation, European Bioinformatics Institute, Proteomics
                 Services, Wellcome Trust Genome Campus, Hinxton, Cambridge,
                 UK. Computational Proteomics Unit and Cambridge Centre for
                 Proteomics, Department of Biochemistry, University of
                 Cambridge, Cambridge, UK. Universit{\"a}t T{\"u}bingen,
                 Wilhelm Schickard Institut f{\"u}r Informatik, Applied
                 Bioinformatics Group,D-72076 T{\"u}bingen, Germany.
                 Eberhard-Karls-Universit{\"a}t T{\"u}bingen, Department of
                 Computer Science, Applied bioinformatics, 72076 T{\"u}bingen.
                 Computational Biology Branch, National Center for
                 Biotechnology Information, National Library of Medicine,
                 National Institutes of Health, Bethesda, MD, 20892, USA.
                 Division of Immunology, Allergy and Infectious Diseases,
                 Department of Dermatology, Medical University of Vienna,
                 Austria. Department of Computational Medicine and
                 Bioinformatics, University of Michigan, Ann Arbor, MI 48109,
                 USA.",
  abstract    = "Motivation: BioContainers (biocontainers.pro) is an
                 open-source and community-driven framework which provides
                 platform independent executable environments for
                 bioinformatics software. BioContainers allows labs of all
                 sizes to easily install bioinformatics software, maintain
                 multiple versions of the same software and combine tools into
                 powerful analysis pipelines. BioContainers is based on popular
                 open-source projects Docker and rkt frameworks, that allow
                 software to be installed and executed under an isolated and
                 controlled environment. Also, it provides infrastructure and
                 basic guidelines to create, manage and distribute
                 bioinformatics containers with a special focus on omics
                 technologies. These containers can be integrated into more
                 comprehensive bioinformatics pipelines and different
                 architectures (local desktop, cloud environments or HPC
                 clusters). Availability and Implementation: The software is
                 freely available at github.com/BioContainers/. Contact:
                 yperez@ebi.ac.uk.",
  journal     = "Bioinformatics",
  volume      =  33,
  number      =  16,
  pages       = "2580--2582",
  month       =  aug,
  year        =  2017,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btx192",
  keywords    = "containers",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "28379341",
  doi         = "10.1093/bioinformatics/btx192"
}

@MISC{noauthor_undated-tb,
  title        = "Open Containers Initiative",
  booktitle    = "Open Containers Initiative",
  url          = "https://www.opencontainers.org/",
  howpublished = "\url{https://www.opencontainers.org/}",
  note         = "Accessed: 2017-2-10"
}

@ARTICLE{Bandrowski2016-cg,
  title       = "{RRIDs}: A Simple Step toward Improving Reproducibility
                 through Rigor and Transparency of Experimental Methods",
  author      = "Bandrowski, Anita E and Martone, Maryann E",
  affiliation = "Center for Research in Biological Systems, UCSD, La Jolla, CA
                 92093, USA. Electronic address: abandrowski@ucsd.edu. Center
                 for Research in Biological Systems, UCSD, La Jolla, CA 92093,
                 USA.",
  abstract    = "With the call for more rigorous scientific reporting,
                 authentication, and transparency from the scientific community
                 and funding agencies, one critical step is to make finding and
                 identifying key resources in the published literature
                 tractable. We discuss here the use of Research Resource
                 Identifiers (RRIDs) as one tool to help resolve this tricky
                 problem in reproducibility.",
  journal     = "Neuron",
  volume      =  90,
  number      =  3,
  pages       = "434--436",
  month       =  may,
  year        =  2016,
  url         = "http://dx.doi.org/10.1016/j.neuron.2016.04.030",
  language    = "en",
  issn        = "0896-6273, 1097-4199",
  pmid        = "27151636",
  doi         = "10.1016/j.neuron.2016.04.030",
  pmc         = "PMC5854161"
}

@ARTICLE{Ramasamy2008-mf,
  title       = "Key issues in conducting a meta-analysis of gene expression
                 microarray datasets",
  author      = "Ramasamy, Adaikalavan and Mondry, Adrian and Holmes, Chris C
                 and Altman, Douglas G",
  affiliation = "Centre for Statistics in Medicine, University of Oxford,
                 Oxford, United Kingdom. ramasamy@stats.ox.ac.uk",
  journal     = "PLoS medicine",
  volume      =  5,
  number      =  9,
  pages       = "e184",
  month       =  sep,
  year        =  2008,
  url         = "http://dx.doi.org/10.1371/journal.pmed.0050184",
  language    = "en",
  issn        = "1549-1277, 1549-1676",
  pmid        = "18767902",
  doi         = "10.1371/journal.pmed.0050184",
  pmc         = "PMC2528050"
}

@ARTICLE{Begley2012-mt,
  title       = "Drug development: Raise standards for preclinical cancer
                 research",
  author      = "Begley, C Glenn and Ellis, Lee M",
  affiliation = "Hematology and Oncology Research, Amgen, Thousand Oaks,
                 California 91359, USA.",
  journal     = "Nature",
  volume      =  483,
  number      =  7391,
  pages       = "531--533",
  month       =  mar,
  year        =  2012,
  url         = "http://dx.doi.org/10.1038/483531a",
  language    = "en",
  issn        = "0028-0836, 1476-4687",
  pmid        = "22460880",
  doi         = "10.1038/483531a"
}

@ARTICLE{Sahoo2019-yo,
  title       = "{ProvCaRe}: Characterizing scientific reproducibility of
                 biomedical research studies using semantic provenance metadata",
  author      = "Sahoo, Satya S and Valdez, Joshua and Kim, Matthew and
                 Rueschman, Michael and Redline, Susan",
  affiliation = "Department of Population and Quantitative Health Sciences,
                 School of Medicine, Case Western Reserve University,
                 Cleveland, OH, USA. Electronic address: satya.sahoo@case.edu.
                 Department of Population and Quantitative Health Sciences,
                 School of Medicine, Case Western Reserve University,
                 Cleveland, OH, USA. Department of Medicine, Brigham and
                 Women's Hospital and Beth Israel Deaconess Medical Center,
                 Harvard Medical School, Boston, MA, USA.",
  abstract    = "OBJECTIVE: Reproducibility of research studies is key to
                 advancing biomedical science by building on sound results and
                 reducing inconsistencies between published results and study
                 data. We propose that the available data from research studies
                 combined with provenance metadata provide a framework for
                 evaluating scientific reproducibility. We developed the
                 ProvCaRe platform to model, extract, and query semantic
                 provenance information from 435, 248 published articles.
                 METHODS: The ProvCaRe platform consists of: (1) the S3 model
                 and a formal ontology; (2) a provenance-focused text
                 processing workflow to generate provenance triples consisting
                 of subject, predicate, and object using metadata extracted
                 from articles; and (3) the ProvCaRe knowledge repository that
                 supports ``provenance-aware'' hypothesis-driven search
                 queries. A new provenance-based ranking algorithm is used to
                 rank the articles in the search query results. RESULTS: The
                 ProvCaRe knowledge repository contains 48.9 million provenance
                 triples. Seven research hypotheses were used as search queries
                 for evaluation and the resulting provenance triples were
                 analyzed using five categories of provenance terms. The
                 highest number of terms (34\%) described provenance related to
                 population cohort followed by 29\% of terms describing
                 statistical data analysis methods, and only 5\% of the terms
                 described the measurement instruments used in a study. In
                 addition, the analysis showed that some articles included a
                 higher number of provenance terms across multiple provenance
                 categories suggesting a higher potential for reproducibility
                 of these research studies. CONCLUSION: The ProvCaRe knowledge
                 repository (https://provcare. CASE: edu/) is one of the
                 largest provenance resources for biomedical research studies
                 that combines intuitive search functionality with a new
                 provenance-based ranking feature to list articles related to a
                 search query.",
  journal     = "International journal of medical informatics",
  volume      =  121,
  pages       = "10--18",
  month       =  jan,
  year        =  2019,
  url         = "http://dx.doi.org/10.1016/j.ijmedinf.2018.10.009",
  keywords    = "ProvCaRe knowledge repository; ProvCaRe ontology; Provenance
                 metadata; Provenance-based ranking; S3 model; Scientific
                 reproducibility; W3C PROV specifications",
  language    = "en",
  issn        = "1386-5056, 1872-8243",
  pmid        = "30545485",
  doi         = "10.1016/j.ijmedinf.2018.10.009",
  pmc         = "PMC6343667"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lipscomb2000-nc,
  title     = "Medical Subject Headings ({MeSH})",
  author    = "Lipscomb, C E",
  abstract  = "In 1960, medical librarianship was on the cusp of a revolution.
               The first issue of the new Index Medicus series was published.
               On the horizon was a computerization project undertaken by the
               National Library of Medicine (NLM) to store and retrieve
               information. The …",
  journal   = "Bulletin of the Medical Library Association",
  publisher = "ncbi.nlm.nih.gov",
  volume    =  88,
  number    =  3,
  pages     = "265--266",
  month     =  jul,
  year      =  2000,
  url       = "https://www.ncbi.nlm.nih.gov/pubmed/10928714",
  language  = "en",
  issn      = "0025-7338",
  pmid      = "10928714",
  pmc       = "PMC35238"
}

@ARTICLE{Hrynaszkiewicz2020-bw,
  title    = "Publishers' Responsibilities in Promoting Data Quality and
              Reproducibility",
  author   = "Hrynaszkiewicz, Iain",
  abstract = "Scholarly publishers can help to increase data quality and
              reproducible research by promoting transparency and openness.
              Increasing transparency can be achieved by publishers in six key
              areas: (1) understanding researchers' problems and motivations,
              by conducting and responding to the findings of surveys; (2)
              raising awareness of issues and encouraging behavioural and
              cultural change, by introducing consistent journal policies on
              sharing research data, code and materials; (3) improving the
              quality and objectivity of the peer-review process by
              implementing reporting guidelines and checklists and using
              technology to identify misconduct; (4) improving scholarly
              communication infrastructure with journals that publish all
              scientifically sound research, promoting study registration,
              partnering with data repositories and providing services that
              improve data sharing and data curation; (5) increasing incentives
              for practising open research with data journals and software
              journals and implementing data citation and badges for
              transparency; and (6) making research communication more open and
              accessible, with open-access publishing options, permitting text
              and data mining and sharing publisher data and metadata and
              through industry and community collaboration. This chapter
              describes practical approaches being taken by publishers, in
              these six areas, their progress and effectiveness and the
              implications for researchers publishing their work.",
  journal  = "Handbook of experimental pharmacology",
  volume   =  257,
  pages    = "319--348",
  year     =  2020,
  url      = "http://dx.doi.org/10.1007/164_2019_290",
  keywords = "Data sharing; Open access; Open science; Peer review; Publishing;
              Reporting guidelines; Reproducible research; Research data;
              Scholarly communication",
  language = "en",
  issn     = "0171-2004",
  pmid     = "31691858",
  doi      = "10.1007/164\_2019\_290"
}

@ARTICLE{Sansone2019-rz,
  title       = "{FAIRsharing} as a community approach to standards,
                 repositories and policies",
  author      = "Sansone, Susanna-Assunta and McQuilton, Peter and Rocca-Serra,
                 Philippe and Gonzalez-Beltran, Alejandra and Izzo,
                 Massimiliano and Lister, Allyson L and Thurston, Milo and
                 {FAIRsharing Community}",
  affiliation = "Oxford e-Research Centre, Department of Engineering Science,
                 University of Oxford, Oxford, UK.
                 susanna-assunta.sansone@oerc.ox.ac.uk. Oxford e-Research
                 Centre, Department of Engineering Science, University of
                 Oxford, Oxford, UK.",
  journal     = "Nature biotechnology",
  volume      =  37,
  number      =  4,
  pages       = "358--367",
  month       =  apr,
  year        =  2019,
  url         = "http://dx.doi.org/10.1038/s41587-019-0080-8",
  language    = "en",
  issn        = "1087-0156, 1546-1696",
  pmid        = "30940948",
  doi         = "10.1038/s41587-019-0080-8",
  pmc         = "PMC6785156"
}

@ARTICLE{Baker2016-qu,
  title    = "1,500 scientists lift the lid on reproducibility",
  author   = "Baker, Monya",
  abstract = "Survey sheds light on the `crisis' rocking research.",
  journal  = "Nature News",
  volume   =  533,
  number   =  7604,
  pages    = "452",
  month    =  may,
  year     =  2016,
  url      = "https://www.nature.com/articles/533452a",
  keywords = "prospectus I;in prospectus;prospectus II",
  doi      = "10.1038/533452a"
}

@MISC{Yuen2016-gt,
  title     = "{Ga4Gh/Dockstore}: 1.0",
  author    = "Yuen, Denis and Duncan, Andrew and Liu, Victor and O'Connor,
               Brian and Patricia, Janice and {oicr-vchung} and Amstutz, Peter
               and Badger, The Gitter",
  abstract  = "Production release",
  publisher = "Zenodo",
  year      =  2016,
  url       = "http://dx.doi.org/10.5281/zenodo.154185",
  doi       = "10.5281/zenodo.154185"
}

@ARTICLE{Ioannidis2005-se,
  title       = "Why most published research findings are false",
  author      = "Ioannidis, John P A",
  affiliation = "Department of Hygiene and Epidemiology, University of Ioannina
                 School of Medicine, Ioannina, Greece. jioannid@cc.uoi.gr",
  abstract    = "There is increasing concern that most current published
                 research findings are false. The probability that a research
                 claim is true may depend on study power and bias, the number
                 of other studies on the same question, and, importantly, the
                 ratio of true to no relationships among the relationships
                 probed in each scientific field. In this framework, a research
                 finding is less likely to be true when the studies conducted
                 in a field are smaller; when effect sizes are smaller; when
                 there is a greater number and lesser preselection of tested
                 relationships; where there is greater flexibility in designs,
                 definitions, outcomes, and analytical modes; when there is
                 greater financial and other interest and prejudice; and when
                 more teams are involved in a scientific field in chase of
                 statistical significance. Simulations show that for most study
                 designs and settings, it is more likely for a research claim
                 to be false than true. Moreover, for many current scientific
                 fields, claimed research findings may often be simply accurate
                 measures of the prevailing bias. In this essay, I discuss the
                 implications of these problems for the conduct and
                 interpretation of research.",
  journal     = "PLoS medicine",
  publisher   = "Public Library of Science",
  volume      =  2,
  number      =  8,
  pages       = "e124",
  month       =  aug,
  year        =  2005,
  url         = "http://dx.doi.org/10.1371/journal.pmed.0020124",
  keywords    = "prospectus II;in prospectus;reproducibility case studies",
  language    = "en",
  issn        = "1549-1277, 1549-1676",
  pmid        = "16060722",
  doi         = "10.1371/journal.pmed.0020124",
  pmc         = "PMC1182327"
}

@ARTICLE{Murillo2014-ck,
  title     = "Examining data sharing and data reuse in the {DataONE}
               environment",
  author    = "Murillo, Angela P",
  journal   = "Proceedings of the American Society for Information Science and
               Technology",
  publisher = "Wiley Online Library",
  volume    =  51,
  number    =  1,
  pages     = "1--5",
  year      =  2014,
  url       = "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/meet.2014.14505101155"
}

@ARTICLE{Cormier2021-ed,
  title     = "Go Get Data ({GGD}) is a framework that facilitates reproducible
               access to genomic data",
  author    = "Cormier, Michael J and Belyeu, Jonathan R and Pedersen, Brent S
               and Brown, Joseph and K{\"o}ster, Johannes and Quinlan, Aaron R",
  abstract  = "The rapid increase in the amount of genomic data provides
               researchers with an opportunity to integrate diverse datasets
               and annotations when addressing a wide range of biological
               questions. However, genomic datasets are deposited on different
               platforms and are stored in numerous formats from multiple
               genome builds, which complicates the task of collecting,
               annotating, transforming, and integrating data as needed. Here,
               we developed Go Get Data (GGD) as a fast, reproducible approach
               to installing standardized data recipes. GGD is available on
               Github ( https://gogetdata.github.io/ ), is extendable to other
               data types, and can streamline the complexities typically
               associated with data integration, saving researchers time and
               improving research reproducibility. Modern biological research
               is complicated by the difficulty of collecting, transforming,
               annotating, and integrating datasets. Here, the authors present
               Go Get Data, a fast, reproducible approach to installing
               standardized data recipes, with an application to genomics data.",
  journal   = "Nature communications",
  publisher = "Nature Publishing Group",
  volume    =  12,
  number    =  1,
  pages     = "1--6",
  month     =  apr,
  year      =  2021,
  url       = "https://www.nature.com/articles/s41467-021-22381-z",
  language  = "en",
  issn      = "2041-1723, 2041-1723",
  doi       = "10.1038/s41467-021-22381-z"
}

@ARTICLE{Ince2012-ya,
  title       = "The case for open computer programs",
  author      = "Ince, Darrel C and Hatton, Leslie and Graham-Cumming, John",
  affiliation = "Department of Computing Open University, Walton Hall, Milton
                 Keynes MK7 6AA, UK. d.c.ince@open.ac.uk",
  abstract    = "Scientific communication relies on evidence that cannot be
                 entirely included in publications, but the rise of
                 computational science has added a new layer of
                 inaccessibility. Although it is now accepted that data should
                 be made available on request, the current regulations
                 regarding the availability of software are inconsistent. We
                 argue that, with some exceptions, anything less than the
                 release of source programs is intolerable for results that
                 depend on computation. The vagaries of hardware, software and
                 natural language will always ensure that exact reproducibility
                 remains uncertain, but withholding code increases the chances
                 that efforts to reproduce results will fail.",
  journal     = "Nature",
  volume      =  482,
  number      =  7386,
  pages       = "485--488",
  month       =  feb,
  year        =  2012,
  url         = "http://dx.doi.org/10.1038/nature10836",
  language    = "en",
  issn        = "0028-0836, 1476-4687",
  pmid        = "22358837",
  doi         = "10.1038/nature10836"
}

@INPROCEEDINGS{Altintas2004-aq,
  title     = "Kepler: an extensible system for design and execution of
               scientific workflows",
  booktitle = "Proceedings. 16th International Conference on Scientific and
               Statistical Database Management, 2004.",
  author    = "Altintas, I and Berkley, C and Jaeger, E and Jones, M and
               Ludascher, B and Mock, S",
  abstract  = "Most scientists conduct analyses and run models in several
               different software and hardware environments, mentally
               coordinating the export and import of data from one environment
               to another. The Kepler scientific workflow system provides
               domain scientists with an easy-to-use yet powerful system for
               capturing scientific workflows (SWFs). SWFs are a formalization
               of the ad-hoc process that a scientist may go through to get
               from raw data to publishable results. Kepler attempts to
               streamline the workflow creation and execution process so that
               scientists can design, execute, monitor, re-run, and communicate
               analytical procedures repeatedly with minimal effort. Kepler is
               unique in that it seamlessly combines high-level workflow design
               with execution and runtime interaction, access to local and
               remote data, and local and remote service invocation. SWFs are
               superficially similar to business process workflows but have
               several challenges not present in the business workflow
               scenario. For example, they often operate on large, complex and
               heterogeneous data, can be computationally intensive and produce
               complex derived data products that may be archived for use in
               reparameterized runs or other workflows. Moreover, unlike
               business workflows, SWFs are often dataflow-oriented as
               witnessed by a number of recent academic systems (e.g.,
               DiscoveryNet, Taverna and Triana) and commercial systems
               (Scitegic/Pipeline-Pilot, Inforsense). In a sense, SWFs are
               often closer to signal-processing and data streaming
               applications than they are to control-oriented business workflow
               applications.",
  publisher = "ieeexplore.ieee.org",
  pages     = "423--424",
  month     =  jun,
  year      =  2004,
  url       = "http://dx.doi.org/10.1109/SSDM.2004.1311241",
  keywords  = "data flow computing;data handling;database management
               systems;scientific information systems;workflow management
               software;DiscoveryNet;Inforsense;Kepler scientific workflow
               system;Scitegic/Pipeline-Pilot;Taverna;Triana;complex data;data
               access;data export;data import;data products;data
               streaming;extensible system;hardware environment;heterogeneous
               data;high-level workflow design;large data;runtime
               interaction;scientific analysis;scientific workflow
               design;scientific workflow execution;service invocation;software
               environment;workflow creation;Biological system
               modeling;Business;Java;Plugs;Power system
               modeling;Prototypes;Runtime;Supercomputers;Web
               services;Yarn;pipelines \& provenance",
  issn      = "1099-3371",
  doi       = "10.1109/SSDM.2004.1311241"
}

@ARTICLE{Whitcher2011-dt,
  title     = "Working with the {DICOM} and {NIfTI} Data Standards in {R}",
  author    = "Whitcher, Brandon and Schmid, Volker J and Thornton, Andrew",
  abstract  = "Two packages, oro.dicom and oro.nifti, are provided for the
               interaction with and manipulation of medical imaging data that
               conform to the DICOM standard or ANALYZE/NIfTI formats. DICOM
               data, from a single file or directory tree, may be uploaded into
               R using basic data structures: a data frame for the header
               information and a matrix for the image data. A list structure is
               used to organize multiple DICOM files. The S4 class framework is
               used to develop basic ANALYZE and NIfTI classes, where NIfTI
               extensions may be used to extend the fixed-byte NIfTI header.
               One example of this, that has been implemented, is an XML-based
               ``audit trail'' tracking the history of operations applied to a
               data set. The conversion from DICOM to ANALYZE/NIfTI is
               straightforward using the capabilities of both packages. The S4
               classes have been developed to provide a userfriendly interface
               to the ANALYZE/NIfTI data formats; allowing easy data input,
               data output, image processing and visualization.",
  journal   = "Journal of statistical software",
  publisher = "Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen",
  number    =  6,
  month     =  oct,
  year      =  2011,
  url       = "https://epub.ub.uni-muenchen.de/58553/",
  language  = "en",
  issn      = "1548-7660",
  doi       = "10.18637/jss.v044.i06"
}

@ARTICLE{Queralt-Rosinach2016-uq,
  title       = "{DisGeNET-RDF}: harnessing the innovative power of the
                 Semantic Web to explore the genetic basis of diseases",
  author      = "Queralt-Rosinach, N{\'u}ria and Pi{\~n}ero, Janet and Bravo,
                 {\`A}lex and Sanz, Ferran and Furlong, Laura I",
  affiliation = "Integrative Biomedical Informatics (IBI) Group, Research
                 Programme on Biomedical Informatics (GRIB), Hospital del Mar
                 Medical Research Institute (IMIM), Department of Experimental
                 and Health Sciences (DCEXS), Universitat Pompeu Fabra (UPF),
                 C/Doctor Aiguader 88, E-08003 Barcelona, Spain. Integrative
                 Biomedical Informatics (IBI) Group, Research Programme on
                 Biomedical Informatics (GRIB), Hospital del Mar Medical
                 Research Institute (IMIM), Department of Experimental and
                 Health Sciences (DCEXS), Universitat Pompeu Fabra (UPF),
                 C/Doctor Aiguader 88, E-08003 Barcelona, Spain. Integrative
                 Biomedical Informatics (IBI) Group, Research Programme on
                 Biomedical Informatics (GRIB), Hospital del Mar Medical
                 Research Institute (IMIM), Department of Experimental and
                 Health Sciences (DCEXS), Universitat Pompeu Fabra (UPF),
                 C/Doctor Aiguader 88, E-08003 Barcelona, Spain. Integrative
                 Biomedical Informatics (IBI) Group, Research Programme on
                 Biomedical Informatics (GRIB), Hospital del Mar Medical
                 Research Institute (IMIM), Department of Experimental and
                 Health Sciences (DCEXS), Universitat Pompeu Fabra (UPF),
                 C/Doctor Aiguader 88, E-08003 Barcelona, Spain. Integrative
                 Biomedical Informatics (IBI) Group, Research Programme on
                 Biomedical Informatics (GRIB), Hospital del Mar Medical
                 Research Institute (IMIM), Department of Experimental and
                 Health Sciences (DCEXS), Universitat Pompeu Fabra (UPF),
                 C/Doctor Aiguader 88, E-08003 Barcelona, Spain.",
  abstract    = "MOTIVATION: DisGeNET-RDF makes available knowledge on the
                 genetic basis of human diseases in the Semantic Web.
                 Gene-disease associations (GDAs) and their provenance metadata
                 are published as human-readable and machine-processable web
                 resources. The information on GDAs included in DisGeNET-RDF is
                 interlinked to other biomedical databases to support the
                 development of bioinformatics approaches for translational
                 research through evidence-based exploitation of a rich and
                 fully interconnected linked open data. AVAILABILITY AND
                 IMPLEMENTATION: http://rdf.disgenet.org/ CONTACT:
                 support@disgenet.org.",
  journal     = "Bioinformatics",
  volume      =  32,
  number      =  14,
  pages       = "2236--2238",
  month       =  jul,
  year        =  2016,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btw214",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "27153650",
  doi         = "10.1093/bioinformatics/btw214",
  pmc         = "PMC4937199"
}

@ARTICLE{Heery2000-cl,
  title    = "Application Profiles: Mixing and Matching Metadata Schemas",
  author   = "Heery, Rachel and Patel, Manjula",
  journal  = "Ariadne",
  number   =  25,
  year     =  2000,
  url      = "http://www.ariadne.ac.uk/issue/25/app-profiles/",
  issn     = "1361-3200"
}

@ARTICLE{Kluyver2016-eu,
  title     = "Jupyter Notebooks---a publishing format for reproducible
               computational workflows",
  author    = "Kluyver, Thomas and Ragan-Kelley, Benjamin and P{\'e}rez,
               Fernando and Granger, Brian and Bussonnier, Matthias and
               Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica and
               Grout, Jason and Corlay, Sylvain and {Others}",
  abstract  = "... it and modify it themselves. While nbconvert and nbviewer
               facilitate sharing statically rendered notebooks, a new project
               called Binder (http:// mybinder . org/) enables sharing of live
               notebooks, Page 101. T. Kluyver et al./Jupyter ...",
  journal   = "Positioning and Power in Academic Publishing: Players, Agents
               and Agendas",
  publisher = "books.google.com",
  pages     = "87",
  year      =  2016,
  url       = "https://books.google.com/books?hl=en&lr=&id=Lgy3DAAAQBAJ&oi=fnd&pg=PA87&dq=%22mybinder%22&ots=N0F-7QvIcl&sig=OEcpWretgMASEOoAEetLnxbDXzQ",
  keywords  = "notebooks \& script provenance"
}

@ARTICLE{Donoho2010-xp,
  title       = "An invitation to reproducible computational research",
  author      = "Donoho, David L",
  affiliation = "Department of Statistics, Stanford University, Stanford, CA
                 94305, USA. donoho@stanford.edu",
  journal     = "Biostatistics",
  volume      =  11,
  number      =  3,
  pages       = "385--388",
  month       =  jul,
  year        =  2010,
  url         = "http://dx.doi.org/10.1093/biostatistics/kxq028",
  keywords    = "best practices and general reproducibility/bitch and
                 moan/opinion articles",
  language    = "en",
  issn        = "1465-4644, 1468-4357",
  pmid        = "20538873",
  doi         = "10.1093/biostatistics/kxq028"
}

@ARTICLE{Palmblad2019-uk,
  title       = "Automated workflow composition in mass spectrometry-based
                 proteomics",
  author      = "Palmblad, Magnus and Lamprecht, Anna-Lena and Ison, Jon and
                 Schw{\"a}mmle, Veit",
  affiliation = "Center for Proteomics and Metabolomics, Leiden University
                 Medical Center, RC Leiden, The Netherlands. Department of
                 Information and Computing Sciences, Utrecht University, CC
                 Utrecht, The Netherlands. National Life Science Supercomputing
                 Center, Technical University of Denmark, Kongens Lyngby,
                 Denmark. Department of Biochemistry and Molecular Biology and
                 VILLUM Center for Bioanalytical Sciences, University of
                 Southern Denmark, Odense, Denmark.",
  abstract    = "MOTIVATION: Numerous software utilities operating on mass
                 spectrometry (MS) data are described in the literature and
                 provide specific operations as building blocks for the
                 assembly of on-purpose workflows. Working out which tools and
                 combinations are applicable or optimal in practice is often
                 hard. Thus researchers face difficulties in selecting
                 practical and effective data analysis pipelines for a specific
                 experimental design. RESULTS: We provide a toolkit to support
                 researchers in identifying, comparing and benchmarking
                 multiple workflows from individual bioinformatics tools.
                 Automated workflow composition is enabled by the tools'
                 semantic annotation in terms of the EDAM ontology. To
                 demonstrate the practical use of our framework, we created and
                 evaluated a number of logically and semantically equivalent
                 workflows for four use cases representing frequent tasks in
                 MS-based proteomics. Indeed we found that the results computed
                 by the workflows could vary considerably, emphasizing the
                 benefits of a framework that facilitates their systematic
                 exploration. AVAILABILITY AND IMPLEMENTATION: The project
                 files and workflows are available from
                 https://github.com/bio-tools/biotoolsCompose/tree/master/Automatic-Workflow-Composition.
                 SUPPLEMENTARY INFORMATION: Supplementary data are available at
                 Bioinformatics online.",
  journal     = "Bioinformatics",
  volume      =  35,
  number      =  4,
  pages       = "656--664",
  month       =  feb,
  year        =  2019,
  url         = "http://dx.doi.org/10.1093/bioinformatics/bty646",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "30060113",
  doi         = "10.1093/bioinformatics/bty646",
  pmc         = "PMC6378944"
}

@ARTICLE{Williams2012-mo,
  title       = "Open {PHACTS}: semantic interoperability for drug discovery",
  author      = "Williams, Antony J and Harland, Lee and Groth, Paul and
                 Pettifer, Stephen and Chichester, Christine and Willighagen,
                 Egon L and Evelo, Chris T and Blomberg, Niklas and Ecker,
                 Gerhard and Goble, Carole and Mons, Barend",
  affiliation = "Royal Society of Chemistry, ChemSpider, US Office, Wake
                 Forest, NC 27587, USA. tony27587@gmail.com",
  abstract    = "Open PHACTS is a public-private partnership between academia,
                 publishers, small and medium sized enterprises and
                 pharmaceutical companies. The goal of the project is to
                 deliver and sustain an 'open pharmacological space' using and
                 enhancing state-of-the-art semantic web standards and
                 technologies. It is focused on practical and robust
                 applications to solve specific questions in drug discovery
                 research. OPS is intended to facilitate improvements in drug
                 discovery in academia and industry and to support open
                 innovation and in-house non-public drug discovery research.
                 This paper lays out the challenges and how the Open PHACTS
                 project is hoping to address these challenges technically and
                 socially.",
  journal     = "Drug discovery today",
  publisher   = "Elsevier",
  volume      =  17,
  number      = "21-22",
  pages       = "1188--1198",
  month       =  nov,
  year        =  2012,
  url         = "http://dx.doi.org/10.1016/j.drudis.2012.05.016",
  keywords    = "APIs \& Semantic ontologies",
  language    = "en",
  issn        = "1359-6446, 1878-5832",
  pmid        = "22683805",
  doi         = "10.1016/j.drudis.2012.05.016"
}

@ARTICLE{Nust2018-wx,
  title       = "Reproducible research and {GIScience}: an evaluation using
                 {AGILE} conference papers",
  author      = "N{\"u}st, Daniel and Granell, Carlos and Hofer, Barbara and
                 Konkol, Markus and Ostermann, Frank O and Sileryte, Rusne and
                 Cerutti, Valentina",
  affiliation = "Institute for Geoinformatics, University of M{\"u}nster,
                 M{\"u}nster, Germany. Institute of New Imaging Technologies,
                 Universitat Jaume I de Castell{\'o}n, Castell{\'o}n, Spain.
                 Interfaculty Department of Geoinformatics - Z\_GIS, University
                 of Salzburg, Salzburg, Austria. Faculty of Geo-Information
                 Science and Earth Observation (ITC), University of Twente,
                 Enschede, The Netherlands. Faculty of Architecture and the
                 Built Environment, Delft University of Technology, Delft, The
                 Netherlands.",
  abstract    = "The demand for reproducible research is on the rise in
                 disciplines concerned with data analysis and computational
                 methods. Therefore, we reviewed current recommendations for
                 reproducible research and translated them into criteria for
                 assessing the reproducibility of articles in the field of
                 geographic information science (GIScience). Using this
                 criteria, we assessed a sample of GIScience studies from the
                 Association of Geographic Information Laboratories in Europe
                 (AGILE) conference series, and we collected feedback about the
                 assessment from the study authors. Results from the author
                 feedback indicate that although authors support the concept of
                 performing reproducible research, the incentives for doing
                 this in practice are too small. Therefore, we propose concrete
                 actions for individual researchers and the GIScience
                 conference series to improve transparency and reproducibility.
                 For example, to support researchers in producing reproducible
                 work, the GIScience conference series could offer awards and
                 paper badges, provide author guidelines for computational
                 research, and publish articles in Open Access formats.",
  journal     = "PeerJ",
  volume      =  6,
  pages       = "e5072",
  month       =  jul,
  year        =  2018,
  url         = "http://dx.doi.org/10.7717/peerj.5072",
  keywords    = "AGILE; Data science; GIScience; Open access; Open science;
                 Reproducible conference publications; Reproducible research",
  language    = "en",
  issn        = "2167-8359",
  pmid        = "30013826",
  doi         = "10.7717/peerj.5072",
  pmc         = "PMC6047504"
}

@ARTICLE{Rosenberg2021-an,
  title       = "Reproducible results policy",
  author      = "Rosenberg, David E and Jones, Amber Spackman and Filion, Yves
                 and Teasley, Rebecca and Sandoval-Solis, Samuel and Stagge,
                 James H and Abdallah, Adel and Castronova, Anthony and
                 Ostfeld, Avi and Watkins, Jr, David",
  affiliation = "Professor, Dept. of Civil and Environmental Engineering and
                 Utah Water Research Laboratory, Utah State Univ., 8200 Old
                 Main Hill, Logan, UT 84322-8200 (corresponding author). ORCID:
                 .; Research Associate and Ph.D. Candidate, Dept. of Civil and
                 Environmental Engineering and Utah Water Research Laboratory,
                 Utah State Univ., 8200 Old Main Hill, Logan, UT 84322-8200.;
                 Professor, Dept. of Civil Engineering, Queen's Univ., 58
                 University Ave., Kingston, ON, Canada K7K0B9.; Associate
                 Professor, Dept. of Civil Engineering, Univ. of Minnesota
                 Duluth, 1405 University Dr., Duluth, MI 55812.; Associate
                 Professor, Dept. of Land, Air, and Water Resources, Univ. of
                 California, Davis, 1 Shields Ave., Davis, CA 95616.; Assistant
                 Professor, Civil, Environmental and Geodetic Engineering, Ohio
                 State Univ., 470 Hitchcock Hall, 2070 Neil Ave., Columbus, OH
                 43210.; Program Manager, Water Data Exchange (WaDE), Western
                 States Water Council, 682 East Vine St., Suite 7, Salt Lake
                 City, UT 84107.; Hydrologic Scientist, Consortium of
                 Universities for the Advancement of Hydrologic Science, Inc.
                 (CUAHSI), 150 Cambridgepark Dr., Suite 203, Cambridge, MA
                 02140.; Professor and Vice Dean, Civil and Environmental
                 Engineering, Technion---Israel Institute of Technology, Rabin
                 Bldg., Room 610, Haifa 32000, Israel.; Distinguished
                 Professor, Dept. of Civil and Environmental Engineering,
                 Michigan Technological Univ., 1400 Townsend Dr., Houghton, MI
                 49931.",
  journal     = "Journal of water resources planning and management",
  publisher   = "American Society of Civil Engineers (ASCE)",
  volume      =  147,
  number      =  2,
  pages       = "01620001",
  month       =  feb,
  year        =  2021,
  url         = "http://ascelibrary.org/doi/10.1061/%28ASCE%29WR.1943-5452.0001368",
  language    = "en",
  issn        = "0733-9496, 1943-5452",
  doi         = "10.1061/(asce)wr.1943-5452.0001368"
}

@ARTICLE{Fanelli2018-ek,
  title   = "Opinion: Is science really facing a reproducibility crisis, and do
             we need it to?",
  author  = "Fanelli, Daniele",
  journal = "Proceedings of the National Academy of Sciences of the United
             States of America",
  volume  =  115,
  number  =  11,
  pages   = "2628--2631",
  month   =  mar,
  year    =  2018,
  url     = "http://www.pnas.org/lookup/doi/10.1073/pnas.1708272114",
  issn    = "0027-8424, 1091-6490",
  doi     = "10.1073/pnas.1708272114"
}

@INPROCEEDINGS{Gueld2002-tr,
  title      = "Quality of {DICOM} header information for image categorization",
  booktitle  = "Medical Imaging 2002: {PACS} and Integrated Medical Information
                Systems: Design and Evaluation",
  author     = "Gueld, Mark Oliver and Kohnen, Michael and Keysers, Daniel and
                Schubert, Henning and Wein, Berthold B and Bredno, Joerg and
                Lehmann, Thomas Martin",
  abstract   = "The widely used DICOM 3.0 imaging protocol specifies optional
                tags to store specific information on modality and body region
                within the header: Body Part Examined and Anatomic Structure.
                We investigate whether this information can be used for the
                automated categorization of medical images, as this is an
                important first step for medical image retrieval. Our survey
                examines the headers generated by four digital image modalities
                (2 CTs, 2 MRIs) in clinical routine at the Aachen University
                Hospital within a period of four months. The manufacturing
                dates of the modalities range from 1995 to 1999, with software
                revisions from 1999 and 2000. Only one modality sets the DICOM
                tag Body Part Examined. 90 out of 580 images (15.5\%) contained
                false tag entries causing a wrong categorization. This result
                was verified during a second evaluation period of one month one
                year later (562 images, 15.3\% error rate). The main reason is
                the dependency of the tag on the examination protocol of the
                modality, which controls all relevant parameters of the imaging
                process. In routine, the clinical personnel often applies an
                examination protocol outside its normal context to improve the
                imaging quality. This is, however, done without manually
                adjusting the categorization specific tag values. The values
                specified by DICOM for the tag Body Part Examined are
                insufficient to encode the anatomic region precisely. Thus, an
                automated categorization relying on DICOM tags alone is
                impossible.",
  publisher  = "International Society for Optics and Photonics",
  volume     =  4685,
  pages      = "280--287",
  month      =  may,
  year       =  2002,
  url        = "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/4685/0000/Quality-of-DICOM-header-information-for-image-categorization/10.1117/12.467017.short",
  keywords   = "Content Based Image Retrieval; CBIR; DICOM; Image
                Categorization; PACS; Standardization; Valid;",
  conference = "Medical Imaging 2002: PACS and Integrated Medical Information
                Systems: Design and Evaluation",
  doi        = "10.1117/12.467017"
}

@ARTICLE{Gangemi2017-ad,
  title     = "The Publishing Workflow Ontology ({PWO})",
  author    = "Gangemi, Aldo and Peroni, Silvio and Shotton, David and Vitali,
               Fabio",
  abstract  = "Abstract In this paper we introduce the Publishing Workflow
               Ontology (PWO), ie, an OWL 2 DL ontology for the description of
               workflows that is particularly suitable for formalising typical
               publishing processes such as the publication of articles in
               journals. We support the",
  journal   = "Semantic Web",
  publisher = "IOS Press",
  number    = "Preprint",
  pages     = "1--16",
  year      =  2017,
  url       = "http://content.iospress.com/articles/semantic-web/sw230",
  keywords  = "APIs \& Semantic ontologies"
}

@ARTICLE{Emsley2018-yd,
  title     = "A framework for the preservation of a Docker container",
  author    = "Emsley, Iain and De Roure, David",
  abstract  = "Reliably building and maintaining systems across environments is
               a continuing problem. A project or experiment may run for years.
               Software and hardware may change as can the operating system.
               Containerisation is a technology that is used in a variety of
               companies, such as Google, Amazon and IBM, and scientific
               projects to rapidly deploy a set of services repeatably. Using
               Dockerfiles to ensure that a container is built repeatably, to
               allow conformance and easy updating when changes take place are
               becoming common within projects. Its seen as part of sustainable
               software development. Containerisation technology occupies a
               dual space: it is both a repository of software and software
               itself. In considering Docker in this fashion, we should verify
               that the Dockerfile can be reproduced. Using a subset of the
               Dockerfile specification, a domain specific language is created
               to ensure that Docker files can be reused at a later stage to
               recreate the original environment. We provide a simple framework
               to address the question of the preservation of containers and
               its environment. We present experiments on an existing
               Dockerfile and conclude with a discussion of future work. Taking
               our work, a pipeline was implemented to check that a defined
               Dockerfile conforms to our desired model, extracts the Docker
               and operating system details. This will help the reproducibility
               of results by creating the machine environment and package
               versions. It also helps development and testing through ensuring
               that the system is repeatably built and that any changes in the
               software environment can be equally shared in the Dockerfile.
               This work supports not only the citation process it also the
               open scientific one by providing environmental details of the
               work. As a part of the pipeline to create the container, we
               capture the processes used and put them into the W3C PROV
               ontology. This provides the potential for providing it with a
               persistent identifier and traceability of the processes used to
               preserve the metadata. Our future work will look at the question
               of linking this output to a workflow ontology to preserve the
               complete workflow with the commands and parameters to be given
               to the containers. We see this provenance within the build
               process useful to provide a complete overview of the workflow.",
  journal   = "International journal of digital curation",
  publisher = "Edinburgh University Library",
  volume    =  12,
  number    =  2,
  pages     = "125--135",
  month     =  apr,
  year      =  2018,
  url       = "http://www.ijdc.net/article/view/509",
  issn      = "1746-8256",
  doi       = "10.2218/ijdc.v12i2.509"
}

@ARTICLE{noauthor_undated-dp,
  title    = "Journal Article Tag Suite 1.0: National Information Standards
              Organization standard of journal extensible markup language",
  author   = {Sun Huh},
  abstract = "In the era of information technology, scholarly journals cannot
              escape the rising tide of technological advancement. To be
              exposed more easily to readers, the web forms of scholarly
              journals and articles become more important year after year.
              Furthermore, there is a trend of print journals closing, and a
              significant emergence of online journals. Journal Article Tag
              Suite (JATS) extensible markup language (XML) became an National
              Information Standards Organization standard language in online
              journal publishing in 2012. It is an essential format to present
              readers with a more user-friendly interface. JATS XML was
              developed by PubMed Central (PMC) XML, which was a deposit form
              of articles to PMC. Editors and other publishing-related
              personnel should be able to understand the concept and production
              process of XML files. When JATS XML is produced, a variety of web
              presentation views can be generated, such as PubReader and epub
              3.0. Further, JATS XML can be easily converted to digital object
              identifier CrossRef XML, CrossMark XML, and FundRef XML. Small
              scholarly society journal editors and publishers can promote the
              visibility of their journals by depositing JATS XML files to PMC
              or ScienceCentral. Owing to these benefits of JATS XML,
              publishers and editors should now adopt JATS XML for journal
              publishing. Keywords: Extensible markup language; Information
              technology; Journal Article Tag Suite; Journal publishing; Open
              access",
  journal  = "Science education",
  url      = "http://escienceediting.org/journal/view.php?number=21",
  language = "en",
  issn     = "0036-8326",
  doi      = "10.6087/kcse.2014.1.99",
  year     = {2014}
}

@ARTICLE{The_Gene_Ontology_Consortium2019-pt,
  title     = "{The Gene Ontology Resource: 20 years and still {GOing} strong.}",
  author    = "{The Gene Ontology Consortium}",
  abstract  = "The Gene Ontology resource (GO; http://geneontology.org)
               provides structured, computable knowledge regarding the
               functions of genes and gene products. Founded in 1998, GO has
               become widely adopted in the life sciences, and its contents are
               under continual improvement, both in quantity and in quality.
               Here, we report the major developments of the GO resource during
               the past two years. Each monthly release of the GO resource is
               now packaged and given a unique identifier (DOI), enabling
               GO-based analyses on a specific release to be reproduced in the
               future. The molecular function ontology has been refactored to
               better represent the overall activities of gene products, with a
               focus on transcription regulator activities. Quality assurance
               efforts have been ramped up to address potentially out-of-date
               or inaccurate annotations. New evidence codes for
               high-throughput experiments now enable users to filter out
               annotations obtained from these sources. GO-CAM, a new framework
               for representing gene function that is more expressive than
               standard GO annotations, has been released, and users can now
               explore the growing repository of these models. We also provide
               the 'GO ribbon' widget for visualizing GO annotations to a gene;
               the widget can be easily embedded in any web page.",
  journal   = "Nucleic acids research",
  publisher = "Oxford University Press",
  volume    =  47,
  number    = "D1",
  pages     = "D330--D338",
  year      =  2019,
  url       = "http://dx.doi.org/10.1093/nar/gky1055",
  keywords  = "charlie\_rcr.bib",
  issn      = "0305-1048, 1362-4962",
  pmid      = "30395331",
  doi       = "10.1093/nar/gky1055"
}

@ARTICLE{Pineau2020-bs,
  title         = "Improving Reproducibility in Machine Learning Research (A
                   Report from the {NeurIPS} 2019 Reproducibility Program)",
  author        = "Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha,
                   Koustuv and Larivi{\`e}re, Vincent and Beygelzimer, Alina
                   and d'Alch{\'e}-Buc, Florence and Fox, Emily and Larochelle,
                   Hugo",
  abstract      = "One of the challenges in machine learning research is to
                   ensure that presented and published results are sound and
                   reliable. Reproducibility, that is obtaining similar results
                   as presented in a paper or talk, using the same code and
                   data (when available), is a necessary step to verify the
                   reliability of research findings. Reproducibility is also an
                   important step to promote open and accessible research,
                   thereby allowing the scientific community to quickly
                   integrate new findings and convert ideas to practice.
                   Reproducibility also promotes the use of robust experimental
                   workflows, which potentially reduce unintentional errors. In
                   2019, the Neural Information Processing Systems (NeurIPS)
                   conference, the premier international conference for
                   research in machine learning, introduced a reproducibility
                   program, designed to improve the standards across the
                   community for how we conduct, communicate, and evaluate
                   machine learning research. The program contained three
                   components: a code submission policy, a community-wide
                   reproducibility challenge, and the inclusion of the Machine
                   Learning Reproducibility checklist as part of the paper
                   submission process. In this paper, we describe each of these
                   components, how it was deployed, as well as what we were
                   able to learn from this initiative.",
  month         =  mar,
  year          =  2020,
  url           = "http://arxiv.org/abs/2003.12206",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "2003.12206",
  primaryClass  = "cs.LG",
  arxivid       = "2003.12206"
}

@INCOLLECTION{Pearson1990-kf,
  title     = "[5] Rapid and sensitive sequence comparison with {FASTP} and
               {FASTA}",
  booktitle = "Methods in Enzymology",
  author    = "Pearson, William R",
  abstract  = "The FASTA program can search the NBRF protein sequence library
               (2.5 million residues) in less than 20 min on an IBM-PC
               microcomputer and unambiguously detect proteins that shared a
               common ancestor billions of years in the past. FASTA is both
               fast and selective because it initially considers only amino
               acid identities. Its sensitivity is increased not only by using
               the PAM250 matrix to score and rescore regions with large
               numbers of identities but also by joining initial regions. The
               results of searches with FASTA compare favorably with results
               using NWS-based programs that are 100 times slower. FASTA is
               slightly less sensitive but considerably more selective. It is
               not clear that NWS-based programs would be more successful in
               finding distantly related members of the G-protein-coupled
               receptor family. The joining step by FASTA to calculate the
               initn score is especially useful for sequences that share
               regions of sequence similarity that are separated by
               variable-length loops. FASTP and FASTA were designed to identify
               protein sequences that have descended from a common ancestor,
               and they have proved very useful for this task. In many cases, a
               FASTA sequence search will result in a list of high scoring
               library sequences that are homologous to the query sequence, or
               the search will result in a list of sequences with similarity
               scores, that cannot be distinguished from the bulk of the
               library. In either case, the question of whether there are
               sequences in the library that are clearly related to the query
               sequence has been answered unambiguously. Unfortunately, the
               results often will not be so clear-cut, and careful analysis and
               the biological context are required. In the course of analyzing
               the G-protein-coupled receptor family, several proteins were
               found that, because with optimization, appeared to be members of
               this family which were not previously recognized. RDF2 analysis
               showed borderline z values, and only a careful examination of
               the sequence alignments that focused on the conserved residues
               provided convincing evidence that the high scores were
               fortuitous. As sequence comparison methods become more powerful
               by becoming more sensitive, they become more likely to mislead,
               and even greater care is required.",
  publisher = "Academic Press",
  volume    =  183,
  pages     = "63--98",
  month     =  jan,
  year      =  1990,
  url       = "http://www.sciencedirect.com/science/article/pii/007668799083007V",
  doi       = "10.1016/0076-6879(90)83007-V"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Greenberg2005-gy,
  title     = "Understanding metadata and metadata schemes",
  author    = "Greenberg, J",
  abstract  = "Although the development and implementation of metadata schemes
               over the last decade has been extensive, research examining the
               sum of these activities is limited. This limitation is likely
               due to the massive scope of the topic. A framework is needed to
               study the full extent …",
  journal   = "Cataloging \& classification quarterly",
  publisher = "Taylor \& Francis",
  year      =  2005,
  url       = "https://www.tandfonline.com/doi/abs/10.1300/J104v40n03_02",
  keywords  = "Greenbergs;Greenberg"
}

@ARTICLE{Allen2008-ho,
  title    = "{Deep semantic analysis of text}",
  author   = "Allen, J F and Swift, M and De Beaumont, W",
  journal  = "Proceedings of the 2008 Conference on Semantics in Text
              Processing",
  volume   =  1,
  pages    = "343",
  year     =  2008,
  keywords = "charlie\_rcr.bib"
}

@ARTICLE{Papoutsoglou2020-be,
  title       = "Enabling reusability of plant phenomic datasets with {MIAPPE}
                 1.1",
  author      = "Papoutsoglou, Evangelia A and Faria, Daniel and Arend, Daniel
                 and Arnaud, Elizabeth and Athanasiadis, Ioannis N and Chaves,
                 In{\^e}s and Coppens, Frederik and Cornut, Guillaume and
                 Costa, Bruno V and {\'C}wiek-Kupczy{\'n}ska, Hanna and
                 Droesbeke, Bert and Finkers, Richard and Gruden, Kristina and
                 Junker, Astrid and King, Graham J and Krajewski, Pawe{\l} and
                 Lange, Matthias and Laporte, Marie-Ang{\'e}lique and Michotey,
                 C{\'e}lia and Oppermann, Markus and Ostler, Richard and
                 Poorter, Hendrik and Ram{\i} Rez-Gonzalez, Ricardo and Ram{\v
                 s}ak, {\v Z}iva and Reif, Jochen C and Rocca-Serra, Philippe
                 and Sansone, Susanna-Assunta and Scholz, Uwe and Tardieu,
                 Fran{\c c}ois and Uauy, Cristobal and Usadel, Bj{\"o}rn and
                 Visser, Richard G F and Weise, Stephan and Kersey, Paul J and
                 Miguel, C{\'e}lia M and Adam-Blondon, Anne-Fran{\c c}oise and
                 Pommier, Cyril",
  affiliation = "Plant Breeding, Wageningen University \& Research, PO Box 386,
                 Wageningen, 6700AJ, the Netherlands. BioData.pt, Instituto
                 Gulbenkian de Ci{\^e}ncia, 2780-156, Oeiras, Portugal.
                 INESC-ID, 1000-029, Lisboa, Portugal. Leibniz Institute of
                 Plant Genetics and Crop Plant Research (IPK) Gatersleben,
                 06466, Seeland, Germany. Bioversity International, Parc
                 Scientifique Agropolis II, Montpellier Cedex 5, 34397, France.
                 Geo-Information Science and Remote Sensing Laboratory,
                 Wageningen University, Droevendaalsesteeg 3, Wageningen,
                 6708PB, the Netherlands. Instituto de Tecnologia Qu{\'\i}mica
                 e Biol{\'o}gica Ant{\'o}nio Xavier, Universidade Nova de
                 Lisboa (ITQB NOVA) Avenida da Rep{\'u}blica, 2780-157, Oeiras,
                 Portugal. Instituto de Biologia Experimental e Tecnol{\'o}gica
                 (iBET), 2780-157, Oeiras, Portugal. Department of Plant
                 Biotechnology and Bioinformatics, Ghent University,
                 Technologiepark 71, Ghent, 9052, Belgium. VIB Center for Plant
                 Systems Biology, Technologiepark 71, Ghent, 9052, Belgium.
                 Universit{\'e} Paris-Saclay, INRAE, URGI, Versailles, 78026,
                 France. BioISI - Biosystems \& Integrative Sciences Institute,
                 Faculdade de Ci{\^e}ncias, Universidade de Lisboa, Lisboa,
                 1749-016, Portugal. Institute of Plant Genetics, Polish
                 Academy of Sciences, ul. Strzeszy{\'n}ska 34, 60-479,
                 Pozna{\'n}, Poland. Department of Biotechnology and Systems
                 Biology, National Institute of Biology, SI1000, Ljubljana,
                 Slovenia. Southern Cross Plant Science, Southern Cross
                 University, Lismore, NSW 2577, Australia. Computational and
                 Analytical Sciences, Rothamsted Research, Harpenden, AL5 2JQ,
                 UK. Plant Sciences (IBG-2), Forschungszentrum J{\"u}lich GmbH,
                 D-52425, J{\"u}lich, Germany. Department of Biological
                 Sciences, Macquarie University, North Ryde, NSW 2109,
                 Australia. Department of Crop Genetics, John Innes Centre,
                 Norwich Research Park, Colney, Norwich, NR4 7UH, UK. Oxford
                 e-Research Centre, Department of Engineering Science,
                 University of Oxford, 7 Keble Road, Oxford, OX1 3QG, UK. INRA,
                 Laboratoire d'Ecophysiologie des Plantes sous Stress
                 Environnementaux, UMR759, Montpellier, 34060, France.
                 Institute for Biology I, BioSC, RWTH Aachen University,
                 Worringer Weg 3, 52074, Aachen, Germany. Royal Botanic
                 Gardens, Kew, Richmond, TW9 3AE, UK.",
  abstract    = "Enabling data reuse and knowledge discovery is increasingly
                 critical in modern science, and requires an effort towards
                 standardising data publication practices. This is particularly
                 challenging in the plant phenotyping domain, due to its
                 complexity and heterogeneity. We have produced the MIAPPE 1.1
                 release, which enhances the existing MIAPPE standard in
                 coverage, to support perennial plants, in structure, through
                 an explicit data model, and in clarity, through definitions
                 and examples. We evaluated MIAPPE 1.1 by using it to express
                 several heterogeneous phenotyping experiments in a range of
                 different formats, to demonstrate its applicability and the
                 interoperability between the various implementations.
                 Furthermore, the extended coverage is demonstrated by the fact
                 that one of the datasets could not have been described under
                 MIAPPE 1.0. MIAPPE 1.1 marks a major step towards enabling
                 plant phenotyping data reusability, thanks to its extended
                 coverage, and especially the formalisation of its data model,
                 which facilitates its implementation in different formats.
                 Community feedback has been critical to this development, and
                 will be a key part of ensuring adoption of the standard.",
  journal     = "The New phytologist",
  volume      =  227,
  number      =  1,
  pages       = "260--273",
  month       =  jul,
  year        =  2020,
  url         = "http://dx.doi.org/10.1111/nph.16544",
  keywords    = "findability; interoperability; metadata; phenomics; plant
                 phenotyping; reusability; standards",
  language    = "en",
  issn        = "0028-646X, 1469-8137",
  pmid        = "32171029",
  doi         = "10.1111/nph.16544",
  pmc         = "PMC7317793"
}

@MISC{Rechert2016-pg,
  title   = "Preserving Containers -- Requirements and a {Todo-List}",
  author  = "Rechert, Klaus and Liebetraut, Thomas and Wehrle, Dennis and
             Cochrane, Euan",
  journal = "Digital Libraries: Knowledge, Information, and Data in an Open
             Access Society",
  pages   = "225--230",
  year    =  2016,
  url     = "http://dx.doi.org/10.1007/978-3-319-49304-6_27",
  doi     = "10.1007/978-3-319-49304-6\_27"
}

@ARTICLE{Nust2021-sd,
  title     = "{CODECHECK}: an Open Science initiative for the independent
               execution of computations underlying research articles during
               peer review to improve reproducibility",
  author    = "N{\"u}st, Daniel and Eglen, Stephen J",
  abstract  = "The traditional scientific paper falls short of effectively
               communicating computational research. To help improve this
               situation, we propose a system by which the computational
               workflows underlying research articles are checked. The
               CODECHECK system uses open infrastructure and tools and can be
               integrated into review and publication processes in multiple
               ways. We describe these integrations along multiple dimensions
               (importance, who, openness, when). In collaboration with
               academic publishers and conferences, we demonstrate CODECHECK
               with 25 reproductions of diverse scientific publications. These
               CODECHECKs show that asking for reproducible workflows during a
               collaborative review can effectively improve executability.
               While CODECHECK has clear limitations, it may represent a
               building block in Open Science and publishing ecosystems for
               improving the reproducibility, appreciation, and, potentially,
               the quality of non-textual research artefacts. The CODECHECK
               website can be accessed here: https://codecheck.org.uk/.",
  journal   = "F1000Research",
  publisher = "F1000 Research Limited",
  volume    =  10,
  number    =  253,
  pages     = "253",
  month     =  mar,
  year      =  2021,
  url       = "https://f1000research.com/articles/10-253/v1/pdf",
  keywords  = "reproducible research, Open Science, peer review,
               reproducibility, code sharing, data sharing, quality control,
               scholarly publishing",
  language  = "en",
  doi       = "10.12688/f1000research.51738.1"
}

@ARTICLE{Wilkinson2016-qr,
  title       = "The {FAIR} Guiding Principles for scientific data management
                 and stewardship",
  author      = "Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I
                 Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak,
                 Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva
                 Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau
                 and Brookes, Anthony J and Clark, Tim and Crosas, Merc{\`e}
                 and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and
                 Evelo, Chris T and Finkers, Richard and Gonzalez-Beltran,
                 Alejandra and Gray, Alasdair J G and Groth, Paul and Goble,
                 Carole and Grethe, Jeffrey S and Heringa, Jaap and 't Hoen,
                 Peter A C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and
                 Kok, Joost and Lusher, Scott J and Martone, Maryann E and
                 Mons, Albert and Packer, Abel L and Persson, Bengt and
                 Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and
                 Sansone, Susanna-Assunta and Schultes, Erik and Sengstag,
                 Thierry and Slater, Ted and Strawn, George and Swertz, Morris
                 A and Thompson, Mark and van der Lei, Johan and van Mulligen,
                 Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg,
                 Peter and Wolstencroft, Katherine and Zhao, Jun and Mons,
                 Barend",
  affiliation = "Center for Plant Biotechnology and Genomics, Universidad
                 Polit{\'e}cnica de Madrid, Madrid 28223, Spain. Stanford
                 University, Stanford 94305-5411, USA. Nature Genetics, New
                 York 10004-1562, USA. Euretos and Phortos Consultants,
                 Rotterdam 2741 CA, The Netherlands. ELIXIR, Wellcome Genome
                 Campus, Hinxton CB10 1SA, UK. Lygature, Eindhoven 5656 AG, The
                 Netherlands. Vrije Universiteit Amsterdam, Dutch Techcenter
                 for Life Sciences, Amsterdam 1081 HV, The Netherlands. Office
                 of the Director, National Institutes of Health, Rockville
                 20892, USA. TNO, Zeist 3700 AJ, The Netherlands. Department of
                 Genetics, University of Leicester, Leicester LE1 7RH, UK.
                 Harvard Medical School, Boston, Massachusetts MA 02115, USA.
                 Harvard University, Cambridge, Massachusetts MA 02138, USA.
                 Data Archiving and Networked Services (DANS), The Hague 2593
                 HW, The Netherlands. GigaScience, Beijing Genomics Institute,
                 Shenzhen 518083, China. Department of Bioinformatics,
                 Maastricht University, Maastricht 6200 MD, The Netherlands.
                 Wageningen UR Plant Breeding, Wageningen 6708 PB, The
                 Netherlands. Oxford e-Research Center, University of Oxford,
                 Oxford OX1 3QG, UK. Heriot-Watt University, Edinburgh EH14
                 4AS, UK. School of Computer Science, University of Manchester,
                 Manchester M13 9PL, UK. Center for Research in Biological
                 Systems, School of Medicine, University of California San
                 Diego, La Jolla, California 92093-0446, USA. Dutch Techcenter
                 for the Life Sciences, Utrecht 3501 DE, The Netherlands.
                 Department of Human Genetics, Leiden University Medical
                 Center, Dutch Techcenter for the Life Sciences, Leiden 2300
                 RC, The Netherlands. Dutch TechCenter for Life Sciences and
                 ELIXIR-NL, Utrecht 3501 DE, The Netherlands. VU University
                 Amsterdam, Amsterdam 1081 HV, The Netherlands. Leiden Center
                 of Data Science, Leiden University, Leiden 2300 RA, The
                 Netherlands. Netherlands eScience Center, Amsterdam 1098 XG,
                 The Netherlands. National Center for Microscopy and Imaging
                 Research, UCSD, San Diego 92103, USA. Phortos Consultants, San
                 Diego 92011, USA. SciELO/FAPESP Program, UNIFESP Foundation,
                 S{\~a}o Paulo 05468-901, Brazil. Bioinformatics Infrastructure
                 for Life Sciences (BILS), Science for Life Laboratory, Dept of
                 Cell and Molecular Biology, Uppsala University, S-751 24,
                 Uppsala, Sweden. Leiden University Medical Center, Leiden 2333
                 ZA, The Netherlands. Bayer CropScience, Gent Area 1831,
                 Belgium. Leiden Institute for Advanced Computer Science,
                 Leiden University Medical Center, Leiden 2300 RA, The
                 Netherlands. Swiss Institute of Bioinformatics and University
                 of Basel, Basel 4056, Switzerland. Cray, Inc., Seattle 98164,
                 USA. University Medical Center Groningen (UMCG), University of
                 Groningen, Groningen 9713 GZ, The Netherlands. Erasmus MC,
                 Rotterdam 3015 CE, The Netherlands. Independent Open Access
                 and Open Science Advocate, Guildford GU1 3PW, UK. Micelio,
                 Antwerp 2180, Belgium. Max Planck Compute and Data Facility,
                 MPS, Garching 85748, Germany. Leiden Institute of Advanced
                 Computer Science, Leiden University, Leiden 2333 CA, The
                 Netherlands. Department of Computer Science, Oxford
                 University, Oxford OX1 3QD, UK. Leiden University Medical
                 Center, Leiden and Dutch TechCenter for Life Sciences, Utrecht
                 2333 ZA, The Netherlands.",
  abstract    = "There is an urgent need to improve the infrastructure
                 supporting the reuse of scholarly data. A diverse set of
                 stakeholders-representing academia, industry, funding
                 agencies, and scholarly publishers-have come together to
                 design and jointly endorse a concise and measureable set of
                 principles that we refer to as the FAIR Data Principles. The
                 intent is that these may act as a guideline for those wishing
                 to enhance the reusability of their data holdings. Distinct
                 from peer initiatives that focus on the human scholar, the
                 FAIR Principles put specific emphasis on enhancing the ability
                 of machines to automatically find and use the data, in
                 addition to supporting its reuse by individuals. This Comment
                 is the first formal publication of the FAIR Principles, and
                 includes the rationale behind them, and some exemplar
                 implementations in the community.",
  journal     = "Scientific data",
  publisher   = "nature.com",
  volume      =  3,
  pages       = "160018",
  month       =  mar,
  year        =  2016,
  url         = "http://dx.doi.org/10.1038/sdata.2016.18",
  keywords    = "printed;citedinwf;prospectus I;in prospectus",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "26978244",
  doi         = "10.1038/sdata.2016.18",
  pmc         = "PMC4792175"
}

@ARTICLE{Rajesh2021-lm,
  title       = "Improving the completeness of public metadata accompanying
                 omics studies",
  author      = "Rajesh, Anushka and Chang, Yutong and Abedalthagafi, Malak S
                 and Wong-Beringer, Annie and Love, Michael I and Mangul,
                 Serghei",
  affiliation = "Department of Pharmacology and Pharmaceutical Sciences,
                 University of Southern California, Los Angeles, CA, 90089,
                 USA. anushkar@usc.edu. Department of Pharmacology and
                 Pharmaceutical Sciences, University of Southern California,
                 Los Angeles, CA, 90089, USA. Genomics Research Department,
                 King Fahad Medical City and King Abdulaziz City for Science
                 and Technology, Riyadh, Saudi Arabia. Department of Clinical
                 Pharmacy, University of Southern California, Los Angeles, CA,
                 90089, USA. Department of Biostatistics, University of North
                 Carolina at Chapel Hill, Chapel Hill, NC, 27516, USA.
                 Department of Genetics, University of North Carolina at Chapel
                 Hill, Chapel Hill, NC, 27514, USA. Department of Clinical
                 Pharmacy, University of Southern California, Los Angeles, CA,
                 90089, USA. serghei.mangul@gmail.com.",
  journal     = "Genome biology",
  volume      =  22,
  number      =  1,
  pages       = "106",
  month       =  apr,
  year        =  2021,
  url         = "http://dx.doi.org/10.1186/s13059-021-02332-z",
  language    = "en",
  issn        = "1465-6906",
  pmid        = "33858487",
  doi         = "10.1186/s13059-021-02332-z",
  pmc         = "PMC8048353"
}

@ARTICLE{Dippo_undated-rd,
  title  = "The Role of Metadata in Statistics",
  author = "Dippo, Cathryn S and Bureau of Labor, U and Sundgren, Bo and Dippo,
            Statistics Sweden Cathryn and Statistics, Bureau of Labor and Ne, 2
            Massachusetts Avenue and C., D",
  url    = "http://www.bls.gov/ore/pdf/st000040.pdf"
}

@ARTICLE{Gyori2017-zt,
  title    = "{From word models to executable models of signaling networks
              using automated assembly}",
  author   = "Gyori, Benjamin M and Bachman, John A and Subramanian, Kartik and
              Muhlich, Jeremy L and Galescu, Lucian and Sorger, Peter K",
  abstract = "Word models (natural language descriptions of molecular
              mechanisms) are a common currency in spoken and written
              communication in biomedicine but are of limited use in predicting
              the behavior of complex biological networks. We present an
              approach to building computational models directly from natural
              language using automated assembly. Molecular mechanisms described
              in simple English are read by natural language processing
              algorithms, converted into an intermediate representation, and
              assembled into executable or network models. We have implemented
              this approach in the Integrated Network and Dynamical Reasoning
              Assembler (INDRA), which draws on existing natural language
              processing systems as well as pathway information in Pathway
              Commons and other online resources. We demonstrate the use of
              INDRA and natural language to model three biological processes of
              increasing scope: (i) p53 dynamics in response to DNA damage,
              (ii) adaptive drug resistance in BRAF-V600E-mutant melanomas, and
              (iii) the RAS signaling pathway. The use of natural language
              makes the task of developing a model more efficient and it
              increases model transparency, thereby promoting collaboration
              with the broader biology community.",
  journal  = "Molecular systems biology",
  volume   =  13,
  number   =  11,
  pages    = "954",
  year     =  2017,
  url      = "http://msb.embopress.org/lookup/doi/10.15252/msb.20177651",
  keywords = "computational modeling,natural language
              processing,signaling;charlie\_rcr.bib",
  issn     = "1744-4292",
  pmid     = "29175850",
  doi      = "10.15252/msb.20177651"
}

@ARTICLE{Berthold2009-uw,
  title     = "{KNIME} - the Konstanz Information Miner: Version 2.0 and Beyond",
  author    = "Berthold, Michael R and Cebron, Nicolas and Dill, Fabian and
               Gabriel, Thomas R and K{\"o}tter, Tobias and Meinl, Thorsten and
               Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd",
  abstract  = "Abstract The Konstanz Information Miner is a modular
               environment, which enables easy visual assembly and interactive
               execution of a data pipeline. It is designed as a teaching,
               research and collaboration platform, which enables simple
               integration of new algorithms",
  journal   = "SIGKDD Explor. Newsl.",
  publisher = "ACM",
  volume    =  11,
  number    =  1,
  pages     = "26--31",
  month     =  nov,
  year      =  2009,
  url       = "http://doi.acm.org/10.1145/1656274.1656280",
  address   = "New York, NY, USA",
  keywords  = "citedinwf;prospectus III;in prospectus",
  issn      = "1931-0145",
  doi       = "10.1145/1656274.1656280"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@techreport{Oberkampf2018-kn,
  title     = "20 Allotrope Data {Format--Semantic} Data Management in Life
               Sciences. pdf",
  author    = "Oberkampf, Heiner and Krieg, Helge and Senger, Christian and
               Weber, Thomas and Colsman, Wolfgang",
  abstract  = "Research  intensive  industries  like  the  (bio-) pharmaceutical  industry require a data architecture enabling universal laboratory data storage comprising describing  metadata,  ontologies  and  vocabularies,  long-term  data  persistence, and metadata structure visualization. We present the Allotrope Data Format, the Allotrope Foundation Ontologies, ZONTAL Spaceand OSee as a comprehensive set of tools for laboratory data management.",
  publisher = "Semantic Web Applications and Tools for Healthcare and Life Sciences",
  year      =  2018,
  url       = "https://swat4hcls.figshare.com/articles/20_Allotrope_Data_Format_Semantic_Data_Management_in_Life_Sciences_pdf/7346489/files/13574621.pdf",
  institution = {{OSTHUS}}
}

@ARTICLE{Arabas2014-dq,
  title         = "Case Studies and Challenges in Reproducibility in the
                   Computational Sciences",
  author        = "Arabas, Sylwester and Bareford, Michael R and de Silva,
                   Lakshitha R and Gent, Ian P and Gorman, Benjamin M and
                   Hajiarabderkani, Masih and Henderson, Tristan and Hutton,
                   Luke and Konovalov, Alexander and Kotthoff, Lars and
                   McCreesh, Ciaran and Nacenta, Miguel A and Paul, Ruma R and
                   Petrie, Karen E J and Razaq, Abdul and Reijsbergen,
                   Dani{\"e}l and Takeda, Kenji",
  abstract      = "This paper investigates the reproducibility of computational
                   science research and identifies key challenges facing the
                   community today. It is the result of the First Summer School
                   on Experimental Methodology in Computational Science
                   Research (https://blogs.cs.st-andrews.ac.uk/emcsr2014/).
                   First, we consider how to reproduce experiments that involve
                   human subjects, and in particular how to deal with different
                   ethics requirements at different institutions. Second, we
                   look at whether parallel and distributed computational
                   experiments are more or less reproducible than serial ones.
                   Third, we consider reproducible computational experiments
                   from fields outside computer science. Our final case study
                   looks at whether reproducibility for one researcher is the
                   same as for another, by having an author attempt to have
                   others reproduce their own, reproducible, paper. This paper
                   is open, executable and reproducible: the whole process of
                   writing this paper is captured in the source control
                   repository hosting both the source of the paper,
                   supplementary codes and data; we are providing setup for
                   several experiments on which we were working; finally, we
                   try to describe what we have achieved during the week of the
                   school in a way that others may reproduce (and hopefully
                   improve) our experiments.",
  month         =  aug,
  year          =  2014,
  url           = "http://arxiv.org/abs/1408.2123",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "1408.2123",
  primaryClass  = "cs.CE",
  arxivid       = "1408.2123"
}

@ARTICLE{Hucka2018-cu,
  title    = "{The Systems Biology Markup Language ({SBML)}: language
              specification for level 3 version 2 core}",
  author   = "Hucka, M and Bergmann, F T and Dr{\"a}ger, A and Hoops, S and
              Keating, S M and Le Nov{\`e}re, N and Myers, C J and Olivier, B G
              and Sahle, S and {Others}",
  journal  = "Journal of integrative bioinformatics",
  volume   =  15,
  year     =  2018,
  keywords = "charlie\_rcr.bib"
}

@ARTICLE{Ioannidis2009-at,
  title       = "Repeatability of published microarray gene expression analyses",
  author      = "Ioannidis, John P A and Allison, David B and Ball, Catherine A
                 and Coulibaly, Issa and Cui, Xiangqin and Culhane, Aed{\'\i}n
                 C and Falchi, Mario and Furlanello, Cesare and Game, Laurence
                 and Jurman, Giuseppe and Mangion, Jon and Mehta, Tapan and
                 Nitzberg, Michael and Page, Grier P and Petretto, Enrico and
                 van Noort, Vera",
  affiliation = "Clinical and Molecular Epidemiology Unit, Department of
                 Hygiene and Epidemiology, University of Ioannina School of
                 Medicine, Ioannina 45110, Greece. jioannid@cc.uoi.gr",
  abstract    = "Given the complexity of microarray-based gene expression
                 studies, guidelines encourage transparent design and public
                 data availability. Several journals require public data
                 deposition and several public databases exist. However, not
                 all data are publicly available, and even when available, it
                 is unknown whether the published results are reproducible by
                 independent scientists. Here we evaluated the replication of
                 data analyses in 18 articles on microarray-based gene
                 expression profiling published in Nature Genetics in
                 2005-2006. One table or figure from each article was
                 independently evaluated by two teams of analysts. We
                 reproduced two analyses in principle and six partially or with
                 some discrepancies; ten could not be reproduced. The main
                 reason for failure to reproduce was data unavailability, and
                 discrepancies were mostly due to incomplete data annotation or
                 specification of data processing and analysis. Repeatability
                 of published microarray studies is apparently limited. More
                 strict publication rules enforcing public data availability
                 and explicit description of data processing and analysis
                 should be considered.",
  journal     = "Nature genetics",
  volume      =  41,
  number      =  2,
  pages       = "149--155",
  month       =  feb,
  year        =  2009,
  url         = "http://dx.doi.org/10.1038/ng.295",
  language    = "en",
  issn        = "1061-4036, 1546-1718",
  pmid        = "19174838",
  doi         = "10.1038/ng.295"
}

@ARTICLE{Publio2018-ib,
  title         = "{ML-Schema}: Exposing the Semantics of Machine Learning with
                   Schemas and Ontologies",
  author        = "Publio, Gustavo Correa and Esteves, Diego and
                   {\L}awrynowicz, Agnieszka and Panov, Pan{\v c}e and
                   Soldatova, Larisa and Soru, Tommaso and Vanschoren, Joaquin
                   and Zafar, Hamid",
  abstract      = "The ML-Schema, proposed by the W3C Machine Learning Schema
                   Community Group, is a top-level ontology that provides a set
                   of classes, properties, and restrictions for representing
                   and interchanging information on machine learning
                   algorithms, datasets, and experiments. It can be easily
                   extended and specialized and it is also mapped to other more
                   domain-specific ontologies developed in the area of machine
                   learning and data mining. In this paper we overview existing
                   state-of-the-art machine learning interchange formats and
                   present the first release of ML-Schema, a canonical format
                   resulted of more than seven years of experience among
                   different research institutions. We argue that exposing
                   semantics of machine learning algorithms, models, and
                   experiments through a canonical format may pave the way to
                   better interpretability and to realistically achieve the
                   full interoperability of experiments regardless of platform
                   or adopted workflow solution.",
  month         =  jul,
  year          =  2018,
  url           = "http://arxiv.org/abs/1807.05351",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "1807.05351",
  primaryClass  = "cs.LG",
  arxivid       = "1807.05351"
}

@ARTICLE{Cao_undated-bz,
  title    = "{ProvONE}: extending {PROV} to support the {DataONE} scientific
              community",
  author   = "Cao, Yang and Jones, Christopher and Cuevas-Vicentt{\i}n,
              V{\i}ctor and Jones, Matthew B and Lud{\"a}scher, Bertram and
              McPhillips, Timothy and Missier, Paolo and Schwalm, Christopher
              and Slaughter, Peter and Vieglais, Dave and {Others}",
  abstract = "The DataONE federated data network has adopted and
              extended the PROV model to support the collection, storage,
              indexing, and user browsing of the provenance of data packages
              stored in its member nodes. The PROV extension, ProvONE, adds
              provenance",
  journal  = "homepages.cs.ncl.ac.uk",
  url      = "http://homepages.cs.ncl.ac.uk/paolo.missier/doc/dataone-prov-3-years-later.pdf",
  year     = {2016}
}

@ARTICLE{Stodden2013-ce,
  title   = "Setting the default to reproducible",
  author  = "Stodden, Victoria and Borwein, Jonathan and Bailey, David H",
  journal = "computational science research. SIAM News",
  volume  =  46,
  pages   = "4--6",
  year    =  2013,
  url     = "http://stodden.net/icerm_report.pdf"
}

@INPROCEEDINGS{Perez2015-xz,
  title     = "An automatic method for the enrichment of {DICOM} metadata using
               biomedical ontologies",
  booktitle = "2015 37th Annual International Conference of the {IEEE}
               Engineering in Medicine and Biology Society ({EMBC})",
  author    = "P{\'e}rez, W and Tello, A and Saquicela, V and Vidal, M and La
               Cruz, A",
  abstract  = "This work is a novel contribution for enriching medical images
               using semantic annotations with a strategy for unifying
               different ontologies and instances of DICOM medical files. We
               present the L-MOM library (Library for Mapping of Ontological
               Metadata) as a tool for making an automatic mapping between
               instances of DICOM medical files and different medical
               ontologies (e.g., FMA, RadLex, MeSH). The main contributions
               are: i) the domain independent L-MOM library which is able to
               integrate DICOM metadata with ontologies from different domains;
               ii) a strategy to automatically annotate DICOM data with
               universally accepted medical ontologies, and provide values of
               similarity between ontologies and DICOM metadata; and iii) a
               framework to traverse ontological concepts that characterized
               clinical studies of patients registered in the framework
               catalog.",
  pages     = "2551--2554",
  month     =  aug,
  year      =  2015,
  url       = "http://dx.doi.org/10.1109/EMBC.2015.7318912",
  keywords  = "file organisation;medical administrative data processing;medical
               computing;meta data;ontologies (artificial intelligence);DICOM
               metadata;medical images;semantic annotations;DICOM medical
               files;L-MOM library;FMA biomedical ontology;RadLex biomedical
               ontology;MeSH biomedical
               ontology;Ontologies;DICOM;Metadata;Resource description
               framework;Semantics;Libraries;Biological Ontologies;Databases,
               Factual;Humans;Semantics",
  issn      = "1558-4615",
  doi       = "10.1109/EMBC.2015.7318912"
}

@INPROCEEDINGS{Auer2007-kr,
  title     = "{DBpedia}: A Nucleus for a Web of Open Data",
  booktitle = "The Semantic Web",
  author    = "Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and
               Lehmann, Jens and Cyganiak, Richard and Ives, Zachary",
  abstract  = "DBpedia is a community effort to extract structured information
               from Wikipedia and to make this information available on the
               Web. DBpedia allows you to ask sophisticated queries against
               datasets derived from Wikipedia and to link other datasets on
               the Web to Wikipedia data. We describe the extraction of the
               DBpedia datasets, and how the resulting information is published
               on the Web for human- and machine-consumption. We describe some
               emerging applications from the DBpedia community and show how
               website authors can facilitate DBpedia content within their
               sites. Finally, we present the current status of interlinking
               DBpedia with other open datasets on the Web and outline how
               DBpedia could serve as a nucleus for an emerging Web of open
               data.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "722--735",
  year      =  2007,
  url       = "http://dx.doi.org/10.1007/978-3-540-76298-0_52",
  doi       = "10.1007/978-3-540-76298-0\_52"
}

@ARTICLE{Rosenberg2016-xg,
  title       = "Neurophysiological analytics for all! Free open-source
                 software tools for documenting, analyzing, visualizing, and
                 sharing using electronic notebooks",
  author      = "Rosenberg, David M and Horn, Charles C",
  affiliation = "Biobehavioral Oncology Program, University of Pittsburgh
                 Cancer Institute, Pittsburgh, Pennsylvania; Department of
                 Neuroscience, University of Pittsburgh, Pittsburgh,
                 Pennsylvania; Biobehavioral Oncology Program, University of
                 Pittsburgh Cancer Institute, Pittsburgh, Pennsylvania;
                 Division of Gastroenterology, Hepatology, and Nutrition,
                 Department of Medicine, University of Pittsburgh School of
                 Medicine, Pittsburgh, Pennsylvania; Department of
                 Anesthesiology, University of Pittsburgh School of Medicine,
                 Pittsburgh, Pennsylvania; and Center for Neuroscience,
                 University of Pittsburgh, Pittsburgh, Pennsylvania
                 chorn@pitt.edu.",
  abstract    = "Neurophysiology requires an extensive workflow of information
                 analysis routines, which often includes incompatible
                 proprietary software, introducing limitations based on
                 financial costs, transfer of data between platforms, and the
                 ability to share. An ecosystem of free open-source software
                 exists to fill these gaps, including thousands of analysis and
                 plotting packages written in Python and R, which can be
                 implemented in a sharable and reproducible format, such as the
                 Jupyter electronic notebook. This tool chain can largely
                 replace current routines by importing data, producing
                 analyses, and generating publication-quality graphics. An
                 electronic notebook like Jupyter allows these analyses, along
                 with documentation of procedures, to display locally or
                 remotely in an internet browser, which can be saved as an
                 HTML, PDF, or other file format for sharing with team members
                 and the scientific community. The present report illustrates
                 these methods using data from electrophysiological recordings
                 of the musk shrew vagus-a model system to investigate
                 gut-brain communication, for example, in cancer
                 chemotherapy-induced emesis. We show methods for spike sorting
                 (including statistical validation), spike train analysis, and
                 analysis of compound action potentials in notebooks. Raw data
                 and code are available from notebooks in data supplements or
                 from an executable online version, which replicates all
                 analyses without installing software-an implementation of
                 reproducible research. This demonstrates the promise of
                 combining disparate analyses into one platform, along with the
                 ease of sharing this work. In an age of diverse,
                 high-throughput computational workflows, this methodology can
                 increase efficiency, transparency, and the collaborative
                 potential of neurophysiological research.",
  journal     = "Journal of neurophysiology",
  volume      =  116,
  number      =  2,
  pages       = "252--262",
  month       =  aug,
  year        =  2016,
  url         = "http://dx.doi.org/10.1152/jn.00137.2016",
  keywords    = "IPython; Jupyter; Python; electronic notebook;
                 electrophysiology; neurophysiology; open source; reproducible
                 research",
  language    = "en",
  issn        = "0022-3077, 1522-1598",
  pmid        = "27098025",
  doi         = "10.1152/jn.00137.2016",
  pmc         = "PMC4969392"
}

@ARTICLE{Whitaker2016-gl,
  title    = "Showing your working: A guide to reproducible neuroimaging
              analyses",
  author   = "Whitaker, Kirstie",
  abstract = "In this presentation I will describe resources to help
              researchers ensure their work is reproducible.",
  journal  = "figshare",
  month    =  nov,
  year     =  2016,
  url      = "https://figshare.com/articles/Showing_your_working_A_guide_to_reproducible_neuroimaging_analyses/4244996",
  doi      = "10.6084/m9.figshare.4244996.v1"
}

@ARTICLE{Stathias2018-vl,
  title       = "Sustainable data and metadata management at the {BD2K-LINCS}
                 Data Coordination and Integration Center",
  author      = "Stathias, Vasileios and Koleti, Amar and Vidovi{\'c}, Du{\v
                 s}ica and Cooper, Daniel J and Jagodnik, Kathleen M and
                 Terryn, Raymond and Forlin, Michele and Chung, Caty and Torre,
                 Denis and Ayad, Nagi and Medvedovic, Mario and Ma'ayan, Avi
                 and Pillai, Ajay and Sch{\"u}rer, Stephan C",
  affiliation = "BD2K-LINCS Data Coordination and Integration Center,
                 University of Miami, Miami, FL 33136, USA. Department of Human
                 Genetics and Genomics, Miller School of Medicine, University
                 of Miami, Miami, FL 33136, USA. Department of Molecular and
                 Cellular Pharmacology, Miller School of Medicine, University
                 of Miami, Miami, FL 33136, USA. Center for Computational
                 Science, University of Miami, Miami, FL 33146, USA. Department
                 of Pharmacological Sciences, Icahn School of Medicine at Mount
                 Sinai, New York, NY 10029, USA. Department of Psychiatry and
                 Behavioral Sciences, University of Miami, Miami, FL 33136,
                 USA. Division of Biostatistics and Bioinformatics, Department
                 of Environmental Health, University of Cincinnati, Cincinnati,
                 OH 45221, USA. Division of Genome Sciences, National Human
                 Genome Research Institute, National Institutes of Health,
                 Bethesda, MD 20891, USA.",
  abstract    = "The NIH-funded LINCS Consortium is creating an extensive
                 reference library of cell-based perturbation response
                 signatures and sophisticated informatics tools incorporating a
                 large number of perturbagens, model systems, and assays. To
                 date, more than 350 datasets have been generated including
                 transcriptomics, proteomics, epigenomics, cell phenotype and
                 competitive binding profiling assays. The large volume and
                 variety of data necessitate rigorous data standards and
                 effective data management including modular data processing
                 pipelines and end-user interfaces to facilitate accurate and
                 reliable data exchange, curation, validation, standardization,
                 aggregation, integration, and end user access. Deep metadata
                 annotations and the use of qualified data standards enable
                 integration with many external resources. Here we describe the
                 end-to-end data processing and management at the DCIC to
                 generate a high-quality and persistent product. Our data
                 management and stewardship solutions enable a functioning
                 Consortium and make LINCS a valuable scientific resource that
                 aligns with big data initiatives such as the BD2K NIH Program
                 and concords with emerging data science best practices
                 including the findable, accessible, interoperable, and
                 reusable (FAIR) principles.",
  journal     = "Scientific data",
  volume      =  5,
  pages       = "180117",
  month       =  jun,
  year        =  2018,
  url         = "http://dx.doi.org/10.1038/sdata.2018.117",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "29917015",
  doi         = "10.1038/sdata.2018.117",
  pmc         = "PMC6007090"
}

@MISC{Evanko_undated-jv,
  title        = "Guidelines for algorithms and software in Nature Methods :
                  Methagora",
  author       = "Evanko, Daniel",
  url          = "http://blogs.nature.com/methagora/2014/02/guidelines-for-algorithms-and-software-in-nature-methods.html",
  howpublished = "\url{http://blogs.nature.com/methagora/2014/02/guidelines-for-algorithms-and-software-in-nature-methods.html}",
  note         = "Accessed: 2016-12-24"
}

@BOOK{Chan1995-tw,
  title     = "Library of Congress Subject Headings: Principles and
               Application. Third Edition",
  author    = "Chan, Lois Mai",
  abstract  = "The Library of Congress Subject Headings (LCSH) system has
               become a primary subject retrieval tool in an environment that
               is very different from that for which it was designed and
               developed. This book is a comprehensive guide to the principles
               and application of LCSH that reflects changes and developments
               in the field. The text is divided into three parts. Part 1 gives
               a brief history of the system, analyzes its principles, and
               describes the vocabulary and subject authority control. Part 2
               deals with the application of LC subject headings on LC MARC
               (Machine-readable cataloging) records. LC policies are outlined
               with regard to the assignment of subject headings in general and
               the treatment of certain types of materials in particular. Part
               3 discusses the future prospects of the system as an online
               retrieval tool, drawing on recent literature. Appendices contain
               sample MARC subject authority records and bibliographic records,
               lists of the most frequently used free-floating",
  publisher = "Libraries Unlimited, Inc., P.O. Box 6633, Englewood, CO
               80155-6633 (paperback: ISBN-1-56308-191-1, $35; clothbound:
               ISBN-1-56308-195-4, $46).",
  year      =  1995,
  url       = "https://eric.ed.gov/?id=ED387146",
  keywords  = "Authority Control (Information); Cataloging; Classification;
               Futures (of Society); Information Policy; Information Retrieval;
               Information Systems; Library Catalogs; Online Catalogs; Subject
               Index Terms; Vocabulary",
  language  = "en",
  isbn      = "9781563081910"
}

@ARTICLE{Brito2020-wu,
  title       = "Recommendations to enhance rigor and reproducibility in
                 biomedical research",
  author      = "Brito, Jaqueline J and Li, Jun and Moore, Jason H and Greene,
                 Casey S and Nogoy, Nicole A and Garmire, Lana X and Mangul,
                 Serghei",
  affiliation = "Department of Clinical Pharmacy, School of Pharmacy,
                 University of Southern California, 1985 Zonal Avenue, Los
                 Angeles, CA 90089, USA. Department of Computational Medicine
                 \& Bioinformatics, Medical School, University of Michigan,
                 1301 Catherine Street, Ann Arbor, MI 48109, USA. Department of
                 Biostatistics, Epidemiology, and Informatics, Institute for
                 Biomedical Informatics, University of Pennsylvania, 3700
                 Hamilton Walk, Philadelphia, PA 19104, USA. Department of
                 Systems Pharmacology and Translational Therapeutics, Perelman
                 School of Medicine, University of Pennsylvania, 3400 Civic
                 Center Boulevard, Philadelphia, PA 19104, USA. Childhood
                 Cancer Data Lab, Alex's Lemonade Stand, 1429 Walnut St, Floor
                 10, Philadelphia, PA 19102, USA. GigaScience, 26/F, Kings Wing
                 Plaza 2, 1 On Kwan Street, Shek Mun, N.T., Hong Kong.
                 Quantitative and Computational Biology, University of Southern
                 California, Los Angeles, CA 90089, USA.",
  abstract    = "Biomedical research depends increasingly on computational
                 tools, but mechanisms ensuring open data, open software, and
                 reproducibility are variably enforced by academic
                 institutions, funders, and publishers. Publications may
                 present software for which source code or documentation are or
                 become unavailable; this compromises the role of peer review
                 in evaluating technical strength and scientific contribution.
                 Incomplete ancillary information for an academic software
                 package may bias or limit subsequent work. We provide 8
                 recommendations to improve reproducibility, transparency, and
                 rigor in computational biology-precisely the values that
                 should be emphasized in life science curricula. Our
                 recommendations for improving software availability,
                 usability, and archival stability aim to foster a sustainable
                 data science ecosystem in life science research.",
  journal     = "GigaScience",
  volume      =  9,
  number      =  6,
  month       =  jun,
  year        =  2020,
  url         = "http://dx.doi.org/10.1093/gigascience/giaa056",
  keywords    = "archival stability; big data; installability; open science;
                 reproducible research; rigor",
  language    = "en",
  issn        = "2047-217X",
  pmid        = "32479592",
  doi         = "10.1093/gigascience/giaa056",
  pmc         = "PMC7263079"
}

@ARTICLE{Michener2006-cc,
  title    = "Meta-information concepts for ecological data management",
  author   = "Michener, William K",
  abstract = "Ecological databases continue to grow in volume, breadth and
              complexity. Higher level descriptions of data (i.e., metadata)
              and information derived from subsequent data processing and
              analyses (i.e., ``meta-information'' in the broadest sense) are
              essential for understanding and using the increasingly complex
              and voluminous data and information. The concepts of
              meta-information, in general, and metadata, in particular, have
              evolved in concert with the increasing needs for functionality by
              the community. From a scientific perspective, metadata may be
              characterized as having developed from initially supporting data
              discovery; to facilitating acquisition, comprehension and
              utilization of data by humans; and, most recently, to beginning
              to enable automated data discovery, ingestion, processing and
              analysis via metadata-enabled scientific workflow systems. The
              continued conceptual and operational developments in metadata
              required to support comprehensive automated scientific workflow
              systems portend many challenges and opportunities. For example,
              there are significant opportunities for collaboration among
              ecologists and computer scientists in developing domain-specific
              controlled vocabularies and ontologies that provide the basis for
              semantic mediation---the ``glue'' technologies that enable
              automated data discovery, ingestion, processing and analysis.
              Similarly, there are opportunities for computer scientists and
              engineers to develop new mechanisms that support automated
              metadata encoding---such as providing the information that would
              be necessary to understand the end-to-end flow of sensor data
              from in situ data collection, streaming through quality assurance
              filtering, aggregation, transformation and additional processing,
              analysis, and publication of digital products. As the
              technologies mature, we still have many sociological barriers to
              overcome including the needs for increased attention to software
              usability testing and engineering to enhance user-friendliness of
              metadata management software, new capital investments in
              ecological data archives, and increasing the metadata management
              benefit--cost ratio for the average scientist via incentives and
              enabling tools.",
  journal  = "Ecological informatics",
  volume   =  1,
  number   =  1,
  pages    = "3--7",
  year     =  2006,
  url      = "http://www.sciencedirect.com/science/article/pii/S157495410500004X",
  issn     = "1574-9541",
  doi      = "10.1016/j.ecoinf.2005.08.004"
}

@BOOK{National_Academies_of_Sciences_Engineering_and_Medicine_Policy_and_Global_Affairs_Committee_on_Science_Engineering_Medicine_and_Public_Policy_Board_on_Research_Data_and_Information_Division_on_Engineering_and_Physical_Sciences_Committee_on_Applied_and_Theoretical_Statistics_Board_on_Mathematical_Sciences_and_Analytics_Division_on_Earth_and_Life_Studies_Nuclear_and_Radiation_Studies_Board_Division_of_Behavioral_and_Social_Sciences_and_Education_Committee_on_National_Statistics_Board_on_Behavioral_Cognitive_and_Sensory_Sciences_Committee_on_Reproducibility_and_Replicability_in_Science2019-tk,
  title     = "Reproducibility and Replicability in Science",
  author    = "{National Academies of Sciences, Engineering, and Medicine;
               Policy and Global Affairs; Committee on Science, Engineering,
               Medicine, and Public Policy; Board on Research Data and
               Information; Division on Engineering and Physical Sciences;
               Committee on Applied and Theoretical Statistics; Board on
               Mathematical Sciences and Analytics; Division on Earth and Life
               Studies; Nuclear and Radiation Studies Board; Division of
               Behavioral and Social Sciences and Education; Committee on
               National Statistics; Board on Behavioral, Cognitive, and Sensory
               Sciences; Committee on Reproducibility and Replicability in
               Science}",
  abstract  = "One of the pathways by which the scientific community confirms
               the validity of a new scientific discovery is by repeating the
               research that produced it. When a scientific effort fails to
               independently confirm the computations or results of a previous
               study, some fear that it may be a symptom of a lack of rigor in
               science, while others argue that such an observed inconsistency
               can be an important precursor to new discovery. Concerns about
               reproducibility and replicability have been expressed in both
               scientific and popular media. As these concerns came to light,
               Congress requested that the National Academies of Sciences,
               Engineering, and Medicine conduct a study to assess the extent
               of issues related to reproducibility and replicability and to
               offer recommendations for improving rigor and transparency in
               scientific research. Reproducibility and Replicability in
               Science defines reproducibility and replicability and examines
               the factors that may lead to non-reproducibility and
               non-replicability in research. Unlike the typical expectation of
               reproducibility between two computations, expectations about
               replicability are more nuanced, and in some cases a lack of
               replicability can aid the process of scientific discovery. This
               report provides recommendations to researchers, academic
               institutions, journals, and funders on steps they can take to
               improve reproducibility and replicability in science.",
  publisher = "National Academies Press (US)",
  month     =  oct,
  year      =  2019,
  url       = "http://dx.doi.org/10.17226/25303",
  address   = "Washington (DC)",
  isbn      = "9780309486163",
  doi       = "10.17226/25303"
}

@ARTICLE{Freire2012-do,
  title     = "Making Computations and Publications Reproducible with
               {VisTrails}",
  author    = "Freire, Juliana",
  abstract  = "The VisTrails system supports the creation of reproducible
               experiments. VisTrails integrates data acquisition, derivation,
               analysis, and visualization as executable components throughout
               the scientific exploration process, and through systematic
               provenance capture, it makes it easier to generate and share
               reproducible results. Using VisTrails, authors can link results
               to their provenance, reviewers can assess the experiment's
               validity, and readers can repeat and utilize the computations.",
  journal   = "Computing in science \& engineering",
  publisher = "IEEE Computer Society",
  volume    =  14,
  number    =  4,
  pages     = "18--25",
  month     =  jul,
  year      =  2012,
  url       = "https://aip.scitation.org/doi/abs/10.1109/MCSE.2012.76",
  annote    = "doi: 10.1109/MCSE.2012.76",
  issn      = "1521-9615",
  doi       = "10.1109/MCSE.2012.76"
}

@INPROCEEDINGS{Gil2015-vl,
  title     = "{OntoSoft}: Capturing Scientific Software Metadata",
  booktitle = "Proceedings of the 8th International Conference on Knowledge
               Capture",
  author    = "Gil, Yolanda and Ratnakar, Varun and Garijo, Daniel",
  publisher = "ACM",
  pages     = "32",
  month     =  oct,
  year      =  2015,
  url       = "http://dl.acm.org/citation.cfm?doid=2815833.2816955",
  keywords  = "Ontologies; knowledge capture; software reuse",
  isbn      = "9781450338493",
  doi       = "10.1145/2815833.2816955"
}

@INPROCEEDINGS{Bouthillier2019-yq,
  title     = "Unreproducible Research is Reproducible",
  booktitle = "Proceedings of the 36th International Conference on Machine
               Learning",
  author    = "Bouthillier, Xavier and Laurent, C{\'e}sar and Vincent, Pascal",
  editor    = "Chaudhuri, Kamalika and Salakhutdinov, Ruslan",
  abstract  = "The apparent contradiction in the title is a wordplay on the
               different meanings attributed to the word reproducible across
               different scientific fields. What we imply is that
               unreproducible findings can be built upon reproducible methods.
               Without denying the importance of facilitating the reproduction
               of methods, we deem important to reassert that reproduction of
               findings is a fundamental step of the scientific inquiry. We
               argue that the commendable quest towards easy deterministic
               reproducibility of methods and numerical results should not have
               us forget the even more important necessity of ensuring the
               reproducibility of empirical findings and conclusions by
               properly accounting for essential sources of variations. We
               provide experiments to exemplify the brittleness of current
               common practice in the evaluation of models in the field of deep
               learning, showing that even if the results could be reproduced,
               a slightly different experiment would not support the findings.
               We hope to help clarify the distinction between exploratory and
               empirical research in the field of deep learning and believe
               more energy should be devoted to proper empirical research in
               our community. This work is an attempt to promote the use of
               more rigorous and diversified methodologies. It is not an
               attempt to impose a new methodology and it is not a critique on
               the nature of exploratory research.",
  publisher = "PMLR",
  volume    =  97,
  pages     = "725--734",
  series    = "Proceedings of Machine Learning Research",
  year      =  2019,
  url       = "http://proceedings.mlr.press/v97/bouthillier19a.html",
  address   = "Long Beach, California, USA"
}

@ARTICLE{Ciccarese2008-qa,
  title       = "The {SWAN} biomedical discourse ontology",
  author      = "Ciccarese, Paolo and Wu, Elizabeth and Wong, Gwen and Ocana,
                 Marco and Kinoshita, June and Ruttenberg, Alan and Clark, Tim",
  affiliation = "Massachusetts General Hospital, Boston, MA 02129, USA; Harvard
                 Medical School, Boston, MA 02115, USA.",
  abstract    = "Developing cures for highly complex diseases, such as
                 neurodegenerative disorders, requires extensive
                 interdisciplinary collaboration and exchange of biomedical
                 information in context. Our ability to exchange such
                 information across sub-specialties today is limited by the
                 current scientific knowledge ecosystem's inability to properly
                 contextualize and integrate data and discourse in
                 machine-interpretable form. This inherently limits the
                 productivity of research and the progress toward cures for
                 devastating diseases such as Alzheimer's and Parkinson's. SWAN
                 (Semantic Web Applications in Neuromedicine) is an
                 interdisciplinary project to develop a practical, common,
                 semantically structured, framework for biomedical discourse
                 initially applied, but not limited, to significant problems in
                 Alzheimer Disease (AD) research. The SWAN ontology has been
                 developed in the context of building a series of applications
                 for biomedical researchers, as well as in extensive
                 discussions and collaborations with the larger bio-ontologies
                 community. In this paper, we present and discuss the SWAN
                 ontology of biomedical discourse. We ground its development
                 theoretically, present its design approach, explain its main
                 classes and their application, and show its relationship to
                 other ongoing activities in biomedicine and bio-ontologies.",
  journal     = "Journal of biomedical informatics",
  publisher   = "Elsevier",
  volume      =  41,
  number      =  5,
  pages       = "739--751",
  month       =  oct,
  year        =  2008,
  url         = "http://dx.doi.org/10.1016/j.jbi.2008.04.010",
  keywords    = "APIs \& Semantic ontologies",
  language    = "en",
  issn        = "1532-0464, 1532-0480",
  pmid        = "18583197",
  doi         = "10.1016/j.jbi.2008.04.010",
  pmc         = "PMC4536833"
}

@ARTICLE{Rousidis2014-qz,
  title     = "Metadata for Big Data: a preliminary investigation of metadata
               quality issues in research data repositories",
  author    = "Rousidis, Dimitris and Garoufallou, Emmanouel and Balatsoukas,
               Panos and Sicilia, Miguel-Angel",
  journal   = "Information services \& use",
  publisher = "IOS Press",
  volume    =  34,
  number    = "3-4",
  pages     = "279--286",
  year      =  2014,
  url       = "https://content.iospress.com/articles/information-services-and-use/isu746",
  issn      = "0167-5265"
}

@ARTICLE{Dozmorov2018-fo,
  title    = "{GitHub} statistics as a measure of the impact of open-source
              bioinformatics software",
  author   = "Dozmorov, Mikhail",
  abstract = "Modern research is increasingly data-driven and reliant on
              bioinformatics software. Publication is a common way of
              introducing new software, but not all bioinformatics tools get
              published. Giving there are competing tools, it is important not
              merely to find the appropriate software, but have a metric for
              judging its usefulness. Journal's impact factor has been shown to
              be a poor predictor of software popularity; consequently,
              focusing on publications in high-impact journals limits user's
              choices in finding useful bioinformatics tools. Free and open
              source software repositories on popular code sharing platforms
              such as GitHub provide another venue to follow the latest
              bioinformatics trends. The open source component of GitHub allows
              users to bookmark and copy repositories that are most useful to
              them. This Perspective aims to demonstrate the utility of GitHub
              ``stars,'' ``watchers,'' and ``forks'' (GitHub statistics) as a
              measure of software impact. We compiled lists of impactful
              bioinformatics software and analyzed commonly used impact metrics
              and GitHub statistics of 50 genomics-oriented bioinformatics
              tools. We present examples of community-selected best
              bioinformatics resources and show that GitHub statistics are
              distinct from the journal's impact factor (JIF), citation counts,
              and alternative metrics (Altmetrics, CiteScore) in capturing the
              level of community attention. We suggest the use of GitHub
              statistics as an unbiased measure of the usability of
              bioinformatics software complementing the traditional impact
              metrics.",
  journal  = "Frontiers in Bioengineering and Biotechnology",
  volume   =  6,
  pages    = "198",
  year     =  2018,
  url      = "https://www.frontiersin.org/article/10.3389/fbioe.2018.00198",
  issn     = "2296-4185",
  doi      = "10.3389/fbioe.2018.00198"
}

@ARTICLE{Kaushik2016-sg,
  title       = "{RABIX}: {AN} {OPEN-SOURCE} {WORKFLOW} {EXECUTOR} {SUPPORTING}
                 {RECOMPUTABILITY} {AND} {INTEROPERABILITY} {OF} {WORKFLOW}
                 {DESCRIPTIONS}",
  author      = "Kaushik, Gaurav and Ivkovic, Sinisa and Simonovic, Janko and
                 Tijanic, Nebojsa and Davis-Dusenbery, Brandi and Kural, Deniz",
  affiliation = "Seven Bridges Genomics, 1 Main Street, Cambridge, MA 02140,
                 USA*Corresponding author., gaurav@sevenbridges.com.",
  abstract    = "As biomedical data has become increasingly easy to generate in
                 large quantities, the methods used to analyze it have
                 proliferated rapidly. Reproducible and reusable methods are
                 required to learn from large volumes of data reliably. To
                 address this issue, numerous groups have developed workflow
                 specifications or execution engines, which provide a framework
                 with which to perform a sequence of analyses. One such
                 specification is the Common Workflow Language, an emerging
                 standard which provides a robust and flexible framework for
                 describing data analysis tools and workflows. In addition,
                 reproducibility can be furthered by executors or workflow
                 engines which interpret the specification and enable
                 additional features, such as error logging, file organization,
                 optim1izations to computation and job scheduling, and allow
                 for easy computing on large volumes of data. To this end, we
                 have developed the Rabix Executor, an open-source workflow
                 engine for the purposes of improving reproducibility through
                 reusability and interoperability of workflow descriptions.",
  journal     = "Pacific Symposium on Biocomputing. Pacific Symposium on
                 Biocomputing",
  publisher   = "ncbi.nlm.nih.gov",
  volume      =  22,
  pages       = "154--165",
  year        =  2016,
  url         = "https://www.ncbi.nlm.nih.gov/pubmed/27896971",
  keywords    = "pipelines \& provenance",
  language    = "en",
  issn        = "2335-6936",
  pmid        = "27896971",
  doi         = "10.1142/9789813207813\_0016",
  pmc         = "PMC5166558"
}

@ARTICLE{Greenberg2017-ca,
  title    = "Big Metadata, Smart Metadata, and Metadata Capital: Toward
              Greater Synergy Between Data Science and Metadata",
  author   = "Greenberg, Jane",
  abstract = "The purpose of the paper is to provide a framework for addressing
              the disconnect between metadata and data science. Data science
              cannot progress without metadata research. This paper takes steps
              toward advancing the synergy between metadata and data science,
              and identifies pathways for developing a more cohesive metadata
              research agenda in data science.",
  journal  = "Journal of Data and Information Science",
  volume   =  2,
  number   =  3,
  pages    = "193",
  month    =  jan,
  year     =  2017,
  url      = "http://www.degruyter.com/view/j/jdis.2017.2.issue-3/jdis-2017-0012/jdis-2017-0012.xml",
  keywords = "metadata",
  issn     = "2543-683X",
  doi      = "10.1515/jdis-2017-0012"
}

@ARTICLE{Paskin2010-cw,
  title     = "Digital object identifier ({DOI\textregistered{}}) system",
  author    = "Paskin, Norman",
  journal   = "Encyclopedia of library and information sciences",
  publisher = "Taylor \& Francis England",
  volume    =  3,
  pages     = "1586--1592",
  year      =  2010,
  url       = "http://0-www.doi.org.library.touro.edu/overview/DOI-ELIS-Paskin.pdf"
}

@ARTICLE{Halpern2015-aq,
  title     = "Cumulative human impacts: Supplementary data",
  author    = "Halpern, Benjamin and Frazier, Melanie and Potapenko, John and
               Casey, Kenneth and Koenig, Kellee and {others}",
  abstract  = "Metacat UI",
  publisher = "Knowledge Network for Biocomplexity",
  year      =  2015,
  url       = "https://knb.ecoinformatics.org/view/doi:10.5063/F19Z92TW"
}

@INPROCEEDINGS{Brickley2019-dd,
  title     = "Google Dataset Search: Building a search engine for datasets in
               an open Web ecosystem",
  booktitle = "The World Wide Web Conference",
  author    = "Brickley, Dan and Burgess, Matthew and Noy, Natasha",
  publisher = "Association for Computing Machinery",
  pages     = "1365--1375",
  series    = "WWW '19",
  month     =  may,
  year      =  2019,
  url       = "https://doi.org/10.1145/3308558.3313685",
  address   = "New York, NY, USA",
  keywords  = "structured data, metadata, search, data discovery",
  location  = "San Francisco, CA, USA",
  isbn      = "9781450366748",
  doi       = "10.1145/3308558.3313685"
}

@ARTICLE{Leipzig2016-cg,
  title    = "A review of bioinformatic pipeline frameworks",
  author   = "Leipzig, Jeremy",
  abstract = "High-throughput bioinformatic analyses increasingly rely on
              pipeline frameworks to process sequence and metadata. Modern
              implementations of these frameworks differ on three key
              dimensions: using an implicit or explicit syntax, using a
              configuration, convention or class-based design paradigm and
              offering a command line or workbench interface. Here I survey and
              compare the design philosophies of several current pipeline
              frameworks. I provide practical recommendations based on analysis
              requirements and the user base.",
  journal  = "Briefings in bioinformatics",
  month    =  mar,
  year     =  2016,
  url      = "http://dx.doi.org/10.1093/bib/bbw020",
  keywords = "framework; pipeline; workflow",
  issn     = "1467-5463, 1477-4054",
  pmid     = "27013646",
  doi      = "10.1093/bib/bbw020"
}

@ARTICLE{Bernstein2017-bc,
  title       = "{MetaSRA}: normalized human sample-specific metadata for the
                 Sequence Read Archive",
  author      = "Bernstein, Matthew N and Doan, Anhai and Dewey, Colin N",
  affiliation = "Department of Computer Sciences. Department of Biostatistics
                 and Medical Informatics, University of Wisconsin, Madison, WI
                 53706, USA.",
  abstract    = "Motivation: The NCBI's Sequence Read Archive (SRA) promises
                 great biological insight if one could analyze the data in the
                 aggregate; however, the data remain largely underutilized, in
                 part, due to the poor structure of the metadata associated
                 with each sample. The rules governing submissions to the SRA
                 do not dictate a standardized set of terms that should be used
                 to describe the biological samples from which the sequencing
                 data are derived. As a result, the metadata include many
                 synonyms, spelling variants and references to outside sources
                 of information. Furthermore, manual annotation of the data
                 remains intractable due to the large number of samples in the
                 archive. For these reasons, it has been difficult to perform
                 large-scale analyses that study the relationships between
                 biomolecular processes and phenotype across diverse diseases,
                 tissues and cell types present in the SRA. Results: We present
                 MetaSRA, a database of normalized SRA human sample-specific
                 metadata following a schema inspired by the metadata
                 organization of the ENCODE project. This schema involves
                 mapping samples to terms in biomedical ontologies, labeling
                 each sample with a sample-type category, and extracting
                 real-valued properties. We automated these tasks via a novel
                 computational pipeline. Availability and implementation: The
                 MetaSRA is available at metasra.biostat.wisc.edu via both a
                 searchable web interface and bulk downloads. Software
                 implementing our computational pipeline is available at
                 http://github.com/deweylab/metasra-pipeline. Contact:
                 cdewey@biostat.wisc.edu. Supplementary information:
                 Supplementary data are available at Bioinformatics online.",
  journal     = "Bioinformatics",
  volume      =  33,
  number      =  18,
  pages       = "2914--2923",
  month       =  sep,
  year        =  2017,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btx334",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "28535296",
  doi         = "10.1093/bioinformatics/btx334",
  pmc         = "PMC5870770"
}

@INPROCEEDINGS{Carragain2019-ql,
  title     = "A lightweight approach to research object data packaging",
  booktitle = "Bioinformatics Open Source Conference ({BOSC}) 2019",
  author    = "Carrag{\'a}in, Eoghan {\'O} and Goble, Carole and Sefton, Peter
               and Soiland-Reyes, Stian",
  year      =  2019,
  url       = "https://www.research.manchester.ac.uk/portal/files/106897058/RO_Crate_BOSC2019.pdf"
}

@ARTICLE{Ison2016-hl,
  title       = "Tools and data services registry: a community effort to
                 document bioinformatics resources",
  author      = "Ison, Jon and Rapacki, Kristoffer and M{\'e}nager, Herv{\'e}
                 and Kala{\v s}, Mat{\'u}{\v s} and Rydza, Emil and Chmura,
                 Piotr and Anthon, Christian and Beard, Niall and Berka, Karel
                 and Bolser, Dan and Booth, Tim and Bretaudeau, Anthony and
                 Brezovsky, Jan and Casadio, Rita and Cesareni, Gianni and
                 Coppens, Frederik and Cornell, Michael and Cuccuru, Gianmauro
                 and Davidsen, Kristian and Vedova, Gianluca Della and Dogan,
                 Tunca and Doppelt-Azeroual, Olivia and Emery, Laura and
                 Gasteiger, Elisabeth and Gatter, Thomas and Goldberg, Tatyana
                 and Grosjean, Marie and Gr{\"u}ning, Bj{\"o}rn and
                 Helmer-Citterich, Manuela and Ienasescu, Hans and Ioannidis,
                 Vassilios and Jespersen, Martin Closter and Jimenez, Rafael
                 and Juty, Nick and Juvan, Peter and Koch, Maximilian and
                 Laibe, Camille and Li, Jing-Woei and Licata, Luana and
                 Mareuil, Fabien and Mi{\v c}eti{\'c}, Ivan and Friborg, Rune
                 M{\o}llegaard and Moretti, Sebastien and Morris, Chris and
                 M{\"o}ller, Steffen and Nenadic, Aleksandra and Peterson, Hedi
                 and Profiti, Giuseppe and Rice, Peter and Romano, Paolo and
                 Roncaglia, Paola and Saidi, Rabie and Schafferhans, Andrea and
                 Schw{\"a}mmle, Veit and Smith, Callum and Sperotto, Maria
                 Maddalena and Stockinger, Heinz and Va{\v r}ekov{\'a}, Radka
                 Svobodov{\'a} and Tosatto, Silvio C E and de la Torre, Victor
                 and Uva, Paolo and Via, Allegra and Yachdav, Guy and Zambelli,
                 Federico and Vriend, Gert and Rost, Burkhard and Parkinson,
                 Helen and L{\o}ngreen, Peter and Brunak, S{\o}ren",
  affiliation = "Center for Biological Sequence Analysis Department of Systems
                 Biology, Technical University of Denmark, Denmark
                 jison@cbs.dtu.dk. Center for Biological Sequence Analysis
                 Department of Systems Biology, Technical University of
                 Denmark, Denmark. Centre d'Informatique pour la Biologie,
                 C3BI, Institut Pasteur, France. Computational Biology Unit,
                 Department of Informatics, University of Bergen, Norway.
                 Department of Veterinary Clinical and Animal Sciences, Faculty
                 for Health and Medical Sciences, University of Copenhagen,
                 Denmark. School of Computer Science, University of Manchester,
                 UK. Department of Physical Chemistry, RCPTM, Faculty of
                 Science, Palacky University, Czech Republic. The European
                 Bioinformatics Institute (EMBL-EBI), UK. NEBC Wallingford,
                 Centre for Ecology and Hydrology, UK. INRA, UMR Institut de
                 G{\'e}n{\'e}tique, Environnement et Protection des Plantes
                 (IGEPP), BioInformatics Platform for Agroecosystems Arthropods
                 (BIPAA), France INRIA, IRISA, GenOuest Core Facility, France.
                 Loschmidt Laboratories, Department of Experimental Biology and
                 Research Centre for Toxic Compounds in the Environment
                 RECETOX, Masaryk University, Czech Republic. Bologna
                 Biocomputing Group, University of Bologna, Italy. Dept. of
                 Biology, University of Rome Tor Vergata, Italy. Department of
                 Plant Systems Biology, VIB, Belgium Department of Plant
                 Biotechnology and Bioinformatics, Ghent University, Belgium.
                 Faculty of Life Sciences, University of Manchester, UK.
                 Bioinformatics, CRS4, Italy. Dept. of Computer Science,
                 Systems and Communication. Univ. Milano-Bicocca, Italy.
                 UniProt, European Bioinformatics Institute (EMBL-EBI), UK. SIB
                 Swiss Institute of Bioinformatics, Switzerland. Faculty of
                 Technology and Center for Biotechnology, Universit{\"a}t
                 Bielefeld, Germany. Department of Informatics,
                 Bioinformatics-I12, TUM, Germany. Institut Fran{\c c}ais de
                 Bioinformatique (French Institute of Bioinformatics), CNRS,
                 UMS3601, France. Albert-Ludwigs-Universit{\"a}t Freiburg,
                 Fahnenbergplatz, 79085 Freiburg. Centre for Molecular
                 Bioinformatics, Dept. of Biology, University of Rome Tor
                 Vergata, Italy. Bioinformatics Centre, Department of Biology,
                 University of Copenhagen, Denmark. Centre for Functional
                 Genomics and Biochips, Faculty of Medicine, University of
                 Ljubljana, Slovenia. Faculty of Medicine, The Chinese
                 University of Hong Kong, China Hong Kong Bioinformatics
                 Centre, School of Life Sciences,The Chinese University of Hong
                 Kong, China. Department of Biomedical Sciences, University of
                 Padua, Italy. Bioinformatics Research Centre (BiRC), Denmark.
                 SIB Swiss Institute of Bioinformatics, Switzerland Department
                 of Ecology and Evolution, Biophore, Evolutionary
                 Bioinformatics group, University of Lausanne, Switzerland.
                 STFC, Daresbury Laboratory, UK. Department of Dermatology,
                 University of L{\"u}beck, Germany Institute for Biostatistics
                 and Informatics in Medicine and Ageing Research, Rostock
                 University Medical Center, Germany. Institute of Computer
                 Science, University of Tartu, Estonia. Department of
                 Computing, William Penney Laboratory, Imperial College London,
                 UK. IRCCS AOU San Martino IST, Italy. Protein Research Group,
                 Department for Biochemistry and Molecular Biology, University
                 of Southern Denmark, Denmark. Instruct, WTCHG, UK. Central
                 European Institute of Technology (CEITEC), Czech Republic.
                 National Bioinformatics Institute Unit (INB), Fundacion Centro
                 Nacional de Investigaciones Oncologicas, Spain. Dept. of
                 Physics, Sapienza University, Italy. Institute of Biomembranes
                 and Bioenergetics, National Research Council (CNR), and Dept.
                 of Biosciences, University of Milano, Italy. Radboud
                 University Medical Centre, CMBI, Netherlands. Center for
                 Biological Sequence Analysis Department of Systems Biology,
                 Technical University of Denmark, Denmark Novo Nordisk
                 Foundation Center for Protein Research, Faculty of Health and
                 Medical Sciences, University of Copenhagen, Denmark.",
  abstract    = "Life sciences are yielding huge data sets that underpin
                 scientific discoveries fundamental to improvement in human
                 health, agriculture and the environment. In support of these
                 discoveries, a plethora of databases and tools are deployed,
                 in technically complex and diverse implementations, across a
                 spectrum of scientific disciplines. The corpus of
                 documentation of these resources is fragmented across the Web,
                 with much redundancy, and has lacked a common standard of
                 information. The outcome is that scientists must often
                 struggle to find, understand, compare and use the best
                 resources for the task at hand.Here we present a
                 community-driven curation effort, supported by ELIXIR-the
                 European infrastructure for biological information-that
                 aspires to a comprehensive and consistent registry of
                 information about bioinformatics resources. The sustainable
                 upkeep of this Tools and Data Services Registry is assured by
                 a curation effort driven by and tailored to local needs, and
                 shared amongst a network of engaged partners.As of November
                 2015, the registry includes 1785 resources, with depositions
                 from 126 individual registrations including 52 institutional
                 providers and 74 individuals. With community support, the
                 registry can become a standard for dissemination of
                 information about bioinformatics resources: we welcome
                 everyone to join us in this common endeavour. The registry is
                 freely available at https://bio.tools.",
  journal     = "Nucleic acids research",
  volume      =  44,
  number      = "D1",
  pages       = "D38--47",
  month       =  jan,
  year        =  2016,
  url         = "http://dx.doi.org/10.1093/nar/gkv1116",
  language    = "en",
  issn        = "0305-1048, 1362-4962",
  pmid        = "26538599",
  doi         = "10.1093/nar/gkv1116",
  pmc         = "PMC4702812"
}

@ARTICLE{Bidgood1992-yj,
  title       = "Introduction to the {ACR-NEMA} {DICOM} standard",
  author      = "Bidgood, Jr, W D and Horii, S C",
  affiliation = "Department of Radiology, University of Florida College of
                 Medicine, Gainesville 32610-0374.",
  abstract    = "In 1982, the American College of Radiology (ACR) and the
                 National Electrical Manufacturers Association (NEMA) formed a
                 committee to develop standards for the interconnection of
                 digital imaging devices. Version 1.0 of the standard,
                 published in 1985, specifies a hardware interface supporting
                 point-to-point (not network) image transmission, a data
                 dictionary (a set of rules for encoding information), and a
                 set of commands to initiate transactions. Version 2.0,
                 published in 1988, also addresses point-to-point image
                 transmission and provides semantic rules by which messages
                 (streams of bits representing information in transit from one
                 device to another) are organized. Version 3.0, also referred
                 to as DICOM (Digital Imaging and Communications in Medicine),
                 will be finalized in 1992. The DICOM standard encourages open
                 systems interconnection of imaging equipment over standard
                 networks, while maintaining compatibility with earlier
                 point-to-point connection standards. The DICOM standard
                 conforms fully with the International Standards Organization
                 reference model for network communications (ISORM), addresses
                 the issue of conformance, and incorporates the concept of
                 object-oriented design.",
  journal     = "Radiographics: a review publication of the Radiological
                 Society of North America, Inc",
  volume      =  12,
  number      =  2,
  pages       = "345--355",
  month       =  mar,
  year        =  1992,
  url         = "http://dx.doi.org/10.1148/radiographics.12.2.1561424",
  language    = "en",
  issn        = "0271-5333",
  pmid        = "1561424",
  doi         = "10.1148/radiographics.12.2.1561424"
}

@ARTICLE{Barba2018-qv,
  title         = "Terminologies for Reproducible Research",
  author        = "Barba, Lorena A",
  abstract      = "Reproducible research---by its many names---has come to be
                   regarded as a key concern across disciplines and stakeholder
                   groups. Funding agencies and journals, professional
                   societies and even mass media are paying attention, often
                   focusing on the so-called ``crisis'' of reproducibility. One
                   big problem keeps coming up among those seeking to tackle
                   the issue: different groups are using terminologies in utter
                   contradiction with each other. Looking at a broad sample of
                   publications in different fields, we can classify their
                   terminology via decision tree: they either, A---make no
                   distinction between the words reproduce and replicate, or
                   B---use them distinctly. If B, then they are commonly
                   divided in two camps. In a spectrum of concerns that starts
                   at a minimum standard of ``same data+same methods=same
                   results,'' to ``new data and/or new methods in an
                   independent study=same findings,'' group 1 calls the minimum
                   standard reproduce, while group 2 calls it replicate. This
                   direct swap of the two terms aggravates an already weighty
                   issue. By attempting to inventory the terminologies across
                   disciplines, I hope that some patterns will emerge to help
                   us resolve the contradictions.",
  month         =  feb,
  year          =  2018,
  url           = "http://arxiv.org/abs/1802.03311",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "1802.03311",
  primaryClass  = "cs.DL",
  arxivid       = "1802.03311"
}

@INPROCEEDINGS{Jupyter2018-nx,
  title     = "Binder 2.0-Reproducible, interactive, sharable environments for
               science at scale",
  booktitle = "Proceedings of the 17th python in science conference",
  author    = "Jupyter, P and Bussonnier, M and Forde, J and Freeman, J and
               Granger, B and Head, T and Holdgraf, C and Kelley, K and
               Nalvarte, G and Osheroff, A and {Others}",
  volume    =  113,
  pages     = "120",
  year      =  2018
}

@ARTICLE{Bourne2015-ei,
  title       = "{DOIs} for {DICOM} raw images: enabling science
                 reproducibility",
  author      = "Bourne, Philip E",
  affiliation = "From the Office of the Director, the National Institutes of
                 Health, 1 Center Dr, Building 1, Room 228, Bethesda, MD 20892.",
  journal     = "Radiology",
  volume      =  275,
  number      =  1,
  pages       = "3--4",
  month       =  apr,
  year        =  2015,
  url         = "http://dx.doi.org/10.1148/radiol.15150144",
  language    = "en",
  issn        = "0033-8419, 1527-1315",
  pmid        = "25799330",
  doi         = "10.1148/radiol.15150144",
  pmc         = "PMC4455617"
}

@ARTICLE{Zheng2016-cu,
  title       = "The Ontology of Biological and Clinical Statistics ({OBCS})
                 for standardized and reproducible statistical analysis",
  author      = "Zheng, Jie and Harris, Marcelline R and Masci, Anna Maria and
                 Lin, Yu and Hero, Alfred and Smith, Barry and He, Yongqun",
  affiliation = "Department of Genetics, University of Pennsylvania Perelman
                 School of Medicine, Philadelphia, PA, 19104, USA.
                 jiezheng@upenn.edu. Division of Systems Leadership and
                 Effectiveness Science, University of Michigan School of
                 Nursing, Ann Arbor, MI, 48109, USA. Department of
                 Biostatistics and Bioinformatics, Duke Medical Center, Duke
                 University, Durham, NC, 27710, USA. Department of Microbiology
                 and Immunology, Unit for Laboratory Animal Medicine,
                 University of Michigan Medical School, Ann Arbor, MI, 48109,
                 USA. Department of Electrical Engineering and Computer
                 Science, Department of Biomedical Engineering, and Department
                 of Statistics, Michigan Institute of Data Science, University
                 of Michigan, Ann Arbor, MI, 48109, USA. Department of
                 Philosophy and National Center for Ontological Research,
                 University at Buffalo, Buffalo, NY, 14203, USA. Department of
                 Microbiology and Immunology, Unit for Laboratory Animal
                 Medicine, University of Michigan Medical School, Ann Arbor,
                 MI, 48109, USA. yongqunh@med.umich.edu.",
  abstract    = "BACKGROUND: Statistics play a critical role in biological and
                 clinical research. However, most reports of scientific results
                 in the published literature make it difficult for the reader
                 to reproduce the statistical analyses performed in achieving
                 those results because they provide inadequate documentation of
                 the statistical tests and algorithms applied. The Ontology of
                 Biological and Clinical Statistics (OBCS) is put forward here
                 as a step towards solving this problem. RESULTS: The terms in
                 OBCS including 'data collection', 'data transformation in
                 statistics', 'data visualization', 'statistical data
                 analysis', and 'drawing a conclusion based on data', cover the
                 major types of statistical processes used in basic biological
                 research and clinical outcome studies. OBCS is aligned with
                 the Basic Formal Ontology (BFO) and extends the Ontology of
                 Biomedical Investigations (OBI), an OBO (Open Biological and
                 Biomedical Ontologies) Foundry ontology supported by over 20
                 research communities. Currently, OBCS comprehends 878 terms,
                 representing 20 BFO classes, 403 OBI classes, 229 OBCS
                 specific classes, and 122 classes imported from ten other OBO
                 ontologies. We discuss two examples illustrating how the
                 ontology is being applied. In the first (biological) use case,
                 we describe how OBCS was applied to represent the high
                 throughput microarray data analysis of immunological
                 transcriptional profiles in human subjects vaccinated with an
                 influenza vaccine. In the second (clinical outcomes) use case,
                 we applied OBCS to represent the processing of electronic
                 health care data to determine the associations between
                 hospital staffing levels and patient mortality. Our case
                 studies were designed to show how OBCS can be used for the
                 consistent representation of statistical analysis pipelines
                 under two different research paradigms. Other ongoing projects
                 using OBCS for statistical data processing are also discussed.
                 The OBCS source code and documentation are available at:
                 https://github.com/obcs/obcs . CONCLUSIONS: The Ontology of
                 Biological and Clinical Statistics (OBCS) is a community-based
                 open source ontology in the domain of biological and clinical
                 statistics. OBCS is a timely ontology that represents
                 statistics-related terms and their relations in a rigorous
                 fashion, facilitates standard data analysis and integration,
                 and supports reproducible biological and clinical research.",
  journal     = "Journal of biomedical semantics",
  volume      =  7,
  number      =  1,
  pages       = "53",
  month       =  sep,
  year        =  2016,
  url         = "https://doi.org/10.1186/s13326-016-0100-2",
  keywords    = "Biological statistics; Clinical outcomes statistics; Data
                 integration; OBCS; Standardization; Statistical
                 analysis;prospectus III;in prospectus;APIs \& Semantic
                 ontologies",
  language    = "en",
  issn        = "2041-1480",
  pmid        = "27627881",
  doi         = "10.1186/s13326-016-0100-2",
  pmc         = "PMC5024438"
}

@ARTICLE{Kanehisa2017-rm,
  title    = "{{KEGG}: New perspectives on genomes, pathways, diseases and
              drugs}",
  author   = "Kanehisa, M and Furumichi, M and Tanabe, M and Sato, Y and
              Morishima, K",
  journal  = "Nucleic acids research",
  volume   =  45,
  pages    = "D353",
  year     =  2017,
  keywords = "charlie\_rcr.bib",
  issn     = "0305-1048"
}

@ARTICLE{Shen2014-tn,
  title    = "Interactive notebooks: Sharing the code",
  author   = "Shen, Helen",
  journal  = "Nature",
  volume   =  515,
  number   =  7525,
  pages    = "151--152",
  month    =  nov,
  year     =  2014,
  url      = "http://dx.doi.org/10.1038/515151a",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "25373681",
  doi      = "10.1038/515151a"
}

@MISC{Pimentel2019-mi,
  title   = "A Survey on Collecting, Managing, and Analyzing Provenance from
             Scripts",
  author  = "Pimentel, Jo{\~a}o Felipe and Freire, Juliana and Murta, Leonardo
             and Braganholo, Vanessa",
  journal = "ACM Computing Surveys",
  volume  =  52,
  number  =  3,
  pages   = "1--38",
  year    =  2019,
  url     = "http://dx.doi.org/10.1145/3311955",
  doi     = "10.1145/3311955"
}

@INPROCEEDINGS{Schelter2017-by,
  title     = "Automatically tracking metadata and provenance of machine
               learning experiments",
  booktitle = "Machine Learning Systems Workshop at {NIPS}",
  author    = "Schelter, Sebastian and Boese, Joos-Hendrik and Kirschnick,
               Johannes and Klein, Thoralf and Seufert, Stephan",
  pages     = "27--29",
  year      =  2017,
  url       = "http://learningsys.org/nips17/assets/papers/paper_13.pdf"
}

@ARTICLE{Hung2019-ul,
  title       = "Building Containerized Workflows Using the
                 {BioDepot-Workflow-Builder}",
  author      = "Hung, Ling-Hong and Hu, Jiaming and Meiss, Trevor and
                 Ingersoll, Alyssa and Lloyd, Wes and Kristiyanto, Daniel and
                 Xiong, Yuguang and Sobie, Eric and Yeung, Ka Yee",
  affiliation = "School of Engineering and Technology, University of
                 Washington, Tacoma, WA 98402, USA. Icahn School of Medicine at
                 Mount Sinai, 1468 Madison Ave, New York, NY 10029, USA. School
                 of Engineering and Technology, University of Washington,
                 Tacoma, WA 98402, USA. Electronic address: kayee@uw.edu.",
  abstract    = "We present the BioDepot-workflow-builder (Bwb), a software
                 tool that allows users to create and execute reproducible
                 bioinformatics workflows using a drag-and-drop interface.
                 Graphical widgets represent Docker containers executing a
                 modular task. Widgets are linked graphically to build
                 bioinformatics workflows that can be reproducibly deployed
                 across different local and cloud platforms. Each widget
                 contains a form-based user interface to facilitate parameter
                 entry and a console to display intermediate results. Bwb
                 provides tools for rapid customization of widgets, containers,
                 and workflows. Saved workflows can be shared using Bwb's
                 native format or exported as shell scripts.",
  journal     = "Cell systems",
  volume      =  9,
  number      =  5,
  pages       = "508--514.e3",
  month       =  nov,
  year        =  2019,
  url         = "http://dx.doi.org/10.1016/j.cels.2019.08.007",
  keywords    = "Docker; RNA sequencing; bioinformatics workflows;
                 reproducibility of research; software development",
  language    = "en",
  issn        = "2405-4720, 2405-4712",
  pmid        = "31521606",
  doi         = "10.1016/j.cels.2019.08.007",
  pmc         = "PMC6883158"
}

@MISC{Leipzig2019-am,
  title  = "Awesome Reproducible Research",
  author = "Leipzig, Jeremy",
  month  =  dec,
  year   =  2019,
  url    = "https://zenodo.org/record/3564746",
  doi    = "10.5281/zenodo.3564746"
}

@ARTICLE{Smith2016-hi,
  title     = "Software citation principles",
  author    = "Smith, Arfon M and Katz, Daniel S and Niemeyer, Kyle E",
  abstract  = "Software is a critical part of modern research and yet there is
               little support across the scholarly ecosystem for its
               acknowledgement and citation. Inspired by the activities of the
               FORCE11 working group focused on data citation, this document
               summarizes the recommendations of the FORCE11 Software Citation
               Working Group and its activities between June 2015 and April
               2016. Based on a review of existing community practices, the
               goal of the working group was to produce a consolidated set of
               citation principles that may encourage broad adoption of a
               consistent policy for software citation across disciplines and
               venues. Our work is presented here as a set of software citation
               principles, a discussion of the motivations for developing the
               principles, reviews of existing community practice, and a
               discussion of the requirements these principles would place upon
               different stakeholders. Working examples and possible technical
               solutions for how these principles can be implemented will be
               discussed in a separate paper.",
  journal   = "PeerJ Computer Science",
  publisher = "PeerJ Inc.",
  volume    =  2,
  pages     = "e86",
  month     =  sep,
  year      =  2016,
  url       = "https://peerj.com/articles/cs-86/",
  keywords  = "Software citation; Software credit; Attribution",
  language  = "en",
  issn      = "2376-5992",
  doi       = "10.7717/peerj-cs.86"
}

@ARTICLE{Motulsky2014-vg,
  title       = "Common misconceptions about data analysis and statistics",
  author      = "Motulsky, Harvey J",
  affiliation = "GraphPad Software Inc., La Jolla, California
                 hmotulsky@graphpad.com.",
  abstract    = "Ideally, any experienced investigator with the right tools
                 should be able to reproduce a finding published in a
                 peer-reviewed biomedical science journal. In fact, however,
                 the reproducibility of a large percentage of published
                 findings has been questioned. Undoubtedly, there are many
                 reasons for this, but one reason may be that investigators
                 fool themselves due to a poor understanding of statistical
                 concepts. In particular, investigators often make these
                 mistakes: 1) P-hacking, which is when you reanalyze a data set
                 in many different ways, or perhaps reanalyze with additional
                 replicates, until you get the result you want; 2) overemphasis
                 on P values rather than on the actual size of the observed
                 effect; 3) overuse of statistical hypothesis testing, and
                 being seduced by the word ``significant''; and 4)
                 over-reliance on standard errors, which are often
                 misunderstood.",
  journal     = "The Journal of pharmacology and experimental therapeutics",
  volume      =  351,
  number      =  1,
  pages       = "200--205",
  month       =  oct,
  year        =  2014,
  url         = "http://dx.doi.org/10.1124/jpet.114.219170",
  language    = "en",
  issn        = "0022-3565, 1521-0103",
  pmid        = "25204545",
  doi         = "10.1124/jpet.114.219170"
}

@ARTICLE{noauthor_2021-it,
  title     = "Announcing the next phase of Executable Research Articles",
  autor     = {Giulia Guizzardi and Nokome Bentley and Giuliano Maciocci},
  abstract  = "eLife and Stencila renew their partnership to make Executable
               Research Articles accessible to even more researchers and
               publishers.",
  journal   = "elifesciences.org",
  publisher = "eLife Sciences Publications Limited",
  month     =  mar,
  year      =  2021,
  url       = "https://elifesciences.org/labs/a04d2b80/announcing-the-next-phase-of-executable-research-articles",
  language  = "en"
}

@ARTICLE{Gentleman2004-ns,
  title       = "Bioconductor: open software development for computational
                 biology and bioinformatics",
  author      = "Gentleman, Robert C and Carey, Vincent J and Bates, Douglas M
                 and Bolstad, Ben and Dettling, Marcel and Dudoit, Sandrine and
                 Ellis, Byron and Gautier, Laurent and Ge, Yongchao and Gentry,
                 Jeff and Hornik, Kurt and Hothorn, Torsten and Huber, Wolfgang
                 and Iacus, Stefano and Irizarry, Rafael and Leisch, Friedrich
                 and Li, Cheng and Maechler, Martin and Rossini, Anthony J and
                 Sawitzki, Gunther and Smith, Colin and Smyth, Gordon and
                 Tierney, Luke and Yang, Jean Y H and Zhang, Jianhua",
  affiliation = "Department of Biostatistical Science, Dana-Farber Cancer
                 Institute, 44 Binney St, Boston, MA 02115, USA.
                 rgentlem@jimmy.harvard.edu",
  abstract    = "The Bioconductor project is an initiative for the
                 collaborative creation of extensible software for
                 computational biology and bioinformatics. The goals of the
                 project include: fostering collaborative development and
                 widespread use of innovative software, reducing barriers to
                 entry into interdisciplinary scientific research, and
                 promoting the achievement of remote reproducibility of
                 research results. We describe details of our aims and methods,
                 identify current challenges, compare Bioconductor to other
                 open bioinformatics projects, and provide working examples.",
  journal     = "Genome biology",
  volume      =  5,
  number      =  10,
  pages       = "R80",
  month       =  sep,
  year        =  2004,
  url         = "http://dx.doi.org/10.1186/gb-2004-5-10-r80",
  language    = "en",
  issn        = "1465-6906",
  pmid        = "15461798",
  doi         = "10.1186/gb-2004-5-10-r80",
  pmc         = "PMC545600"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sefton1_undated-xr,
  title    = "Introducing {RO-Crate}: research object data packaging",
  author   = "Sefton$^1$, Peter and Carrag{\'a}in, Eoghan {\'O} and Goble,
              Carole and Soiland-Reyes, Stian",
  abstract = "BACKGROUND Multiple data packaging initiatives have recently
              emerged, within the Research Data Alliance, Force11, DataOne and
              elsewhere; for example Frictionless data [8] for table-like
              files, BioCompute Objects for regulatory science [9], CodeMeta
              for software …",
  journal  = "conference.eresearch.edu.au",
  url      = "https://conference.eresearch.edu.au/wp-content/uploads/2019/08/2019-eResearch_103_-Introducing-RO-Crate-research-object-data-packaging.pdf",
  year     = {2019}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@unpublished{noauthor_2020-ah,
  title        = "{ISO/TC} 276 - Biotechnology",
  booktitle    = "{ISO}",
  abstract     = "Standardization in the field of biotechnology processes that
                  includes the following topics: Terms and definitions;
                  biobanks and bioresources; analytical methods; bioprocessing;
                  data processing including annotation, analysis, validation,
                  comparability and integration; metrology. ISO/TC 276 Biotechnology will work closely with related committes in order to identify standardization needs and gaps, and collaborate with other organisations to avoid duplications and overlapping standardization activities. The committee will not pursue subjects within the scope of other TCs including but not limited to ISO/TC 212 and ISO/TC 34/SC 16.",
  year         =  2020,
  url          = "https://www.iso.org/committee/4514241.html",
  howpublished = "\url{https://www.iso.org/committee/4514241.html}",
  note         = "Accessed: 2020-5-21"
}

@ARTICLE{Slater2014-qz,
  title       = "Recent advances in modeling languages for pathway maps and
                 computable biological networks",
  author      = "Slater, Ted",
  affiliation = "OpenBEL Consortium, One Alewife Center, Suite 100, Cambridge,
                 MA 02140, USA. Electronic address: ted@openbel.org.",
  abstract    = "As our theories of systems biology grow more sophisticated,
                 the models we use to represent them become larger and more
                 complex. Languages necessarily have the expressivity and
                 flexibility required to represent these models in ways that
                 support high-resolution annotation, and provide for simulation
                 and analysis that are sophisticated enough to allow
                 researchers to master their data in the proper context. These
                 languages also need to facilitate model sharing and
                 collaboration, which is currently best done by using uniform
                 data structures (such as graphs) and language standards. In
                 this brief review, we discuss three of the most recent systems
                 biology modeling languages to appear: BEL, PySB and BCML, and
                 examine how they meet these needs.",
  journal     = "Drug discovery today",
  volume      =  19,
  number      =  2,
  pages       = "193--198",
  month       =  feb,
  year        =  2014,
  url         = "http://dx.doi.org/10.1016/j.drudis.2013.12.011",
  language    = "en",
  issn        = "1359-6446, 1878-5832",
  pmid        = "24444544",
  doi         = "10.1016/j.drudis.2013.12.011"
}

@ARTICLE{Rocca-Serra2010-vz,
  title       = "{ISA} software suite: supporting standards-compliant
                 experimental annotation and enabling curation at the community
                 level",
  author      = "Rocca-Serra, Philippe and Brandizi, Marco and Maguire, Eamonn
                 and Sklyar, Nataliya and Taylor, Chris and Begley, Kimberly
                 and Field, Dawn and Harris, Stephen and Hide, Winston and
                 Hofmann, Oliver and Neumann, Steffen and Sterk, Peter and
                 Tong, Weida and Sansone, Susanna-Assunta",
  affiliation = "The European Bioinformatics Institute, Wellcome Trust Genome
                 Campus, Cambridge, UK.",
  abstract    = "UNLABELLED: The first open source software suite for
                 experimentalists and curators that (i) assists in the
                 annotation and local management of experimental metadata from
                 high-throughput studies employing one or a combination of
                 omics and other technologies; (ii) empowers users to uptake
                 community-defined checklists and ontologies; and (iii)
                 facilitates submission to international public repositories.
                 AVAILABILITY AND IMPLEMENTATION: Software, documentation, case
                 studies and implementations at http://www.isa-tools.org.",
  journal     = "Bioinformatics",
  volume      =  26,
  number      =  18,
  pages       = "2354--2356",
  month       =  sep,
  year        =  2010,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btq415",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "20679334",
  doi         = "10.1093/bioinformatics/btq415",
  pmc         = "PMC2935443"
}

@ARTICLE{Zhang2003-fh,
  title     = "Data preparation for data mining",
  author    = "Zhang, Shichao and Zhang, Chengqi and Yang, Qiang",
  abstract  = "Data preparation is a fundamental stage of data analysis. While
               a lot of low-quality information is available in various data
               sources and on the Web, many organizations or companies are
               interested in how to transform the data into cleaned forms which
               can be used for high-profit purposes. This goal generates an
               urgent need for data analysis aimed at cleaning the raw data. In
               this paper, we first show the importance of data preparation in
               data analysis, then introduce some research achievements in the
               area of data preparation. Finally, we suggest some future
               directions of research and development.",
  journal   = "Applied artificial intelligence: AAI",
  publisher = "Taylor \& Francis",
  volume    =  17,
  number    = "5-6",
  pages     = "375--381",
  month     =  may,
  year      =  2003,
  url       = "https://doi.org/10.1080/713827180",
  annote    = "doi: 10.1080/713827180",
  issn      = "0883-9514",
  doi       = "10.1080/713827180"
}

@INPROCEEDINGS{Wang2019-dx,
  title     = "Artificial Intelligence for Data Discovery and Reuse Demands
               Healthy Data Ecosystem and Community Efforts",
  booktitle = "Proceedings of the Conference on Artificial Intelligence for
               Data Discovery and Reuse",
  author    = "Wang, Huajin and Webster, Keith",
  year      =  2019,
  url       = "https://pdfs.semanticscholar.org/0e83/d75f08c2bd6270efb3c559ac2326c4a04df1.pdf"
}

@INPROCEEDINGS{Dumontier2014-ea,
  title     = "{Bio2RDF} release 3: a larger connected network of linked data
               for the life sciences",
  booktitle = "Proceedings of the 2014 International Conference on Posters \&
               Demonstrations Track",
  author    = "Dumontier, Michel and Callahan, Alison and Cruz-Toledo, Jose and
               Ansell, Peter and Emonet, Vincent and Belleau, Fran{\c c}ois and
               Droit, Arnaud",
  volume    =  1272,
  pages     = "401--404",
  year      =  2014,
  url       = "https://pdfs.semanticscholar.org/0748/362466f02bf939e9a58a79062cbd73c7c71e.pdf"
}

@MISC{Rauh_undated-ej,
  title  = "Reproducible and Transparent Research Practices in Published
            Neurology Research",
  author = "Rauh, Shelby and Torgerson, Trevor and Johnson, Austin L and
            Pollard, Jonathan and Tritz, Daniel and Vassar, Matt",
  url    = "http://dx.doi.org/10.1101/763730",
  doi    = "10.1101/763730"
}

@ARTICLE{Capadisli2015-nf,
  title     = "Linked {SDMX} data",
  author    = "Capadisli, Sarven and Auer, S{\"o}ren and Ngonga Ngomo,
               Axel-Cyrille",
  journal   = "Semantic Web",
  publisher = "IOS Press",
  volume    =  6,
  number    =  2,
  pages     = "105--112",
  year      =  2015,
  url       = "https://content.iospress.com/articles/semantic-web/sw123"
}

@ARTICLE{Love2020-ce,
  title       = "Tximeta: Reference sequence checksums for provenance
                 identification in {RNA-seq}",
  author      = "Love, Michael I and Soneson, Charlotte and Hickey, Peter F and
                 Johnson, Lisa K and Pierce, N Tessa and Shepherd, Lori and
                 Morgan, Martin and Patro, Rob",
  affiliation = "Department of Biostatistics, University of North
                 Carolina-Chapel Hill, Chapel Hill, North Carolina, United
                 States of America. Department of Genetics, University of North
                 Carolina-Chapel Hill, Chapel Hill, North Carolina, United
                 States of America. Friedrich Miescher Institute for Biomedical
                 Research, Basel, Switzerland. SIB Swiss Institute of
                 Bioinformatics, Basel, Switzerland. Epigenetics and
                 Development Division, The Walter and Eliza Hall Institute of
                 Medical Research, Parkville, Victoria, Australia. The
                 Department of Medical Biology, University of Melbourne,
                 Parkville, Victoria, Australia. Department of Population
                 Health and Reproduction, University of California, Davis,
                 Davis, California, United States of America. Roswell Park
                 Comprehensive Cancer Center, Buffalo, New York, United States
                 of America. Department of Computer Science, University of
                 Maryland, College Park, Maryland, United States of America.",
  abstract    = "Correct annotation metadata is critical for reproducible and
                 accurate RNA-seq analysis. When files are shared publicly or
                 among collaborators with incorrect or missing annotation
                 metadata, it becomes difficult or impossible to reproduce
                 bioinformatic analyses from raw data. It also makes it more
                 difficult to locate the transcriptomic features, such as
                 transcripts or genes, in their proper genomic context, which
                 is necessary for overlapping expression data with other
                 datasets. We provide a solution in the form of an
                 R/Bioconductor package tximeta that performs numerous
                 annotation and metadata gathering tasks automatically on
                 behalf of users during the import of transcript quantification
                 files. The correct reference transcriptome is identified via a
                 hashed checksum stored in the quantification output, and key
                 transcript databases are downloaded and cached locally. The
                 computational paradigm of automatically adding annotation
                 metadata based on reference sequence checksums can greatly
                 facilitate genomic workflows, by helping to reduce overhead
                 during bioinformatic analyses, preventing costly bioinformatic
                 mistakes, and promoting computational reproducibility. The
                 tximeta package is available at
                 https://bioconductor.org/packages/tximeta.",
  journal     = "PLoS computational biology",
  publisher   = "journals.plos.org",
  volume      =  16,
  number      =  2,
  pages       = "e1007664",
  month       =  feb,
  year        =  2020,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1007664",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "32097405",
  doi         = "10.1371/journal.pcbi.1007664",
  pmc         = "PMC7059966"
}

@INPROCEEDINGS{Amstutz2015-fa,
  title     = "Portable workflow and tool descriptions with the {CWL}",
  booktitle = "Bioinformatics Open Source Conference",
  author    = "Amstutz, Peter and Tijani{\'c}, Neboj{\v s}a and Soiland-Reyes,
               Stian and Kern, John and Stojanovic, Luka and Pierce, Tim and
               Chilton, John and Mikheev, Maxim and Lampa, Samuel and
               M{\'e}nager, Herv{\'e} and {Others}",
  abstract  = "Bioinformatics workflow platforms provide provenance tracking,
               execution and data management, repeatability, and an environment
               for data exploration and visualization. Example F/OSS
               bioinformatics workflow platforms include Arvados, Galaxy,
               Mobyle, iPlant DiscoveryEnvironment, Apache Taverna and Yabi.
               Each one presently represent workflows using different
               vocabularies and formats, and adding new tools requires
               different procedures for each system. Neither the description of
               the workflows nor the descriptions of the tools that power them
               are usable outside of the platforms they were written for. This
               results in duplicated effort, reduced reusability, and impedes
               collaboration. Three engineers (Peter Amstutz, John Chilton, and
               Nebojsa Tijanic) from leading bioinformatics platform teams
               (Curoverse, Galaxy Team, and Seven Bridges Genomics) and a tool
               author (Michael R. Crusoe / khmer project / Michigan State
               University) started working together at the BOSC 2014 Codefest
               with an initial focus on developing a portable means of
               representing, sharing and invoking command line tools which was
               then the basis for portable workflow descriptions. The group
               placed high value on re-using existing formats and ontologies;
               they governed themselves with a lazy consensus / do-ocracy
               approach. On March 31st, 2015 the group released their second
               draft of the Common Workflow Language specification. The
               serialized form is a YAML document that is validated by an
               Apache Avro schema and can be interpreted as an RDF graph using
               JSON-LD. The documents are also valid Wf4Ever `wfdesc'
               descriptions after a simple transformation. Future drafts will
               include the use of the EDAM ontology to describe the tools
               enabling discovery via the ELIXIR tool registry. Seven Bridges
               Genomics, the Galaxy Project, and the organization behind
               Arvados (Curoverse) have started to implement support for the
               Common Workflow Language, with interest from other projects and
               organizations like Apache Taverna, BioDatomics and the Broad
               Institute. Developers on the Galaxy Team are exploring adding
               CWL tool description support with plans to add support for the
               CWL workflow descriptions. Tool authors and other community
               members will benefit as they will only have to describe their
               tool and workflow interfaces once. This will enable scientists,
               researchers and other analysts to share their workflows and
               pipelines in an interoperable and yet human readable manner.",
  year      =  2015,
  url       = "https://www.research.manchester.ac.uk/portal/files/45797989/cwl_abstract_bosc.pdf"
}

@ARTICLE{Andersson2015-ib,
  title    = "Coordinated international action to accelerate genome-to-phenome
              with {FAANG}, the Functional Annotation of Animal Genomes project",
  author   = "Andersson, Leif and Archibald, Alan L and Bottema, Cynthia D and
              Brauning, Rudiger and Burgess, Shane C and Burt, Dave W and
              Casas, Eduardo and Cheng, Hans H and Clarke, Laura and Couldrey,
              Christine and Dalrymple, Brian P and Elsik, Christine G and
              Foissac, Sylvain and Giuffra, Elisabetta and Groenen, Martien A
              and Hayes, Ben J and Huang, Lusheng S and Khatib, Hassan and
              Kijas, James W and Kim, Heebal and Lunney, Joan K and McCarthy,
              Fiona M and McEwan, John C and Moore, Stephen and Nanduri, Bindu
              and Notredame, Cedric and Palti, Yniv and Plastow, Graham S and
              Reecy, James M and Rohrer, Gary A and Sarropoulou, Elena and
              Schmidt, Carl J and Silverstein, Jeffrey and Tellam, Ross L and
              Tixier-Boichard, Michele and Tosser-Klopp, Gwenola and Tuggle,
              Christopher K and Vilkki, Johanna and White, Stephen N and Zhao,
              Shuhong and Zhou, Huaijun and {FAANG Consortium}",
  abstract = "We describe the organization of a nascent international effort,
              the Functional Annotation of Animal Genomes (FAANG) project,
              whose aim is to produce comprehensive maps of functional elements
              in the genomes of domesticated animal species.",
  journal  = "Genome biology",
  volume   =  16,
  pages    = "57",
  month    =  mar,
  year     =  2015,
  url      = "http://dx.doi.org/10.1186/s13059-015-0622-4",
  language = "en",
  issn     = "1465-6906",
  pmid     = "25854118",
  doi      = "10.1186/s13059-015-0622-4",
  pmc      = "PMC4373242"
}

@ARTICLE{Ball2009-rd,
  title   = "Scientific data application profile scoping study report",
  author  = "Ball, Alexander",
  journal = "June 3rd",
  year    =  2009,
  url     = "http://alexball.me.uk/docs/ball2009sda/?style=plain"
}

@ARTICLE{Hillion2017-wg,
  title       = "Using bio.tools to generate and annotate workbench tool
                 descriptions",
  author      = "Hillion, Kenzo-Hugo and Kuzmin, Ivan and Khodak, Anton and
                 Rasche, Eric and Crusoe, Michael and Peterson, Hedi and Ison,
                 Jon and M{\'e}nager, Herv{\'e}",
  affiliation = "Bioinformatics and Biostatistics HUB, Centre de
                 Bioinformatique, Biostatistique et Biologie Int{\'e}grative
                 (C3BI, USR 3756 Institut Pasteur et CNRS), Paris, France.
                 Institute of Computer Science, University of Tartu, Tartu,
                 Estonia. Igor Sikorsky Kyiv Polytechnic Institute, National
                 Technical University of Ukraine, Kyiv, Ukraine. Lehrstuhl
                 f{\"u}r Bioinformatik, Institut f{\"u}r Informatik,
                 Albert-Ludwigs-Universit{\"a}t Freiburg, Freiburg, Germany.
                 Common Workflow Language Project, Vilnius, Lithuania. DTU
                 Bioinformatics, Technical University of Denmark, Copenhagen,
                 Denmark.",
  abstract    = "Workbench and workflow systems such as Galaxy, Taverna,
                 Chipster, or Common Workflow Language (CWL)-based frameworks,
                 facilitate the access to bioinformatics tools in a
                 user-friendly, scalable and reproducible way. Still, the
                 integration of tools in such environments remains a
                 cumbersome, time consuming and error-prone process. A major
                 consequence is the incomplete or outdated description of tools
                 that are often missing important information, including
                 parameters and metadata such as publication or links to
                 documentation. ToolDog (Tool DescriptiOn Generator)
                 facilitates the integration of tools - which have been
                 registered in the ELIXIR tools registry (https://bio.tools) -
                 into workbench environments by generating tool description
                 templates. ToolDog includes two modules. The first module
                 analyses the source code of the bioinformatics software with
                 language-specific plugins, and generates a skeleton for a
                 Galaxy XML or CWL tool description. The second module is
                 dedicated to the enrichment of the generated tool description,
                 using metadata provided by bio.tools. This last module can
                 also be used on its own to complete or correct existing tool
                 descriptions with missing metadata.",
  journal     = "F1000Research",
  publisher   = "ncbi.nlm.nih.gov",
  volume      =  6,
  month       =  nov,
  year        =  2017,
  url         = "http://dx.doi.org/10.12688/f1000research.12974.1",
  keywords    = "bioinformatics; common workflow language; galaxy;
                 interoperability; registry; tool integration",
  language    = "en",
  issn        = "2046-1402",
  pmid        = "29333231",
  doi         = "10.12688/f1000research.12974.1",
  pmc         = "PMC5747335"
}

@INCOLLECTION{Peroni2014-bx,
  title     = "The Semantic Publishing and Referencing Ontologies",
  booktitle = "Semantic Web Technologies and Legal Scholarly Publishing",
  author    = "Peroni, Silvio",
  editor    = "Peroni, Silvio",
  abstract  = "One of the main research areas in semantic publishing is the
               development of semantic models that fit the requirements of
               authors and publishers. Although several models and metadata
               schemas have been developed in the past, they do not fully
               comply with the vocabulary used by publishers or they are not
               adequate for describing specific topics (e.g., characterisation
               of bibliographic citations, definition of publishing roles,
               description of publishing workflows, etc.). In this chapter I
               introduce the Semantic Publishing and Referencing (SPAR)
               Ontologies, a suite of orthogonal and complementary OWL 2 DL
               ontology modules for the creation of comprehensive
               machine-readable RDF metadata for every aspect of semantic
               publishing and referencing. In particular, I show the
               characteristics and benefits of all the SPAR ontologies, and
               support the entire discussion with several examples of Turtle
               code describing a particular reference of the legal discipline,
               namely Casanovas et al.'s ``OPJK and DILIGENT: ontology
               modelling in a distributed environment''.",
  publisher = "Springer International Publishing",
  pages     = "121--193",
  year      =  2014,
  url       = "https://doi.org/10.1007/978-3-319-04777-5_5",
  address   = "Cham",
  isbn      = "9783319047775",
  doi       = "10.1007/978-3-319-04777-5\_5"
}

@ARTICLE{Smith2007-of,
  title       = "The {OBO} Foundry: coordinated evolution of ontologies to
                 support biomedical data integration",
  author      = "Smith, Barry and Ashburner, Michael and Rosse, Cornelius and
                 Bard, Jonathan and Bug, William and Ceusters, Werner and
                 Goldberg, Louis J and Eilbeck, Karen and Ireland, Amelia and
                 Mungall, Christopher J and {OBI Consortium} and Leontis,
                 Neocles and Rocca-Serra, Philippe and Ruttenberg, Alan and
                 Sansone, Susanna-Assunta and Scheuermann, Richard H and Shah,
                 Nigam and Whetzel, Patricia L and Lewis, Suzanna",
  affiliation = "Department of Philosophy and New York State Center of
                 Excellence in Bioinformatics and Life Sciences, University at
                 Buffalo, Buffalo, New York 14203, USA. phismith@buffalo.edu",
  abstract    = "The value of any kind of data is greatly enhanced when it
                 exists in a form that allows it to be integrated with other
                 data. One approach to integration is through the annotation of
                 multiple bodies of data using common controlled vocabularies
                 or 'ontologies'. Unfortunately, the very success of this
                 approach has led to a proliferation of ontologies, which
                 itself creates obstacles to integration. The Open Biomedical
                 Ontologies (OBO) consortium is pursuing a strategy to overcome
                 this problem. Existing OBO ontologies, including the Gene
                 Ontology, are undergoing coordinated reform, and new
                 ontologies are being created on the basis of an evolving set
                 of shared principles governing ontology development. The
                 result is an expanding family of ontologies designed to be
                 interoperable and logically well formed and to incorporate
                 accurate representations of biological reality. We describe
                 this OBO Foundry initiative and provide guidelines for those
                 who might wish to become involved.",
  journal     = "Nature biotechnology",
  volume      =  25,
  number      =  11,
  pages       = "1251--1255",
  month       =  nov,
  year        =  2007,
  url         = "http://dx.doi.org/10.1038/nbt1346",
  language    = "en",
  issn        = "1087-0156",
  pmid        = "17989687",
  doi         = "10.1038/nbt1346",
  pmc         = "PMC2814061"
}

@ARTICLE{Khan2019-tl,
  title       = "Sharing interoperable workflow provenance: A review of best
                 practices and their practical application in {CWLProv}",
  author      = "Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard
                 O and Lonie, Andrew and Goble, Carole and Crusoe, Michael R",
  affiliation = "The University of Melbourne, School of Computing and
                 Information System, Doug Mcdonnell Building, Parkville,
                 Australia, 3052. Common Workflow Language Project. The
                 University of Manchester, UK.",
  abstract    = "BACKGROUND: The automation of data analysis in the form of
                 scientific workflows has become a widely adopted practice in
                 many fields of research. Computationally driven data-intensive
                 experiments using workflows enable automation, scaling,
                 adaptation, and provenance support. However, there are still
                 several challenges associated with the effective sharing,
                 publication, and reproducibility of such workflows due to the
                 incomplete capture of provenance and lack of interoperability
                 between different technical (software) platforms. RESULTS:
                 Based on best-practice recommendations identified from the
                 literature on workflow design, sharing, and publishing, we
                 define a hierarchical provenance framework to achieve
                 uniformity in provenance and support comprehensive and fully
                 re-executable workflows equipped with domain-specific
                 information. To realize this framework, we present CWLProv, a
                 standard-based format to represent any workflow-based
                 computational analysis to produce workflow output artefacts
                 that satisfy the various levels of provenance. We use open
                 source community-driven standards, interoperable workflow
                 definitions in Common Workflow Language (CWL), structured
                 provenance representation using the W3C PROV model, and
                 resource aggregation and sharing as workflow-centric research
                 objects generated along with the final outputs of a given
                 workflow enactment. We demonstrate the utility of this
                 approach through a practical implementation of CWLProv and
                 evaluation using real-life genomic workflows developed by
                 independent groups. CONCLUSIONS: The underlying principles of
                 the standards utilized by CWLProv enable semantically rich and
                 executable research objects that capture computational
                 workflows with retrospective provenance such that any platform
                 supporting CWL will be able to understand the analysis, reuse
                 the methods for partial reruns, or reproduce the analysis to
                 validate the published findings.",
  journal     = "GigaScience",
  volume      =  8,
  number      =  11,
  month       =  nov,
  year        =  2019,
  url         = "http://dx.doi.org/10.1093/gigascience/giz095",
  keywords    = "BagIt; CWL; Common Workflow Language; RO; Research Object;
                 containers; interoperability; provenance; scientific workflows",
  language    = "en",
  issn        = "2047-217X",
  pmid        = "31675414",
  doi         = "10.1093/gigascience/giz095",
  pmc         = "PMC6824458"
}

@ARTICLE{Valenzuela-Escarcega2018-ue,
  title    = "{Large-scale automated machine reading discovers new
              cancer-driving mechanisms}",
  author   = "Valenzuela-Esc{\'a}rcega, M A and Babur, {\"O} and Hahn-Powell, G
              and Bell, D and Hicks, T and Noriega-Atala, E and Wang, X and
              Surdeanu, M and Demir, E and Morrison, C T",
  journal  = "Database: the journal of biological databases and curation",
  volume   =  2018,
  pages    = "1",
  year     =  2018,
  keywords = "charlie\_rcr.bib"
}

@techreport{Hoyle2016-lt,
  title     = "{DDI} as a Common Format for Export and Import for Statistical
               Packages",
  author    = "Hoyle, Larry and Wackerow, Joachim",
  abstract  = "One of the roles the DDI standard can perform is to serve as a
               medium for the transfer of metadata and data across both space
               and time. A crucial component of this role is the ability to
               represent the data and metadata contained in common data
               analysis and management packages. This paper describes an
               experiment using the program Stat/Transfer to move datasets
               among five popular packages with DDI Lifecycle as an
               intermediary format. We created a dataset in each of the five
               packages and then exported it to DDI Lifecycle via
               Stat/Transfer. We also created a DDI Lifecycle instance and an
               associated delimited dataset, containing as many of the metadata
               elements found in any of the five packages possible and then
               exported it to each of the packages. Success or failure to
               transfer was recorded for a set of generic metadata elements
               identified in an earlier paper. Using a commercial file transfer
               program helped identify which metadata elements were
               transferrable through a generally available machine actionable
               process. The experiment revealed some areas for potential
               improvements to DDI as well as suggestions for data analysis
               packages and research practices.",
  publisher = "IASSIST Quarterly",
  year      =  2016,
  url       = "https://kuscholarworks.ku.edu/handle/1808/19900",
  institution = {KU Scholar Works},
  keywords  = "DDI, data formats, metadata, statistical packages,
               Stat/Transfer, JMP, R, SAS, SPSS, Stata; Supplementary materials"
}

@ARTICLE{Frey2013-fk,
  title       = "Cheminformatics and the Semantic Web: adding value with linked
                 data and enhanced provenance",
  author      = "Frey, Jeremy G and Bird, Colin L",
  affiliation = "Chemistry, Faculty of Natural Environmental Science,
                 University of Southampton Highfield, Southampton, SO17 1BJ,
                 UK. Chemistry, Faculty of Natural Environmental Science,
                 University of Southampton Highfield, Southampton, SO17 1BJ,
                 UK.",
  abstract    = "Cheminformatics is evolving from being a field of study
                 associated primarily with drug discovery into a discipline
                 that embraces the distribution, management, access, and
                 sharing of chemical data. The relationship with the related
                 subject of bioinformatics is becoming stronger and better
                 defined, owing to the influence of Semantic Web technologies,
                 which enable researchers to integrate heterogeneous sources of
                 chemical, biochemical, biological, and medical information.
                 These developments depend on a range of factors: the
                 principles of chemical identifiers and their role in
                 relationships between chemical and biological entities; the
                 importance of preserving provenance and properly curated
                 metadata; and an understanding of the contribution that the
                 Semantic Web can make at all stages of the research lifecycle.
                 The movements toward open access, open source, and open
                 collaboration all contribute to progress toward the goals of
                 integration.",
  journal     = "Wiley interdisciplinary reviews. Computational molecular
                 science",
  volume      =  3,
  number      =  5,
  pages       = "465--481",
  month       =  sep,
  year        =  2013,
  url         = "http://dx.doi.org/10.1002/wcms.1127",
  language    = "en",
  issn        = "1759-0876, 1759-0884",
  pmid        = "24432050",
  doi         = "10.1002/wcms.1127",
  pmc         = "PMC3884755"
}

@MISC{The_Turing_Way_Community2019-fn,
  title  = "The Turing Way: A Handbook for Reproducible Data Science",
  author = "{The Turing Way Community} and Arnold, Becky and Bowler, Louise and
            Gibson, Sarah and Herterich, Patricia and Higman, Rosie and
            Krystalli, Anna and Morley, Alexander and O'Reilly, Martin and
            Whitaker, Kirstie",
  month  =  mar,
  year   =  2019,
  url    = "https://zenodo.org/record/3233986",
  doi    = "10.5281/zenodo.3233986"
}

@ARTICLE{Le_Novere2009-yr,
  title    = "The systems biology graphical notation",
  author   = "Le Novere, N and Hucka, M and Mi, H and Moodie, S and Schreiber,
              F and Sorokin, A and Demir, E and Wegner, K and Aladjem, M I and
              Wimalaratne, S M and {Others}",
  journal  = "Nature biotechnology",
  volume   =  27,
  pages    = "735",
  year     =  2009,
  keywords = "charlie\_rcr.bib",
  issn     = "1087-0156"
}

@ARTICLE{Garijo2017-yx,
  title    = "Abstract, link, publish, exploit: An end to end framework for
              workflow sharing",
  author   = "Garijo, Daniel and Gil, Yolanda and Corcho, Oscar",
  abstract = "Scientific workflows are increasingly used to manage and share
              scientific computations and methods to analyze data. A variety of
              systems have been developed that store the workflows executed and
              make them part of public repositories However, workflows are
              published in the idiosyncratic format of the workflow system used
              for the creation and execution of the workflows. Browsing,
              linking and using the stored workflows and their results often
              becomes a challenge for scientists who may only be familiar with
              one system. In this paper we present an approach for addressing
              this issue by publishing and exploiting workflows as data on the
              Web with a representation that is independent from the workflow
              system used to create them. In order to achieve our goal, we
              follow the Linked Data Principles to publish workflow inputs,
              intermediate results, outputs and codes; and we reuse and extend
              well established standards like W3C PROV. We illustrate our
              approach by publishing workflows and consuming them with
              different tools designed to address common scenarios for workflow
              exploitation.",
  journal  = "Future generations computer systems: FGCS",
  volume   =  75,
  pages    = "271--283",
  month    =  oct,
  year     =  2017,
  url      = "http://www.sciencedirect.com/science/article/pii/S0167739X17300274",
  keywords = "Workflow publishing; Workflow consumption; Workflow
              representation; Provenance; Linked Data; Standards; PROV",
  issn     = "0167-739X",
  doi      = "10.1016/j.future.2017.01.008"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bachman2018-rp,
  title     = "{{FamPlex}: A resource for entity recognition and relationship
               resolution of human protein families and complexes in biomedical
               text mining}",
  author    = "Bachman, John A and Gyori, Benjamin M and Sorger, Peter K",
  abstract  = "For automated reading of scientific publications to extract
               useful information about molecular mechanisms it is critical
               that genes, proteins and other entities be correctly associated
               with uniform identifiers, a process known as named entity
               linking or ``grounding.'' Correct grounding is essential for
               resolving relationships among mined information, curated
               interaction databases, and biological datasets. The accuracy of
               this process is largely dependent on the availability of
               machine-readable resources associating synonyms and
               abbreviations commonly found in biomedical literature with
               uniform identifiers. In a task involving automated reading of
               ∼215,000 articles using the REACH event extraction software we
               found that grounding was disproportionately inaccurate for
               multi-protein families (e.g., ``AKT'') and complexes with
               multiple subunits (e.g.``NF- $\kappa$B''). To address this
               problem we constructed FamPlex, a manually curated resource
               defining protein families and complexes as they are commonly
               encountered in biomedical text. In FamPlex the gene-level
               constituents of families and complexes are defined in a flexible
               format allowing for multi-level, hierarchical membership. To
               create FamPlex, text strings corresponding to entities were
               identified empirically from literature and linked manually to
               uniform identifiers; these identifiers were also mapped to
               equivalent entries in multiple related databases. FamPlex also
               includes curated prefix and suffix patterns that improve named
               entity recognition and event extraction. Evaluation of REACH
               extractions on a test corpus of ∼54,000 articles showed that
               FamPlex significantly increased grounding accuracy for families
               and complexes (from 15 to 71\%). The hierarchical organization
               of entities in FamPlex also made it possible to integrate
               otherwise unconnected mechanistic information across families,
               subfamilies, and individual proteins. Applications of FamPlex to
               the TRIPS/DRUM reading system and the Biocreative VI Bioentity
               Normalization Task dataset demonstrated the utility of FamPlex
               in other settings. FamPlex is an effective resource for
               improving named entity recognition, grounding, and relationship
               resolution in automated reading of biomedical text. The content
               in FamPlex is available in both tabular and Open Biomedical
               Ontology formats at https://github.com/sorgerlab/famplex under
               the Creative Commons CC0 license and has been integrated into
               the TRIPS/DRUM and REACH reading systems.",
  journal   = "BMC bioinformatics",
  publisher = "BMC Bioinformatics",
  volume    =  19,
  number    =  1,
  pages     = "1--14",
  year      =  2018,
  url       = "http://dx.doi.org/10.1186/s12859-018-2211-5",
  keywords  = "Biocuration,Event extraction,Grounding,Named entity
               linking,Named entity recognition,Natural language
               processing,Protein families,Text mining;charlie\_rcr.bib",
  issn      = "1471-2105",
  pmid      = "29954318",
  doi       = "10.1186/s12859-018-2211-5"
}

@INPROCEEDINGS{Greenberg2013-qz,
  title     = "Metadata capital in a data repository",
  booktitle = "International Conference on Dublin Core and Metadata
               Applications",
  author    = "Greenberg, Jane and Swauger, Shea and Feinstein, Elena",
  pages     = "140--150",
  year      =  2013,
  url       = "http://dcpapers.dublincore.org/pubs/article/download/3678/1901"
}

@ARTICLE{Vivian2017-hr,
  title       = "Toil enables reproducible, open source, big biomedical data
                 analyses",
  author      = "Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin
                 and Ketchum, Christopher and Armstrong, Joel and Novak, Adam
                 and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D and
                 Musselman-Brown, Audrey and Schmidt, Hannes and Amstutz, Peter
                 and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and
                 Cline, Melissa and O'Connor, Brian and Hanna, Megan and
                 Birger, Chet and Kent, W James and Patterson, David A and
                 Joseph, Anthony D and Zhu, Jingchun and Zaranek, Sasha and
                 Getz, Gad and Haussler, David and Paten, Benedict",
  affiliation = "Computational Genomics Lab, UC Santa Cruz Genomics Institute,
                 University of California Santa Cruz, Santa Cruz, California,
                 USA. Computational Genomics Lab, UC Santa Cruz Genomics
                 Institute, University of California Santa Cruz, Santa Cruz,
                 California, USA. AMP Lab, University of California Berkeley,
                 Berkeley, California, USA. UC Berkeley ASPIRE Lab, Berkeley,
                 Berkeley, California, USA. Computational Genomics Lab, UC
                 Santa Cruz Genomics Institute, University of California Santa
                 Cruz, Santa Cruz, California, USA. Computational Genomics Lab,
                 UC Santa Cruz Genomics Institute, University of California
                 Santa Cruz, Santa Cruz, California, USA. Computational
                 Genomics Lab, UC Santa Cruz Genomics Institute, University of
                 California Santa Cruz, Santa Cruz, California, USA.
                 Computational Genomics Lab, UC Santa Cruz Genomics Institute,
                 University of California Santa Cruz, Santa Cruz, California,
                 USA. Computational Genomics Lab, UC Santa Cruz Genomics
                 Institute, University of California Santa Cruz, Santa Cruz,
                 California, USA. Computational Genomics Lab, UC Santa Cruz
                 Genomics Institute, University of California Santa Cruz, Santa
                 Cruz, California, USA. Computational Genomics Lab, UC Santa
                 Cruz Genomics Institute, University of California Santa Cruz,
                 Santa Cruz, California, USA. Computational Genomics Lab, UC
                 Santa Cruz Genomics Institute, University of California Santa
                 Cruz, Santa Cruz, California, USA. Curoverse, Somerville,
                 Massachusetts, USA. Computational Genomics Lab, UC Santa Cruz
                 Genomics Institute, University of California Santa Cruz, Santa
                 Cruz, California, USA. Computational Genomics Lab, UC Santa
                 Cruz Genomics Institute, University of California Santa Cruz,
                 Santa Cruz, California, USA. Computational Genomics Lab, UC
                 Santa Cruz Genomics Institute, University of California Santa
                 Cruz, Santa Cruz, California, USA. Computational Genomics Lab,
                 UC Santa Cruz Genomics Institute, University of California
                 Santa Cruz, Santa Cruz, California, USA. Computational
                 Genomics Lab, UC Santa Cruz Genomics Institute, University of
                 California Santa Cruz, Santa Cruz, California, USA. Broad
                 Institute of Harvard and Massachusetts Institute of Technology
                 (MIT), Cambridge, Massachusetts, USA. Broad Institute of
                 Harvard and Massachusetts Institute of Technology (MIT),
                 Cambridge, Massachusetts, USA. Computational Genomics Lab, UC
                 Santa Cruz Genomics Institute, University of California Santa
                 Cruz, Santa Cruz, California, USA. AMP Lab, University of
                 California Berkeley, Berkeley, California, USA. UC Berkeley
                 ASPIRE Lab, Berkeley, Berkeley, California, USA. AMP Lab,
                 University of California Berkeley, Berkeley, California, USA.
                 UC Berkeley ASPIRE Lab, Berkeley, Berkeley, California, USA.
                 Computational Genomics Lab, UC Santa Cruz Genomics Institute,
                 University of California Santa Cruz, Santa Cruz, California,
                 USA. Curoverse, Somerville, Massachusetts, USA. Broad
                 Institute of Harvard and Massachusetts Institute of Technology
                 (MIT), Cambridge, Massachusetts, USA. Computational Genomics
                 Lab, UC Santa Cruz Genomics Institute, University of
                 California Santa Cruz, Santa Cruz, California, USA.
                 Computational Genomics Lab, UC Santa Cruz Genomics Institute,
                 University of California Santa Cruz, Santa Cruz, California,
                 USA.",
  journal     = "Nature biotechnology",
  volume      =  35,
  number      =  4,
  pages       = "314--316",
  month       =  apr,
  year        =  2017,
  url         = "http://dx.doi.org/10.1038/nbt.3772",
  keywords    = "prospectus III;in prospectus",
  language    = "en",
  issn        = "1087-0156, 1546-1696",
  pmid        = "28398314",
  doi         = "10.1038/nbt.3772"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Weibel1998-ue,
  title     = "Dublin core metadata for resource discovery",
  author    = "Weibel, Stuart and Kunze, John and Lagoze, Carl and Wolf, Misha",
  abstract  = "Rfc, 2413. Title, Dublin Core Metadata for Resource Discovery.
               Author, S. Weibel, J. Kunze, C. Lagoze, M. Wolf. Date, September
               1998. Format: TXT=15501, HTML=0 bytes. Obsoleted by, RFC5013.
               Status: INFORMATIONAL. Network Working Group S. Weibel Request
               for …",
  journal   = "Internet Engineering Task Force RFC",
  publisher = "hjp.at",
  volume    =  2413,
  number    =  222,
  pages     = "132",
  year      =  1998,
  url       = "http://www.hjp.at/doc/rfc/rfc2413.html"
}

@ARTICLE{Konkol2019-ii,
  title     = "Creating Interactive Scientific Publications using Bindings",
  author    = "Konkol, Markus and Kray, Christian and Suleiman, Jan",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  volume    =  3,
  number    = "EICS",
  pages     = "1--18",
  month     =  jun,
  year      =  2019,
  url       = "https://doi.org/10.1145/3331158",
  address   = "New York, NY, USA",
  keywords  = "reproducible research, open science, interactive papers,
               research compendia",
  doi       = "10.1145/3331158"
}

@ARTICLE{Goecks2010-qz,
  title       = "Galaxy: a comprehensive approach for supporting accessible,
                 reproducible, and transparent computational research in the
                 life sciences",
  author      = "Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and
                 {Galaxy Team}",
  affiliation = "Department of Biology and Department of Mathematics and
                 Computer Science, Emory University, 1510 Clifton Road NE,
                 Atlanta, GA 30322, USA. jeremy.goecks@emory.edu",
  abstract    = "Increased reliance on computational approaches in the life
                 sciences has revealed grave concerns about how accessible and
                 reproducible computation-reliant results truly are. Galaxy
                 http://usegalaxy.org, an open web-based platform for genomic
                 research, addresses these problems. Galaxy automatically
                 tracks and manages data provenance and provides support for
                 capturing the context and intent of computational methods.
                 Galaxy Pages are interactive, web-based documents that provide
                 users with a medium to communicate a complete computational
                 analysis.",
  journal     = "Genome biology",
  volume      =  11,
  number      =  8,
  pages       = "R86",
  month       =  aug,
  year        =  2010,
  url         = "http://dx.doi.org/10.1186/gb-2010-11-8-r86",
  keywords    = "citedinwf;should\_be\_in\_prospectus;prospectus III;in
                 prospectus",
  language    = "en",
  issn        = "1465-6906",
  pmid        = "20738864",
  doi         = "10.1186/gb-2010-11-8-r86",
  pmc         = "PMC2945788"
}

@ARTICLE{Herz2017-xa,
  title       = "dcmqi: An Open Source Library for Standardized Communication
                 of Quantitative Image Analysis Results Using {DICOM}",
  author      = "Herz, Christian and Fillion-Robin, Jean-Christophe and Onken,
                 Michael and Riesmeier, J{\"o}rg and Lasso, Andras and Pinter,
                 Csaba and Fichtinger, Gabor and Pieper, Steve and Clunie,
                 David and Kikinis, Ron and Fedorov, Andriy",
  affiliation = "Department of Radiology, Brigham and Women's Hospital, Boston,
                 Massachusetts. Harvard Medical School, Harvard University,
                 Boston, Massachusetts. Kitware Inc., Clifton Park, New York.
                 Open Connections GmbH, Oldenburg, Germany. Freelancer,
                 Oldenburg, Germany. Laboratory for Percutaneous Surgery,
                 School of Computing, Queen's University, Kingston, Ontario,
                 Canada. Isomics, Inc., Cambridge, Massachusetts. PixelMed
                 Publishing, LLC, Bangor, Pennsylvania. Department of Computer
                 Science, University of Bremen, Bremen, Germany. Fraunhofer
                 MEVIS, Bremen, Germany. Department of Radiology, Brigham and
                 Women's Hospital, Boston, Massachusetts.
                 fedorov@bwh.harvard.edu andrey.fedorov@gmail.com.",
  abstract    = "Quantitative analysis of clinical image data is an active area
                 of research that holds promise for precision medicine, early
                 assessment of treatment response, and objective
                 characterization of the disease. Interoperability, data
                 sharing, and the ability to mine the resulting data are of
                 increasing importance, given the explosive growth in the
                 number of quantitative analysis methods being proposed. The
                 Digital Imaging and Communications in Medicine (DICOM)
                 standard is widely adopted for image and metadata in
                 radiology. dcmqi (DICOM for Quantitative Imaging) is a free,
                 open source library that implements conversion of the data
                 stored in commonly used research formats into the standard
                 DICOM representation. dcmqi source code is distributed under
                 BSD-style license. It is freely available as a precompiled
                 binary package for every major operating system, as a Docker
                 image, and as an extension to 3D Slicer. Installation and
                 usage instructions are provided in the GitHub repository at
                 https://github.com/qiicr/dcmqi Cancer Res; 77(21); e87-90.
                 \copyright{}2017 AACR.",
  journal     = "Cancer research",
  volume      =  77,
  number      =  21,
  pages       = "e87--e90",
  month       =  nov,
  year        =  2017,
  url         = "http://dx.doi.org/10.1158/0008-5472.CAN-17-0336",
  language    = "en",
  issn        = "0008-5472, 1538-7445",
  pmid        = "29092948",
  doi         = "10.1158/0008-5472.CAN-17-0336",
  pmc         = "PMC5675033"
}

@ARTICLE{Ali2020-fw,
  title         = "The {KEEN} Universe: An Ecosystem for Knowledge Graph
                   Embeddings with a Focus on Reproducibility and
                   Transferability",
  author        = "Ali, Mehdi and Jabeen, Hajira and Hoyt, Charles Tapley and
                   Lehman, Jens",
  abstract      = "There is an emerging trend of embedding knowledge graphs
                   (KGs) in continuous vector spaces in order to use those for
                   machine learning tasks. Recently, many knowledge graph
                   embedding (KGE) models have been proposed that learn low
                   dimensional representations while trying to maintain the
                   structural properties of the KGs such as the similarity of
                   nodes depending on their edges to other nodes. KGEs can be
                   used to address tasks within KGs such as the prediction of
                   novel links and the disambiguation of entities. They can
                   also be used for downstream tasks like question answering
                   and fact-checking. Overall, these tasks are relevant for the
                   semantic web community. Despite their popularity, the
                   reproducibility of KGE experiments and the transferability
                   of proposed KGE models to research fields outside the
                   machine learning community can be a major challenge.
                   Therefore, we present the KEEN Universe, an ecosystem for
                   knowledge graph embeddings that we have developed with a
                   strong focus on reproducibility and transferability. The
                   KEEN Universe currently consists of the Python packages
                   PyKEEN (Python KnowlEdge EmbeddiNgs), BioKEEN (Biological
                   KnowlEdge EmbeddiNgs), and the KEEN Model Zoo for sharing
                   trained KGE models with the community.",
  month         =  jan,
  year          =  2020,
  url           = "http://arxiv.org/abs/2001.10560",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "2001.10560",
  primaryClass  = "cs.LG",
  arxivid       = "2001.10560"
}

@MISC{noauthor_undated-qy,
  title       = "baydem",
  abstract    = "Bayesian tools for reconstructing past demography. Contribute
                 to eehh-stanford/baydem development by creating an account on
                 GitHub.",
  institution = "Github",
  url         = "https://github.com/eehh-stanford/baydem"
}

@INPROCEEDINGS{Jaradeh2019-zw,
  title           = "Open research knowledge graph",
  booktitle       = "Proceedings of the 10th International Conference on
                     Knowledge Capture",
  author          = "Jaradeh, Mohamad Yaser and Oelen, Allard and Farfar, Kheir
                     Eddine and Prinz, Manuel and D'Souza, Jennifer and
                     Kismih{\'o}k, G{\'a}bor and Stocker, Markus and Auer,
                     S{\"o}ren",
  affiliation     = "L3S Research Center, Leibniz University Hannover, Hanover,
                     Germany; TIB Leibniz Information Centre for Science and
                     Technology, Hanover, Germany",
  publisher       = "ACM",
  month           =  sep,
  year            =  2019,
  url             = "https://dl.acm.org/doi/10.1145/3360901.3364435",
  address         = "New York, NY, USA",
  copyright       = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference      = "K-CAP '19: Knowledge Capture Conference",
  location        = "Marina Del Rey CA USA",
  isbn            = "9781450370080",
  doi             = "10.1145/3360901.3364435"
}

@ARTICLE{Hull2006-do,
  title       = "Taverna: a tool for building and running workflows of services",
  author      = "Hull, Duncan and Wolstencroft, Katy and Stevens, Robert and
                 Goble, Carole and Pocock, Mathew R and Li, Peter and Oinn, Tom",
  affiliation = "School of Computer Science, University of Manchester, M13 9PL,
                 UK. duncan.hull@cs.man.ac.uk",
  abstract    = "Taverna is an application that eases the use and integration
                 of the growing number of molecular biology tools and databases
                 available on the web, especially web services. It allows
                 bioinformaticians to construct workflows or pipelines of
                 services to perform a range of different analyses, such as
                 sequence analysis and genome annotation. These high-level
                 workflows can integrate many different resources into a single
                 analysis. Taverna is available freely under the terms of the
                 GNU Lesser General Public License (LGPL) from
                 http://taverna.sourceforge.net/.",
  journal     = "Nucleic acids research",
  publisher   = "academic.oup.com",
  volume      =  34,
  number      = "Web Server issue",
  pages       = "W729--32",
  month       =  jul,
  year        =  2006,
  url         = "http://dx.doi.org/10.1093/nar/gkl320",
  keywords    = "pipelines \& provenance",
  language    = "en",
  issn        = "0305-1048, 1362-4962",
  pmid        = "16845108",
  doi         = "10.1093/nar/gkl320",
  pmc         = "PMC1538887"
}

@ARTICLE{Love2014-se,
  title    = "Moderated estimation of fold change and dispersion for {RNA-seq}
              data with {DESeq2}",
  author   = "Love, Michael I and Huber, Wolfgang and Anders, Simon",
  abstract = "In comparative high-throughput sequencing assays, a fundamental
              task is the analysis of count data, such as read counts per gene
              in RNA-seq, for evidence of systematic changes across
              experimental conditions. Small replicate numbers, discreteness,
              large dynamic range and the presence of outliers require a
              suitable statistical approach. We present DESeq2, a method for
              differential analysis of count data, using shrinkage estimation
              for dispersions and fold changes to improve stability and
              interpretability of estimates. This enables a more quantitative
              analysis focused on the strength rather than the mere presence of
              differential expression. The DESeq2 package is available at
              http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html
              webcite.",
  journal  = "Genome biology",
  volume   =  15,
  number   =  12,
  pages    = "550",
  year     =  2014,
  url      = "http://dx.doi.org/10.1186/s13059-014-0550-8",
  language = "en",
  issn     = "1465-6906",
  pmid     = "25516281",
  doi      = "10.1186/s13059-014-0550-8",
  pmc      = "PMC4302049"
}

@article{Kulmanov2020-mv,
  title    = "Machine learning with biomedical ontologies",
  author   = "Kulmanov, Maxat and Smaili, Fatima Zohra and Gao, Xin and
              Hoehndorf, Robert",
  abstract = "Ontologies have long been employed in the life sciences to
              formally represent and reason over domain knowledge, and they are
              employed in almost every major biological database. Recently,
              ontologies are increasingly being used to provide background
              knowledge in similarity-based analysis and machine learning
              models. The methods employed to combine ontologies and machine
              learning are still novel and actively being developed. We provide
              an overview over the methods that use ontologies to compute
              similarity and incorporate them in machine learning methods; in
              particular, we outline how semantic similarity measures and
              ontology embeddings can exploit the background knowledge in
              biomedical ontologies, and how ontologies can provide constraints
              that improve machine learning models. The methods and experiments
              we describe are available as a set of executable notebooks, and
              we also provide a set of slides and additional resources at . Key
              points \#\#\# Competing Interest Statement The authors have
              declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2020.05.07.082164",
  month    =  may,
  year     =  2020,
  url      = "https://www.biorxiv.org/content/10.1101/2020.05.07.082164v1.abstract",
  language = "en",
  doi      = "10.1101/2020.05.07.082164"
}

@ARTICLE{Bechhofer2013-cx,
  title    = "Why linked data is not enough for scientists",
  author   = "Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier,
              Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and
              Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and
              Gamble, Matthew and Michaelides, Danius and Owen, Stuart and
              Newman, David and Sufi, Shoaib and Goble, Carole",
  abstract = "Scientific data represents a significant portion of the linked
              open data cloud and scientists stand to benefit from the data
              fusion capability this will afford. Publishing linked data into
              the cloud, however, does not ensure the required reusability.
              Publishing has requirements of provenance, quality, credit,
              attribution and methods to provide the reproducibility that
              enables validation of results. In this paper we make the case for
              a scientific data publication model on top of linked data and
              introduce the notion of Research Objects as first class citizens
              for sharing and publishing.",
  journal  = "Future generations computer systems: FGCS",
  volume   =  29,
  number   =  2,
  pages    = "599--611",
  year     =  2013,
  url      = "http://dx.doi.org/10.1016/j.future.2011.08.004",
  keywords = "Research object; Linked data; Reproducibility; Reuse; Sharing;
              Publishing;citedinwf",
  issn     = "0167-739X",
  doi      = "10.1016/j.future.2011.08.004"
}

@ARTICLE{Brazma2001-mw,
  title       = "Minimum information about a microarray experiment
                 ({MIAME)-toward} standards for microarray data",
  author      = "Brazma, A and Hingamp, P and Quackenbush, J and Sherlock, G
                 and Spellman, P and Stoeckert, C and Aach, J and Ansorge, W
                 and Ball, C A and Causton, H C and Gaasterland, T and
                 Glenisson, P and Holstege, F C and Kim, I F and Markowitz, V
                 and Matese, J C and Parkinson, H and Robinson, A and Sarkans,
                 U and Schulze-Kremer, S and Stewart, J and Taylor, R and Vilo,
                 J and Vingron, M",
  affiliation = "European Bioinformatics Institute, EMBL outstation, Wellcome
                 Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK.
                 brazma@ebi.ac.uk",
  abstract    = "Microarray analysis has become a widely used tool for the
                 generation of gene expression data on a genomic scale.
                 Although many significant results have been derived from
                 microarray studies, one limitation has been the lack of
                 standards for presenting and exchanging such data. Here we
                 present a proposal, the Minimum Information About a Microarray
                 Experiment (MIAME), that describes the minimum information
                 required to ensure that microarray data can be easily
                 interpreted and that results derived from its analysis can be
                 independently verified. The ultimate goal of this work is to
                 establish a standard for recording and reporting
                 microarray-based gene expression data, which will in turn
                 facilitate the establishment of databases and public
                 repositories and enable the development of data analysis
                 tools. With respect to MIAME, we concentrate on defining the
                 content and structure of the necessary information rather than
                 the technical format for capturing it.",
  journal     = "Nature genetics",
  volume      =  29,
  number      =  4,
  pages       = "365--371",
  month       =  dec,
  year        =  2001,
  url         = "http://dx.doi.org/10.1038/ng1201-365",
  keywords    = "prospectus I;in prospectus",
  language    = "en",
  issn        = "1061-4036",
  pmid        = "11726920",
  doi         = "10.1038/ng1201-365"
}

@ARTICLE{Stagge2019-fv,
  title       = "Assessing data availability and research reproducibility in
                 hydrology and water resources",
  author      = "Stagge, James H and Rosenberg, David E and Abdallah, Adel M
                 and Akbar, Hadia and Attallah, Nour A and James, Ryan",
  affiliation = "Utah State University, Department of Civil and Environmental
                 Engineering and Utah Water Research Laboratory, Logan, UT
                 84321, USA. The Ohio State University, Department of Civil,
                 Environmental and Geodetic Engineering, Columbus, OH 43210,
                 USA. The Western States Water Council, Salt Lake City, UT
                 84107, USA.",
  abstract    = "There is broad interest to improve the reproducibility of
                 published research. We developed a survey tool to assess the
                 availability of digital research artifacts published alongside
                 peer-reviewed journal articles (e.g. data, models, code,
                 directions for use) and reproducibility of article results. We
                 used the tool to assess 360 of the 1,989 articles published by
                 six hydrology and water resources journals in 2017. Like
                 studies from other fields, we reproduced results for only a
                 small fraction of articles (1.6\% of tested articles) using
                 their available artifacts. We estimated, with 95\% confidence,
                 that results might be reproduced for only 0.6\% to 6.8\% of
                 all 1,989 articles. Unlike prior studies, the survey tool
                 identified key bottlenecks to making work more reproducible.
                 Bottlenecks include: only some digital artifacts available
                 (44\% of articles), no directions (89\%), or all artifacts
                 available but results not reproducible (5\%). The tool (or
                 extensions) can help authors, journals, funders, and
                 institutions to self-assess manuscripts, provide feedback to
                 improve reproducibility, and recognize and reward reproducible
                 articles as examples for others.",
  journal     = "Scientific data",
  volume      =  6,
  pages       = "190030",
  month       =  feb,
  year        =  2019,
  url         = "http://dx.doi.org/10.1038/sdata.2019.30",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "30806638",
  doi         = "10.1038/sdata.2019.30",
  pmc         = "PMC6390703"
}

@ARTICLE{Pedregosa2011-zz,
  title    = "Scikit-learn: Machine Learning in Python",
  author   = "Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre
              and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and
              Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and
              Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and
              Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and
              Duchesnay, {\'E}douard",
  journal  = "Journal of machine learning research: JMLR",
  volume   =  12,
  number   = "Oct",
  pages    = "2825--2830",
  year     =  2011,
  url      = "http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html",
  issn     = "1532-4435"
}

@ARTICLE{Perfetto2016-nt,
  title    = "{{SIGNOR}: A database of causal relationships between biological
              entities}",
  author   = "Perfetto, L and Briganti, L and Calderone, A and Perpetuini, A C
              and Iannuccelli, M and Langone, F and Licata, L and Marinkovic, M
              and Mattioni, A and Pavlidou, T and Peluso, D and Petrilli, L L
              and Pirr{\'o}, S and Posca, D and Santonico, E and Silvestri, A
              and Spada, F and Castagnoli, L and Cesareni, G",
  journal  = "Nucleic acids research",
  volume   =  44,
  pages    = "D548",
  year     =  2016,
  keywords = "charlie\_rcr.bib",
  issn     = "0305-1048"
}

@ARTICLE{Ball2016-up,
  title  = "{RDA} Metadata Standards Directory Working Group",
  author = "Ball, Alex and Greenberg, Jane and Jeffery, Keith and Koskela,
            Rebecca",
  year   =  2016,
  url    = "https://www.rd-alliance.org/system/files/MSDWG-Final-Report.pdf"
}

@ARTICLE{Sandve2013-yv,
  title       = "Ten simple rules for reproducible computational research",
  author      = "Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James
                 and Hovig, Eivind",
  affiliation = "Department of Informatics, University of Oslo, Blindern, Oslo,
                 Norway ; Centre for Cancer Biomedicine, University of Oslo,
                 Blindern, Oslo, Norway.",
  journal     = "PLoS computational biology",
  volume      =  9,
  number      =  10,
  pages       = "e1003285",
  month       =  oct,
  year        =  2013,
  url         = "http://dx.doi.org/10.1371/journal.pcbi.1003285",
  language    = "en",
  issn        = "1553-734X, 1553-7358",
  pmid        = "24204232",
  doi         = "10.1371/journal.pcbi.1003285",
  pmc         = "PMC3812051"
}

@INPROCEEDINGS{Allamanis2013-qk,
  title     = "Mining Source Code Repositories at Massive Scale Using Language
               Modeling",
  booktitle = "Proceedings of the 10th Working Conference on Mining Software
               Repositories",
  author    = "Allamanis, Miltiadis and Sutton, Charles",
  publisher = "IEEE Press",
  pages     = "207--216",
  series    = "MSR '13",
  year      =  2013,
  url       = "http://dl.acm.org/citation.cfm?id=2487085.2487127",
  address   = "Piscataway, NJ, USA",
  location  = "San Francisco, CA, USA",
  isbn      = "9781467329361"
}

@ARTICLE{Cwiek-Kupczynska2020-en,
  title       = "Semantic concept schema of the linear mixed model of
                 experimental observations",
  author      = "{\'C}wiek-Kupczy{\'n}ska, Hanna and Filipiak, Katarzyna and
                 Markiewicz, Augustyn and Rocca-Serra, Philippe and
                 Gonzalez-Beltran, Alejandra N and Sansone, Susanna-Assunta and
                 Millet, Emilie J and van Eeuwijk, Fred and {\L}awrynowicz,
                 Agnieszka and Krajewski, Pawe{\l}",
  affiliation = "Institute of Plant Genetics, Polish Academy of Sciences, ul.
                 Strzeszy{\'n}ska 34, 60-479, Pozna{\'n}, Poland. Institute of
                 Mathematics, Pozna{\'n} University of Technology, ul. Piotrowo
                 3a, 60-965, Pozna{\'n}, Poland. Department of Mathematical and
                 Statistical Methods, Pozna{\'n} University of Life Sciences,
                 ul. Wojska Polskiego 28, 60-637, Pozna{\'n}, Poland. Oxford
                 e-Research Center, Department of Engineering Science,
                 University of Oxford, Oxford, OX1 3QG, UK. Scientific
                 Computing Department, Science and Technology Facilities
                 Council, UK Research \& Innovation, Didcot, OX11 0QX, UK.
                 Biometris, Wageningen University \& Research Centre, P.O. Box
                 16, 6700 AA, Wageningen, The Netherlands. Faculty of Computing
                 and Telecommunications, Poznan University of Technology, ul.
                 Piotrowo 3, 60-965, Pozna{\'n}, Poland. Institute of Plant
                 Genetics, Polish Academy of Sciences, ul. Strzeszy{\'n}ska 34,
                 60-479, Pozna{\'n}, Poland. pkra@igr.poznan.pl.",
  abstract    = "In the information age, smart data modelling and data
                 management can be carried out to address the wealth of data
                 produced in scientific experiments. In this paper, we propose
                 a semantic model for the statistical analysis of datasets by
                 linear mixed models. We tie together disparate statistical
                 concepts in an interdisciplinary context through the
                 application of ontologies, in particular the Statistics
                 Ontology (STATO), to produce FAIR data summaries. We hope to
                 improve the general understanding of statistical modelling and
                 thus contribute to a better description of the statistical
                 conclusions from data analysis, allowing their efficient
                 exploration and automated processing.",
  journal     = "Scientific data",
  volume      =  7,
  number      =  1,
  pages       = "70",
  month       =  feb,
  year        =  2020,
  url         = "http://dx.doi.org/10.1038/s41597-020-0409-7",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "32109232",
  doi         = "10.1038/s41597-020-0409-7",
  pmc         = "PMC7046786"
}

@ARTICLE{Robertson2014-as,
  title       = "The {GBIF} integrated publishing toolkit: facilitating the
                 efficient publishing of biodiversity data on the internet",
  author      = "Robertson, Tim and D{\"o}ring, Markus and Guralnick, Robert
                 and Bloom, David and Wieczorek, John and Braak, Kyle and
                 Otegui, Javier and Russell, Laura and Desmet, Peter",
  affiliation = "Global Biodiversity Information Facility, Copenhagen, Denmark.
                 University of Colorado, Boulder, Colorado, United States of
                 America. University of California, Berkeley, Berkeley,
                 California, United States of America. University of Kansas,
                 Lawrence, Kansas, United States of America. Research Institute
                 for Nature and Forest (INBO), Brussels, Belgium.",
  abstract    = "The planet is experiencing an ongoing global biodiversity
                 crisis. Measuring the magnitude and rate of change more
                 effectively requires access to organized, easily discoverable,
                 and digitally-formatted biodiversity data, both legacy and
                 new, from across the globe. Assembling this coherent digital
                 representation of biodiversity requires the integration of
                 data that have historically been analog, dispersed, and
                 heterogeneous. The Integrated Publishing Toolkit (IPT) is a
                 software package developed to support biodiversity dataset
                 publication in a common format. The IPT's two primary
                 functions are to 1) encode existing species occurrence
                 datasets and checklists, such as records from natural history
                 collections or observations, in the Darwin Core standard to
                 enhance interoperability of data, and 2) publish and archive
                 data and metadata for broad use in a Darwin Core Archive, a
                 set of files following a standard format. Here we discuss the
                 key need for the IPT, how it has developed in response to
                 community input, and how it continues to evolve to streamline
                 and enhance the interoperability, discoverability, and
                 mobilization of new data types beyond basic Darwin Core
                 records. We close with a discussion how IPT has impacted the
                 biodiversity research community, how it enhances data
                 publishing in more traditional journal venues, along with new
                 features implemented in the latest version of the IPT, and
                 future plans for more enhancements.",
  journal     = "PloS one",
  publisher   = "journals.plos.org",
  volume      =  9,
  number      =  8,
  pages       = "e102623",
  month       =  aug,
  year        =  2014,
  url         = "http://dx.doi.org/10.1371/journal.pone.0102623",
  keywords    = "publications \& peer review \& journals",
  language    = "en",
  issn        = "1932-6203",
  pmid        = "25099149",
  doi         = "10.1371/journal.pone.0102623",
  pmc         = "PMC4123864"
}

@ARTICLE{Wilkinson2018-gr,
  title       = "A design framework and exemplar metrics for {FAIRness}",
  author      = "Wilkinson, Mark D and Sansone, Susanna-Assunta and Schultes,
                 Erik and Doorn, Peter and Bonino da Silva Santos, Luiz Olavo
                 and Dumontier, Michel",
  affiliation = "Centro de Biotecnolog{\'\i}a y Gen{\'o}mica de Plantas UPM -
                 INIA, Madrid, Spain. Oxford e-Research Centre, Department of
                 Engineering Science, University of Oxford, Oxford, UK. Dutch
                 Techcentre for Life Sciences, Utrecht, The Netherlands. Data
                 Archiving and Networked Services, Den Haag, The Netherlands.
                 GO FAIR International Support and Coordination Office, Leiden,
                 The Netherlands. Leiden University Medical Centre, Leiden, The
                 Netherlands. Institute of Data Science, Maastricht University,
                 Maastricht, The Netherlands.",
  journal     = "Scientific data",
  volume      =  5,
  pages       = "180118",
  month       =  jun,
  year        =  2018,
  url         = "http://dx.doi.org/10.1038/sdata.2018.118",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "29944145",
  doi         = "10.1038/sdata.2018.118",
  pmc         = "PMC6018520"
}

@ARTICLE{Peng2021-cx,
  title       = "Diversity in immunogenomics: the value and the challenge",
  author      = "Peng, Kerui and Safonova, Yana and Shugay, Mikhail and
                 Popejoy, Alice B and Rodriguez, Oscar L and Breden, Felix and
                 Brodin, Petter and Burkhardt, Amanda M and Bustamante, Carlos
                 and Cao-Lormeau, Van-Mai and Corcoran, Martin M and Duffy,
                 Darragh and Fuentes-Guajardo, Macarena and Fujita, Ricardo and
                 Greiff, Victor and J{\"o}nsson, Vanessa D and Liu, Xiao and
                 Quintana-Murci, Lluis and Rossetti, Maura and Xie, Jianming
                 and Yaari, Gur and Zhang, Wei and Abedalthagafi, Malak S and
                 Adekoya, Khalid O and Ahmed, Rahaman A and Chang, Wei-Chiao
                 and Gray, Clive and Nakamura, Yusuke and Lees, William D and
                 Khatri, Purvesh and Alachkar, Houda and Scheepers, Cathrine
                 and Watson, Corey T and Karlsson Hedestam, Gunilla B and
                 Mangul, Serghei",
  affiliation = "Department of Clinical Pharmacy, School of Pharmacy,
                 University of Southern California, Los Angeles, CA, USA.
                 Computer Science and Engineering Department, University of
                 California San Diego, San Diego, CA, USA. Department of
                 Biochemistry and Molecular Genetics, University of Louisville
                 School of Medicine, Louisville, KY, USA. Shemyakin-Ovchinnikov
                 Institute of Bioorganic Chemistry, Russian Academy of
                 Sciences, Moscow, Russia. Pirogov Russian National Research
                 Medical University, Moscow, Russia. Department of Biomedical
                 Data Science, Stanford University, Stanford, CA, USA.
                 Department of Biological Sciences, Simon Fraser University,
                 Burnaby, British Columbia, Canada. Science for Life
                 Laboratory, Department of Women's and Children Health,
                 Karolinska Institutet, Stockholm, Sweden. Pediatric
                 Rheumatology, Karolinska University Hospital, Stockholm,
                 Sweden. Laboratory of Research on Infectious Vector-borne
                 Diseases, Institut Louis Malard{\'e}, Papeete, French
                 Polynesia. Department of Microbiology, Tumor and Cell Biology,
                 Karolinska Institutet, Stockholm, Sweden. Translational
                 Immunology Laboratory, Institut Pasteur, Paris, France.
                 Departamento de Tecnolog{\'\i}a M{\'e}dica, Facultad de
                 Ciencias de la Salud, Universidad de Tarapac{\'a}, Arica,
                 Chile. Centro de Gen{\'e}tica y Biolog{\'\i}a Molecular,
                 Universidad de San Mart{\'\i}n de Porres, La Molina, Lima,
                 Per{\'u}. Department of Immunology, University of Oslo, Oslo,
                 Norway. Departments of Computational and Quantitative Medicine
                 and Hematology, Beckman Research Institute, City of Hope,
                 Duarte, CA, USA. Tsinghua Shenzhen International Graduate
                 School, Tsinghua University, Shenzhen, China. Human
                 Evolutionary Genetics Unit, Institut Pasteur, UMR 2000, CNRS,
                 Paris, France. Department of Human Genomics and Evolution,
                 Coll{\`e}ge de France, Paris, France. UCLA Immunogenetics
                 Center, Department of Pathology and Laboratory Medicine, David
                 Geffen School of Medicine, University of California Los
                 Angeles, Los Angeles, CA, USA. Department of Pharmacology \&
                 Pharmaceutical Sciences, School of Pharmacy, University of
                 Southern California, Los Angeles, CA, USA. Faculty of
                 Engineering, Bar Ilan Institute of Nanotechnologies and
                 Advanced Materials, Bar Ilan University, Ramat Gan, Israel.
                 Department of Computer Science, City University of Hong Kong,
                 Hong Kong, China. Genomics Research Department, Saudi Human
                 Genome Project, King Fahad Medical City and King Abdulaziz
                 City for Science and Technology, Riyadh, Saudi Arabia.
                 Department of Cell Biology and Genetics, University of Lagos,
                 Lagos, Nigeria. Department of Clinical Pharmacy, School of
                 Pharmacy, Taipei Medical University, New Taipei City, Taiwan.
                 Division of Nephrology, Department of Internal Medicine,
                 Taipei Medical University-Shuang Ho Hospital, New Taipei City,
                 Taiwan. Division of Immunology, Institute of Infectious
                 Disease and Molecular Medicine and Department of Pathology,
                 University of Cape Town, Cape Town, South Africa. Laboratory
                 of Tissue Immunology, National Health Laboratory Services and
                 Groote Schuur Hospital, Cape Town, South Africa. Cancer
                 Precision Medicine Center, Japanese Foundation for Cancer
                 Research, Tokyo, Japan. Institute of Structural and Molecular
                 Biology, Birkbeck College, London, UK. Institute for Immunity,
                 Transplantation and Infection, School of Medicine, Stanford
                 University, CA, USA. Center for Biomedical Research,
                 Department of Medicine, School of Medicine, Stanford
                 University, Stanford, CA, USA. Centre for HIV and STIs,
                 National Institute for Communicable Diseases of the National
                 Health Laboratory Service, Johannesburg, South Africa. South
                 African Medical Research Council Antibody Immunity Research
                 Unit, School of Pathology, University of the Witwatersrand,
                 Johannesburg, South Africa. Department of Clinical Pharmacy,
                 School of Pharmacy, University of Southern California, Los
                 Angeles, CA, USA. serghei.mangul@gmail.com.",
  journal     = "Nature methods",
  volume      =  18,
  number      =  6,
  pages       = "588--591",
  month       =  jun,
  year        =  2021,
  url         = "http://dx.doi.org/10.1038/s41592-021-01169-5",
  language    = "en",
  issn        = "1548-7091, 1548-7105",
  pmid        = "34002093",
  doi         = "10.1038/s41592-021-01169-5"
}

@misc{Allaire2016-vl,
  title  = "Rticles: Article Formats for {R} Markdown",
  author = "Allaire, J J and Wickham, H and Xie, Y and Vaidyanathan, R and
            {Others}",
  year   =  2016
}

@ARTICLE{Team2015-ka,
  title   = "{RStudio}: integrated development for {R}",
  author  = "Team, Rstudio and {Others}",
  journal = "RStudio, Inc. , Boston, MA URL http://www. rstudio. com",
  volume  =  42,
  pages   = "14",
  year    =  2015
}

@ARTICLE{Foster2018-zr,
  title     = "Research infrastructure for the safe analysis of sensitive data",
  author    = "Foster, Ian",
  abstract  = "To use administrative and other new data sources for the
               scientific study of human beings and human behavior, analysts
               need to be able both to connect to these new data and to deploy
               new methods for linking and analyzing the data. These efforts
               are often hindered by legal, technical, and operational
               difficulties. In this article, I examine how new digital
               research infrastructures can be used to reduce such barriers.
               Experiences with data stewardship in other scientific domains
               shows that appropriate infrastructure can enable the efficient,
               secure, and collaborative integration of domain expertise, data,
               and analysis capabilities. I review the state of the art in
               these areas and argue for the use of cloud-hosted enclaves as a
               safe interaction point for analysts, data, and software, and as
               a means of automating and thus professionalizing data
               stewardship processes.",
  journal   = "The Annals of the American Academy of Political and Social
               Science",
  publisher = "SAGE Publications",
  volume    =  675,
  number    =  1,
  pages     = "102--120",
  month     =  jan,
  year      =  2018,
  url       = "http://journals.sagepub.com/doi/10.1177/0002716217742610",
  language  = "en",
  issn      = "0002-7162, 1552-3349",
  doi       = "10.1177/0002716217742610"
}

@ARTICLE{Zaharia2018-wi,
  title   = "Accelerating the Machine Learning Lifecycle with {MLflow}",
  author  = "Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi,
             Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth
             and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and {Others}",
  journal = "IEEE Data Eng. Bull.",
  volume  =  41,
  number  =  4,
  pages   = "39--45",
  year    =  2018,
  url     = "http://sites.computer.org/debull/A18dec/A18DEC-CD.pdf#page=41"
}

@ARTICLE{Ekbia2015-yp,
  title    = "Big data, bigger dilemmas: A critical review",
  author   = "Ekbia, Hamid and Mattioli, Michael and Kouper, Inna and Arave, G
              and Ghazinejad, Ali and Bowman, Timothy and Suri, Venkata
              Ratandeep and Tsou, Andrew and Weingart, Scott and Sugimoto,
              Cassidy R",
  abstract = "The recent interest in Big Data has generated a broad range of
              new academic, corporate, and policy practices along with an
              evolving debate among its proponents, detractors, and skeptics.
              While the practices draw on a common set of tools, techniques,
              and technologies, most contributions to the debate come either
              from a particular disciplinary perspective or with a focus on a
              domain-specific issue. A close examination of these contributions
              reveals a set of common problematics that arise in various guises
              and in different places. It also demonstrates the need for a
              critical synthesis of the conceptual and practical dilemmas
              surrounding Big Data. The purpose of this article is to provide
              such a synthesis by drawing on relevant writings in the sciences,
              humanities, policy, and trade literature. In bringing these
              diverse literatures together, we aim to shed light on the common
              underlying issues that concern and affect all of these areas. By
              contextualizing the phenomenon of Big Data within larger
              socioeconomic developments, we also seek to provide a broader
              understanding of its drivers, barriers, and challenges. This
              approach allows us to identify attributes of Big Data that
              require more attention?autonomy, opacity, generativity,
              disparity, and futurity?leading to questions and ideas for moving
              beyond dilemmas.",
  journal  = "Journal of the Association for Information Science and Technology",
  volume   =  66,
  number   =  8,
  pages    = "1523--1545",
  month    =  aug,
  year     =  2015,
  url      = "http://doi.wiley.com/10.1002/asi.23294",
  issn     = "2330-1635",
  doi      = "10.1002/asi.23294"
}

@MISC{Aufreiter2018-wp,
  title  = "The Reproducible Document Stack reinvents the journal publication
            for a world of computationally reproducible research",
  author = "Aufreiter, Michael and Penfold, Naomi",
  month  =  jul,
  year   =  2018,
  url    = "https://zenodo.org/record/1311612",
  doi    = "10.5281/zenodo.1311612"
}

@ARTICLE{Ashburner2000-hw,
  title     = "Gene Ontology: tool for the unification of biology",
  author    = "Ashburner, Michael and Ball, Catherine A and Blake, Judith A and
               Botstein, David and Butler, Heather and Michael Cherry, J and
               Davis, Allan P and Dolinski, Kara and Dwight, Selina S and
               Eppig, Janan T and Harris, Midori A and Hill, David P and
               Issel-Tarver, Laurie and Kasarskis, Andrew and Lewis, Suzanna
               and Matese, John C and Richardson, Joel E and Ringwald, Martin
               and Rubin, Gerald M and Sherlock, Gavin",
  abstract  = "Genomic sequencing has made it clear that a large fraction of
               the genes specifying the core biological functions are shared by
               all eukaryotes. Knowledge of the biological role of such shared
               proteins in one organism can often be transferred to other
               organisms. The goal of the Gene Ontology Consortium is to
               produce a dynamic, controlled vocabulary that can be applied to
               all eukaryotes even as knowledge of gene and protein roles in
               cells is accumulating and changing. To this end, three
               independent ontologies accessible on the World-Wide Web
               (http://www.geneontology.org) are being constructed: biological
               process, molecular function and cellular component.",
  journal   = "Nature genetics",
  publisher = "Nature Publishing Group",
  volume    =  25,
  number    =  1,
  pages     = "25--29",
  month     =  may,
  year      =  2000,
  url       = "https://www.nature.com/articles/ng0500_25",
  language  = "en",
  issn      = "1061-4036",
  doi       = "10.1038/75556"
}

@ARTICLE{Duck2014-vk,
  title       = "Extracting patterns of database and software usage from the
                 bioinformatics literature",
  author      = "Duck, Geraint and Nenadic, Goran and Brass, Andy and
                 Robertson, David L and Stevens, Robert",
  affiliation = "School of Computer Science, Manchester Institute of
                 Biotechnology and Computational and Evolutionary Biology,
                 Faculty of Life Sciences, The University of Manchester,
                 Manchester M13 9PL, UK. School of Computer Science, Manchester
                 Institute of Biotechnology and Computational and Evolutionary
                 Biology, Faculty of Life Sciences, The University of
                 Manchester, Manchester M13 9PL, UK School of Computer Science,
                 Manchester Institute of Biotechnology and Computational and
                 Evolutionary Biology, Faculty of Life Sciences, The University
                 of Manchester, Manchester M13 9PL, UK. School of Computer
                 Science, Manchester Institute of Biotechnology and
                 Computational and Evolutionary Biology, Faculty of Life
                 Sciences, The University of Manchester, Manchester M13 9PL, UK
                 School of Computer Science, Manchester Institute of
                 Biotechnology and Computational and Evolutionary Biology,
                 Faculty of Life Sciences, The University of Manchester,
                 Manchester M13 9PL, UK. School of Computer Science, Manchester
                 Institute of Biotechnology and Computational and Evolutionary
                 Biology, Faculty of Life Sciences, The University of
                 Manchester, Manchester M13 9PL, UK. School of Computer
                 Science, Manchester Institute of Biotechnology and
                 Computational and Evolutionary Biology, Faculty of Life
                 Sciences, The University of Manchester, Manchester M13 9PL,
                 UK.",
  abstract    = "MOTIVATION: As a natural consequence of being a computer-based
                 discipline, bioinformatics has a strong focus on database and
                 software development, but the volume and variety of resources
                 are growing at unprecedented rates. An audit of database and
                 software usage patterns could help provide an overview of
                 developments in bioinformatics and community common practice,
                 and comparing the links between resources through time could
                 demonstrate both the persistence of existing software and the
                 emergence of new tools. RESULTS: We study the connections
                 between bioinformatics resources and construct networks of
                 database and software usage patterns, based on resource
                 co-occurrence, that correspond to snapshots of common practice
                 in the bioinformatics community. We apply our approach to
                 pairings of phylogenetics software reported in the literature
                 and argue that these could provide a stepping stone into the
                 identification of scientific best practice. AVAILABILITY AND
                 IMPLEMENTATION: The extracted resource data, the scripts used
                 for network generation and the resulting networks are
                 available at http://bionerds.sourceforge.net/networks/.",
  journal     = "Bioinformatics",
  volume      =  30,
  number      =  17,
  pages       = "i601--8",
  month       =  sep,
  year        =  2014,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btu471",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "25161253",
  doi         = "10.1093/bioinformatics/btu471",
  pmc         = "PMC4147923"
}

@techreport{Jones2016-ta,
  title     = "{CodeMeta}: an exchange schema for software metadata. {KNB} Data
               Repository",
  author    = "Jones, Matthew B and Boettiger, Carl and Mayes, Abby Cabunoc and
               Smith, Arfon and Slaughter, Peter and Niemeyer, Kyle and Gil,
               Yolanda and Fenner, Martin and Nowak, Krzysztof and Hahnel, Mark
               and Coy, Luke and Allen, Alice and Crosas, Merc{\`e} and Sands,
               Ashley and Hong, Neil Chue and Cruse, Patricia and Katz, Dan and
               Goble, Carole",
  publisher = "KNB Data Repository",
  institution = "KNB Data Repository",
  year      =  2016,
  url       = "http://dx.doi.org/10.5063/schema/codemeta-1.0",
  doi       = "10.5063/schema/codemeta-1.0"
}

@ARTICLE{Boettiger2015-om,
  title     = "An Introduction to Docker for Reproducible Research",
  author    = "Boettiger, Carl",
  journal   = "ACM SIGOPS Operating Systems Review",
  publisher = "ACM",
  volume    =  49,
  number    =  1,
  pages     = "71--79",
  month     =  jan,
  year      =  2015,
  url       = "http://doi.acm.org/10.1145/2723872.2723882",
  address   = "New York, NY, USA",
  issn      = "0163-5980",
  doi       = "10.1145/2723872.2723882"
}

@article{Halioui2016-rr,
  title    = "Towards an ontology-based recommender system for relevant
              bioinformatics workflows",
  author   = "Halioui, Ahmed and Valtchev, Petko and Diallo, Abdoulaye Banire",
  abstract = "With the large and diverse type of biological data, bioinformatic
              solutions are being more complex and computationally intensive.
              New specialized data skills need to be acquired by researchers in
              order to follow this development. Workflow Management Systems
              rise as an efficient way to automate tasks through abstract
              models in order to assist users during their problem solving
              tasks. However, current solutions could have several problems in
              reusing the developed models for given tasks. The large amount of
              heterogenous data and the lack of knowledge in using
              bioinformatics tools could mislead the users during their
              analyses. To tackle this issue, we propose an ontology-based
              workflow-mining framework generating semantic models of
              bioinformatic best practices in order to assist scientists. To
              this end, concrete workflows are extracted from scientific
              articles and then mined using a rich domain ontology. Discovered
              frequent workflows would help to build a robust pattern-based
              recommender system using relevant concepts and relations of
              practices. In this study, we explore the specific topics of
              phylogenetic analyses. We annotated more than 300 recent articles
              using different ontological concepts and relations. Relative
              supports (frequencies) of discovered workflow components in texts
              show interesting results of relevant resources currently used in
              the different phylogenetic analysis steps. Mining concrete
              workflows from texts lead us to discover abstract but relevant
              patterns of the best combinations of tools, parameters and input
              data for specific phylogenetic problems. Extracted patterns would
              make workflows more intuitive and easy to be reused in similar
              situations. This could provide a stepping-stone into the
              identification of best practices and pave the road to a
              recommender system.",
  journal  = "bioRxiv",
  pages    = "082776",
  month    =  jan,
  year     =  2016,
  url      = "http://biorxiv.org/content/early/2016/10/24/082776.abstract",
  language = "en",
  doi      = "10.1101/082776"
}

@ARTICLE{Cerami2011-ex,
  title    = "{Pathway Commons, a web resource for biological pathway data}",
  author   = "Cerami, E G and Gross, B E and Demir, E and Rodchenkov, I and
              Babur, {\"O} and Anwar, N and Schultz, N and Bader, G D and
              Sander, C",
  journal  = "Nucleic acids research",
  volume   =  39,
  pages    = "685",
  year     =  2011,
  keywords = "charlie\_rcr.bib",
  issn     = "0305-1048"
}

@ARTICLE{Nust2017-ti,
  title     = "Opening the publication process with executable research
               compendia",
  author    = "N{\"u}st, Daniel and Konkol, Markus and Pebesma, Edzer and Kray,
               Christian and Schutzeichel, Marc and Przibytzin, Holger and
               Lorenz, J{\"o}rg",
  journal   = "D-Lib Magazine",
  publisher = "Corporation for National Research Initiatives",
  volume    =  23,
  number    = "1/2",
  year      =  2017,
  url       = "http://www.dlib.org/dlib/january17/nuest/01nuest.html"
}

@ARTICLE{Cuellar2003-ln,
  title     = "An Overview of {CellML} 1.1, a Biological Model Description
               Language",
  author    = "Cuellar, Autumn A and Lloyd, Catherine M and Nielsen, Poul F and
               Bullivant, David P and Nickerson, David P and Hunter, Peter J",
  abstract  = "CellML is an XML-based exchange format developed by the
               University of Auckland in collaboration with Physiome Sciences,
               Inc. CellML 1.1 has a component-based architecture allowing a
               modeller to build complex systems of models that expand and
               reuse previously published models. CellML Metadata is a format
               for encoding contextual information for a model. CellML 1.1 can
               be used in conjunction with CellML Metadata to provide a
               complete description of the structure and underlying mathematics
               of biological models. A repository of over 200
               electrophysiological, mechanical, signal transduction, and
               metabolic pathway models is available at www.cellml.org.",
  journal   = "Simulation",
  publisher = "SAGE Publications Ltd STM",
  volume    =  79,
  number    =  12,
  pages     = "740--747",
  month     =  dec,
  year      =  2003,
  url       = "https://doi.org/10.1177/0037549703040939",
  annote    = "doi: 10.1177/0037549703040939",
  issn      = "0037-5497",
  doi       = "10.1177/0037549703040939"
}

@ARTICLE{Prior2017-yy,
  title       = "The public cancer radiology imaging collections of The Cancer
                 Imaging Archive",
  author      = "Prior, Fred and Smith, Kirk and Sharma, Ashish and Kirby,
                 Justin and Tarbox, Lawrence and Clark, Ken and Bennett,
                 William and Nolan, Tracy and Freymann, John",
  affiliation = "University of Arkansas for Medical Sciences, Little Rock,
                 Arkansas 72205, USA. Emory University, Atlanta, Georgia 30322,
                 USA. Leidos Biomedical Research Inc. Frederick National
                 Laboratory for Cancer Research, Frederick, Maryland 20892,
                 USA. Washington University School of Medicine, St Louis,
                 Missouri 63110, USA.",
  abstract    = "The Cancer Imaging Archive (TCIA) is the U.S. National Cancer
                 Institute's repository for cancer imaging and related
                 information. TCIA contains 30.9 million radiology images
                 representing data collected from approximately 37,568
                 subjects. This data is organized into collections by
                 tumor-type with many collections also including analytic
                 results or clinical data. TCIA staff carefully de-identify and
                 curate all incoming collections prior to making the
                 information available via web browser or programmatic
                 interfaces. Each published collection within TCIA is assigned
                 a Digital Object Identifier that references the collection.
                 Additionally, researchers who use TCIA data may publish the
                 subset of information used in their analysis by requesting a
                 TCIA generated Digital Object Identifier. This data descriptor
                 is a review of a selected subset of existing publicly
                 available TCIA collections. It outlines the curation and
                 publication methods employed by TCIA and makes available 15
                 collections of cancer imaging data.",
  journal     = "Scientific data",
  volume      =  4,
  pages       = "170124",
  month       =  sep,
  year        =  2017,
  url         = "http://dx.doi.org/10.1038/sdata.2017.124",
  language    = "en",
  issn        = "2052-4463",
  pmid        = "28925987",
  doi         = "10.1038/sdata.2017.124",
  pmc         = "PMC5827108"
}

@ARTICLE{Peter2016-vi,
  title     = "Common Workflow Language, v1.0",
  author    = "Peter, Amstutz and Michael R., Crusoe and Neboj{\v s}a,
               Tijani{\'c} and Brad, Chapman and John, Chilton and Michael,
               Heuer and Andrey, Kartashov and Dan, Leehr and Herv{\'e},
               M{\'e}nager and Maya, Nedeljkovich and Matt, Scales and Stian,
               Soiland-Reyes and Luka, Stojanovic",
  abstract  = "The Common Workflow Language (CWL) is an informal, multi-vendor
               working group consisting of various organizations and
               individuals that have an interest in portability of data
               analysis workflows. Our goal is to create specifications that
               enable data scientists to describe analysis tools and workflows
               that are powerful, easy to use, portable, and support
               reproducibility.CWL builds on technologies such as JSON-LD and
               Avro for data modeling and Docker for portable runtime
               environments. CWL is designed to express workflows for
               data-intensive science, such as Bioinformatics, Medical Imaging,
               Chemistry, Physics, and Astronomy.This is v1.0 of the CWL tool
               and workflow specification, released on 2016-07-08.The
               specification, in HTML format, is in the draft-3/docs folder.",
  journal   = "figshare",
  publisher = "escholarship.org",
  month     =  jul,
  year      =  2016,
  url       = "http://www.escholarship.org/uc/item/25z538jj",
  keywords  = "prospectus III;in prospectus;should\_be\_in\_prospectus",
  doi       = "10.6084/m9.figshare.3115156.v2"
}

@ARTICLE{Chirico2003-mx,
  title     = "{ThermoML} an {XML-based} approach for storage and exchange of
               experimental and critically evaluated thermophysical and
               thermochemical property data. 2. Uncertainties",
  author    = "Chirico, Robert D and Frenkel, Michael and Diky, Vladimir V and
               Marsh, Kenneth N and Wilhoit, Randolph C",
  journal   = "Journal of chemical and engineering data",
  publisher = "ACS Publications",
  volume    =  48,
  number    =  5,
  pages     = "1344--1359",
  year      =  2003,
  url       = "https://pubs.acs.org/doi/abs/10.1021/je034088i?casa_token=3PEELRfBL4kAAAAA:J9cp6Xoe2hieWy7RtkNF-R8hha0k6gsQWme_EiN1ZPTHMjjFd2NFfuYQfBJnpOpypYbauxkQ6c0Z_Rw",
  issn      = "0021-9568"
}

@MISC{NICTA_Optimisation_Research_Group2014-iq,
  title     = "{NICTA-ORG/MLG} Seminar: C. Titus Brown - Openness and
               Reproducibility in Computational Science",
  author    = "{NICTA Optimisation Research Group}",
  abstract  = "Computational reproducibility should be relatively
               straightforward to achieve at the time of publication, but
               experience has shown that most research groups ...",
  publisher = "Youtube",
  month     =  dec,
  year      =  2014,
  url       = "https://www.youtube.com/watch?v=12hpAYr5ls0",
  keywords  = "Computational Science (Field Of Study); Seminar"
}

@INPROCEEDINGS{Lerner2014-zv,
  title     = "{RDataTracker}: collecting provenance in an interactive
               scripting environment",
  booktitle = "6th {$\{$USENIX$\}$} Workshop on the Theory and Practice of
               Provenance ({TaPP} 2014)",
  author    = "Lerner, Barbara and Boose, Emery",
  year      =  2014,
  url       = "https://www.usenix.org/system/files/conference/tapp2014/tapp14_paper_lerner.pdf"
}

@ARTICLE{ENCODE_Project_Consortium2007-yi,
  title    = "Identification and analysis of functional elements in 1\% of the
              human genome by the {ENCODE} pilot project",
  author   = "{ENCODE Project Consortium} and Birney, Ewan and
              Stamatoyannopoulos, John A and Dutta, Anindya and Guig{\'o},
              Roderic and Gingeras, Thomas R and Margulies, Elliott H and Weng,
              Zhiping and Snyder, Michael and Dermitzakis, Emmanouil T and
              Thurman, Robert E and Kuehn, Michael S and Taylor, Christopher M
              and Neph, Shane and Koch, Christoph M and Asthana, Saurabh and
              Malhotra, Ankit and Adzhubei, Ivan and Greenbaum, Jason A and
              Andrews, Robert M and Flicek, Paul and Boyle, Patrick J and Cao,
              Hua and Carter, Nigel P and Clelland, Gayle K and Davis, Sean and
              Day, Nathan and Dhami, Pawandeep and Dillon, Shane C and
              Dorschner, Michael O and Fiegler, Heike and Giresi, Paul G and
              Goldy, Jeff and Hawrylycz, Michael and Haydock, Andrew and
              Humbert, Richard and James, Keith D and Johnson, Brett E and
              Johnson, Ericka M and Frum, Tristan T and Rosenzweig, Elizabeth R
              and Karnani, Neerja and Lee, Kirsten and Lefebvre, Gregory C and
              Navas, Patrick A and Neri, Fidencio and Parker, Stephen C J and
              Sabo, Peter J and Sandstrom, Richard and Shafer, Anthony and
              Vetrie, David and Weaver, Molly and Wilcox, Sarah and Yu, Man and
              Collins, Francis S and Dekker, Job and Lieb, Jason D and Tullius,
              Thomas D and Crawford, Gregory E and Sunyaev, Shamil and Noble,
              William S and Dunham, Ian and Denoeud, France and Reymond,
              Alexandre and Kapranov, Philipp and Rozowsky, Joel and Zheng,
              Deyou and Castelo, Robert and Frankish, Adam and Harrow, Jennifer
              and Ghosh, Srinka and Sandelin, Albin and Hofacker, Ivo L and
              Baertsch, Robert and Keefe, Damian and Dike, Sujit and Cheng,
              Jill and Hirsch, Heather A and Sekinger, Edward A and Lagarde,
              Julien and Abril, Josep F and Shahab, Atif and Flamm, Christoph
              and Fried, Claudia and Hackerm{\"u}ller, J{\"o}rg and Hertel,
              Jana and Lindemeyer, Manja and Missal, Kristin and Tanzer, Andrea
              and Washietl, Stefan and Korbel, Jan and Emanuelsson, Olof and
              Pedersen, Jakob S and Holroyd, Nancy and Taylor, Ruth and
              Swarbreck, David and Matthews, Nicholas and Dickson, Mark C and
              Thomas, Daryl J and Weirauch, Matthew T and Gilbert, James and
              Drenkow, Jorg and Bell, Ian and Zhao, Xiaodong and Srinivasan, K
              G and Sung, Wing-Kin and Ooi, Hong Sain and Chiu, Kuo Ping and
              Foissac, Sylvain and Alioto, Tyler and Brent, Michael and
              Pachter, Lior and Tress, Michael L and Valencia, Alfonso and
              Choo, Siew Woh and Choo, Chiou Yu and Ucla, Catherine and
              Manzano, Caroline and Wyss, Carine and Cheung, Evelyn and Clark,
              Taane G and Brown, James B and Ganesh, Madhavan and Patel,
              Sandeep and Tammana, Hari and Chrast, Jacqueline and Henrichsen,
              Charlotte N and Kai, Chikatoshi and Kawai, Jun and Nagalakshmi,
              Ugrappa and Wu, Jiaqian and Lian, Zheng and Lian, Jin and
              Newburger, Peter and Zhang, Xueqing and Bickel, Peter and
              Mattick, John S and Carninci, Piero and Hayashizaki, Yoshihide
              and Weissman, Sherman and Hubbard, Tim and Myers, Richard M and
              Rogers, Jane and Stadler, Peter F and Lowe, Todd M and Wei,
              Chia-Lin and Ruan, Yijun and Struhl, Kevin and Gerstein, Mark and
              Antonarakis, Stylianos E and Fu, Yutao and Green, Eric D and
              Kara{\"o}z, Ula{\c s} and Siepel, Adam and Taylor, James and
              Liefer, Laura A and Wetterstrand, Kris A and Good, Peter J and
              Feingold, Elise A and Guyer, Mark S and Cooper, Gregory M and
              Asimenos, George and Dewey, Colin N and Hou, Minmei and Nikolaev,
              Sergey and Montoya-Burgos, Juan I and L{\"o}ytynoja, Ari and
              Whelan, Simon and Pardi, Fabio and Massingham, Tim and Huang,
              Haiyan and Zhang, Nancy R and Holmes, Ian and Mullikin, James C
              and Ureta-Vidal, Abel and Paten, Benedict and Seringhaus, Michael
              and Church, Deanna and Rosenbloom, Kate and Kent, W James and
              Stone, Eric A and {NISC Comparative Sequencing Program} and
              {Baylor College of Medicine Human Genome Sequencing Center} and
              {Washington University Genome Sequencing Center} and {Broad
              Institute} and {Children's Hospital Oakland Research Institute}
              and Batzoglou, Serafim and Goldman, Nick and Hardison, Ross C and
              Haussler, David and Miller, Webb and Sidow, Arend and Trinklein,
              Nathan D and Zhang, Zhengdong D and Barrera, Leah and Stuart,
              Rhona and King, David C and Ameur, Adam and Enroth, Stefan and
              Bieda, Mark C and Kim, Jonghwan and Bhinge, Akshay A and Jiang,
              Nan and Liu, Jun and Yao, Fei and Vega, Vinsensius B and Lee,
              Charlie W H and Ng, Patrick and Shahab, Atif and Yang, Annie and
              Moqtaderi, Zarmik and Zhu, Zhou and Xu, Xiaoqin and Squazzo,
              Sharon and Oberley, Matthew J and Inman, David and Singer,
              Michael A and Richmond, Todd A and Munn, Kyle J and
              Rada-Iglesias, Alvaro and Wallerman, Ola and Komorowski, Jan and
              Fowler, Joanna C and Couttet, Phillippe and Bruce, Alexander W
              and Dovey, Oliver M and Ellis, Peter D and Langford, Cordelia F
              and Nix, David A and Euskirchen, Ghia and Hartman, Stephen and
              Urban, Alexander E and Kraus, Peter and Van Calcar, Sara and
              Heintzman, Nate and Kim, Tae Hoon and Wang, Kun and Qu, Chunxu
              and Hon, Gary and Luna, Rosa and Glass, Christopher K and
              Rosenfeld, M Geoff and Aldred, Shelley Force and Cooper, Sara J
              and Halees, Anason and Lin, Jane M and Shulha, Hennady P and
              Zhang, Xiaoling and Xu, Mousheng and Haidar, Jaafar N S and Yu,
              Yong and Ruan, Yijun and Iyer, Vishwanath R and Green, Roland D
              and Wadelius, Claes and Farnham, Peggy J and Ren, Bing and Harte,
              Rachel A and Hinrichs, Angie S and Trumbower, Heather and
              Clawson, Hiram and Hillman-Jackson, Jennifer and Zweig, Ann S and
              Smith, Kayla and Thakkapallayil, Archana and Barber, Galt and
              Kuhn, Robert M and Karolchik, Donna and Armengol, Lluis and Bird,
              Christine P and de Bakker, Paul I W and Kern, Andrew D and
              Lopez-Bigas, Nuria and Martin, Joel D and Stranger, Barbara E and
              Woodroffe, Abigail and Davydov, Eugene and Dimas, Antigone and
              Eyras, Eduardo and Hallgr{\'\i}msd{\'o}ttir, Ingileif B and
              Huppert, Julian and Zody, Michael C and Abecasis, Gon{\c c}alo R
              and Estivill, Xavier and Bouffard, Gerard G and Guan, Xiaobin and
              Hansen, Nancy F and Idol, Jacquelyn R and Maduro, Valerie V B and
              Maskeri, Baishali and McDowell, Jennifer C and Park, Morgan and
              Thomas, Pamela J and Young, Alice C and Blakesley, Robert W and
              Muzny, Donna M and Sodergren, Erica and Wheeler, David A and
              Worley, Kim C and Jiang, Huaiyang and Weinstock, George M and
              Gibbs, Richard A and Graves, Tina and Fulton, Robert and Mardis,
              Elaine R and Wilson, Richard K and Clamp, Michele and Cuff, James
              and Gnerre, Sante and Jaffe, David B and Chang, Jean L and
              Lindblad-Toh, Kerstin and Lander, Eric S and Koriabine, Maxim and
              Nefedov, Mikhail and Osoegawa, Kazutoyo and Yoshinaga, Yuko and
              Zhu, Baoli and de Jong, Pieter J",
  abstract = "We report the generation and analysis of functional data from
              multiple, diverse experiments performed on a targeted 1\% of the
              human genome as part of the pilot phase of the ENCODE Project.
              These data have been further integrated and augmented by a number
              of evolutionary and computational analyses. Together, our results
              advance the collective knowledge about human genome function in
              several major areas. First, our studies provide convincing
              evidence that the genome is pervasively transcribed, such that
              the majority of its bases can be found in primary transcripts,
              including non-protein-coding transcripts, and those that
              extensively overlap one another. Second, systematic examination
              of transcriptional regulation has yielded new understanding about
              transcription start sites, including their relationship to
              specific regulatory sequences and features of chromatin
              accessibility and histone modification. Third, a more
              sophisticated view of chromatin structure has emerged, including
              its inter-relationship with DNA replication and transcriptional
              regulation. Finally, integration of these new sources of
              information, in particular with respect to mammalian evolution
              based on inter- and intra-species sequence comparisons, has
              yielded new mechanistic and evolutionary insights concerning the
              functional landscape of the human genome. Together, these studies
              are defining a path for pursuit of a more comprehensive
              characterization of human genome function.",
  journal  = "Nature",
  volume   =  447,
  number   =  7146,
  pages    = "799--816",
  month    =  jun,
  year     =  2007,
  url      = "http://dx.doi.org/10.1038/nature05874",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "17571346",
  doi      = "10.1038/nature05874",
  pmc      = "PMC2212820"
}

@ARTICLE{McPhillips2015-hu,
  title         = "{YesWorkflow}: A {User-Oriented}, {Language-Independent}
                   Tool for Recovering Workflow Information from Scripts",
  author        = "McPhillips, Timothy and Song, Tianhong and Kolisnik, Tyler
                   and Aulenbach, Steve and Belhajjame, Khalid and Bocinsky,
                   Kyle and Cao, Yang and Chirigati, Fernando and Dey, Saumen
                   and Freire, Juliana and Huntzinger, Deborah and Jones,
                   Christopher and Koop, David and Missier, Paolo and
                   Schildhauer, Mark and Schwalm, Christopher and Wei, Yaxing
                   and Cheney, James and Bieda, Mark and Ludaescher, Bertram",
  abstract      = "Scientific workflow management systems offer features for
                   composing complex computational pipelines from modular
                   building blocks, for executing the resulting automated
                   workflows, and for recording the provenance of data products
                   resulting from workflow runs. Despite the advantages such
                   features provide, many automated workflows continue to be
                   implemented and executed outside of scientific workflow
                   systems due to the convenience and familiarity of scripting
                   languages (such as Perl, Python, R, and MATLAB), and to the
                   high productivity many scientists experience when using
                   these languages. YesWorkflow is a set of software tools that
                   aim to provide such users of scripting languages with many
                   of the benefits of scientific workflow systems. YesWorkflow
                   requires neither the use of a workflow engine nor the
                   overhead of adapting code to run effectively in such a
                   system. Instead, YesWorkflow enables scientists to annotate
                   existing scripts with special comments that reveal the
                   computational modules and dataflows otherwise implicit in
                   these scripts. YesWorkflow tools extract and analyze these
                   comments, represent the scripts in terms of entities based
                   on the typical scientific workflow model, and provide
                   graphical renderings of this workflow-like view of the
                   scripts. Future versions of YesWorkflow also will allow the
                   prospective provenance of the data products of these scripts
                   to be queried in ways similar to those available to users of
                   scientific workflow systems.",
  month         =  feb,
  year          =  2015,
  url           = "http://arxiv.org/abs/1502.02403",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "1502.02403",
  primaryClass  = "cs.SE",
  arxivid       = "1502.02403"
}

@ARTICLE{Li2009-vr,
  title       = "The Sequence {Alignment/Map} format and {SAMtools}",
  author      = "Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim
                 and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis,
                 Goncalo and Durbin, Richard and {1000 Genome Project Data
                 Processing Subgroup}",
  affiliation = "Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus,
                 Cambridge, CB10 1SA, UK, Broad Institute of MIT and Harvard,
                 Cambridge, MA 02141, USA.",
  abstract    = "SUMMARY: The Sequence Alignment/Map (SAM) format is a generic
                 alignment format for storing read alignments against reference
                 sequences, supporting short and long reads (up to 128 Mbp)
                 produced by different sequencing platforms. It is flexible in
                 style, compact in size, efficient in random access and is the
                 format in which alignments from the 1000 Genomes Project are
                 released. SAMtools implements various utilities for
                 post-processing alignments in the SAM format, such as
                 indexing, variant caller and alignment viewer, and thus
                 provides universal tools for processing read alignments.
                 AVAILABILITY: http://samtools.sourceforge.net.",
  journal     = "Bioinformatics",
  volume      =  25,
  number      =  16,
  pages       = "2078--2079",
  month       =  aug,
  year        =  2009,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btp352",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "19505943",
  doi         = "10.1093/bioinformatics/btp352",
  pmc         = "PMC2723002"
}

@ARTICLE{Schimanski2018-dc,
  title    = "The evaluation of scholarship in academic promotion and tenure
              processes: Past, present, and future",
  author   = "Schimanski, Lesley A and Alperin, Juan Pablo",
  abstract = "Review, promotion, and tenure (RPT) processes significantly
              affect how faculty direct their own career and scholarly
              progression. Although RPT practices vary between and within
              institutions, and affect various disciplines, ranks, institution
              types, genders, and ethnicity in different ways, some consistent
              themes emerge when investigating what faculty would like to
              change about RPT. For instance, over the last few decades, RPT
              processes have generally increased the value placed on research,
              at the expense of teaching and service, which often results in an
              incongruity between how faculty actually spend their time vs.
              what is considered in their evaluation. Another issue relates to
              publication practices: most agree RPT requirements should
              encourage peer-reviewed works of high quality, but in practice,
              the value of publications is often assessed using shortcuts such
              as the prestige of the publication venue, rather than on the
              quality and rigor of peer review of each individual item. Open
              access and online publishing have made these issues even murkier
              due to misconceptions about peer review practices and concerns
              about predatory online publishers, which leaves traditional
              publishing formats the most desired despite their restricted
              circulation. And, efforts to replace journal-level measures such
              as the impact factor with more precise article-level metrics
              (e.g., citation counts and altmetrics) have been slow to
              integrate with the RPT process. Questions remain as to whether,
              or how, RPT practices should be changed to better reflect faculty
              work patterns and reduce pressure to publish in only the most
              prestigious traditional formats. To determine the most useful way
              to change RPT, we need to assess further the needs and
              perceptions of faculty and administrators, and gain a better
              understanding of the level of influence of written RPT guidelines
              and policy in an often vague process that is meant to allow for
              flexibility in assessing individuals.",
  journal  = "F1000Research",
  volume   =  7,
  pages    = "1605",
  month    =  oct,
  year     =  2018,
  url      = "http://dx.doi.org/10.12688/f1000research.16493.1",
  keywords = "academia; higher education; incentives; promotion; publishing;
              tenure",
  language = "en",
  issn     = "2046-1402",
  pmid     = "30647909",
  doi      = "10.12688/f1000research.16493.1",
  pmc      = "PMC6325612"
}

@ARTICLE{Mangul2019-yu,
  title       = "Systematic benchmarking of omics computational tools",
  author      = "Mangul, Serghei and Martin, Lana S and Hill, Brian L and Lam,
                 Angela Ka-Mei and Distler, Margaret G and Zelikovsky, Alex and
                 Eskin, Eleazar and Flint, Jonathan",
  affiliation = "Department of Computer Science, University of California Los
                 Angeles, 580 Portola Plaza, Los Angeles, CA, 90095, USA.
                 smangul@ucla.edu. Institute for Quantitative and Computational
                 Biosciences, University of California Los Angeles, 611 Charles
                 E Young Drive East, Los Angeles, CA, 90095, USA.
                 smangul@ucla.edu. Institute for Quantitative and Computational
                 Biosciences, University of California Los Angeles, 611 Charles
                 E Young Drive East, Los Angeles, CA, 90095, USA. Department of
                 Computer Science, University of California Los Angeles, 580
                 Portola Plaza, Los Angeles, CA, 90095, USA. Department of
                 Psychiatry and Biobehavioral Sciences, David Geffen School of
                 Medicine, University of California Los Angeles, Los Angeles,
                 CA, 90095, USA. Department of Computer Science, Georgia State
                 University, Atlanta, GA, 30303, USA. The Laboratory of
                 Bioinformatics, I.M. Sechenov First Moscow State Medical
                 University, Moscow, 119991, Russia. Department of Human
                 Genetics, University of California Los Angeles, 695 Charles E.
                 Young, Los Angeles, CA, USA.",
  abstract    = "Computational omics methods packaged as software have become
                 essential to modern biological research. The increasing
                 dependence of scientists on these powerful software tools
                 creates a need for systematic assessment of these methods,
                 known as benchmarking. Adopting a standardized benchmarking
                 practice could help researchers who use omics data to better
                 leverage recent technological innovations. Our review
                 summarizes benchmarking practices from 25 recent studies and
                 discusses the challenges, advantages, and limitations of
                 benchmarking across various domains of biology. We also
                 propose principles that can make computational biology
                 benchmarking studies more sustainable and reproducible,
                 ultimately increasing the transparency of biomedical data and
                 results.",
  journal     = "Nature communications",
  volume      =  10,
  number      =  1,
  pages       = "1393",
  month       =  mar,
  year        =  2019,
  url         = "http://dx.doi.org/10.1038/s41467-019-09406-4",
  language    = "en",
  issn        = "2041-1723",
  pmid        = "30918265",
  doi         = "10.1038/s41467-019-09406-4",
  pmc         = "PMC6437167"
}

@ARTICLE{Wattanakriengkrai2020-ou,
  title         = "{GitHub} Repositories with Links to Academic Papers: Open
                   Access, Traceability, and Evolution",
  author        = "Wattanakriengkrai, Supatsara and Chinthanet, Bodin and Hata,
                   Hideaki and Kula, Raula Gaikovina and Treude, Christoph and
                   Guo, Jin and Matsumoto, Kenichi",
  abstract      = "Traceability between published scientific breakthroughs and
                   their implementation is essential, especially in the case of
                   Open Source Software implements bleeding edge science into
                   its code. However, aligning the link between GitHub
                   repositories and academic papers can prove difficult, and
                   the link impact remains unknown. This paper investigates the
                   role of academic paper references contained in these
                   repositories. We conducted a large-scale study of 20
                   thousand GitHub repositories to establish prevalence of
                   references to academic papers. We use a mixed-methods
                   approach to identify Open Access (OA), traceability and
                   evolutionary aspects of the links. Although referencing a
                   paper is not typical, we find that a vast majority of
                   referenced academic papers are OA. In terms of traceability,
                   our analysis revealed that machine learning is the most
                   prevalent topic of repositories. These repositories tend to
                   be affiliated with academic communities. More than half of
                   the papers do not link back to any repository. A case study
                   of referenced arXiv paper shows that most of these papers
                   are high-impact and influential and do align with academia,
                   referenced by repositories written in different programming
                   languages. From the evolutionary aspect, we find very few
                   changes of papers being referenced and links to them.",
  month         =  apr,
  year          =  2020,
  url           = "http://arxiv.org/abs/2004.00199",
  archivePrefix = "arXiv",
  journal       = "arXiv",
  eprint        = "2004.00199",
  primaryClass  = "cs.SE",
  arxivid       = "2004.00199"
}

@ARTICLE{Stein2010-lo,
  title       = "The case for cloud computing in genome informatics",
  author      = "Stein, Lincoln D",
  affiliation = "Ontario Institute for Cancer Research, Toronto, ON M5G 0A3,
                 Canada. lincoln.stein@gmail.com",
  abstract    = "With DNA sequencing now getting cheaper more quickly than data
                 storage or computation, the time may have come for genome
                 informatics to migrate to the cloud.",
  journal     = "Genome biology",
  publisher   = "genomebiology.biomedcentral.com",
  volume      =  11,
  number      =  5,
  pages       = "207",
  month       =  may,
  year        =  2010,
  url         = "http://dx.doi.org/10.1186/gb-2010-11-5-207",
  language    = "en",
  issn        = "1465-6906",
  pmid        = "20441614",
  doi         = "10.1186/gb-2010-11-5-207",
  pmc         = "PMC2898083"
}

@INPROCEEDINGS{Wirth2000-by,
  title       = "{CRISP-DM}: Towards a standard process model for data mining",
  booktitle   = "Proceedings of the 4th international conference on the
                 practical applications of knowledge discovery and data mining",
  author      = "Wirth, R{\"u}diger and Hipp, Jochen",
  volume      =  1,
  institution = "Springer-Verlag London, UK",
  year        =  2000,
  url         = "http://www.cs.unibo.it/~danilo.montesi/CBD/Beatriz/10.1.1.198.5133.pdf"
}

@ARTICLE{Malone2014-uu,
  title       = "The Software Ontology ({SWO)}: a resource for reproducibility
                 in biomedical data analysis, curation and digital preservation",
  author      = "Malone, James and Brown, Andy and Lister, Allyson L and Ison,
                 Jon and Hull, Duncan and Parkinson, Helen and Stevens, Robert",
  affiliation = "EMBL-EBI, Wellcome Trust Genome Campus, Cambridge, CB10 1SD,
                 UK. School of Computer Science, University of Manchester,
                 Oxford Road, Manchester, M13 9PL, UK.",
  abstract    = "MOTIVATION: Biomedical ontologists to date have concentrated
                 on ontological descriptions of biomedical entities such as
                 gene products and their attributes, phenotypes and so on.
                 Recently, effort has diversified to descriptions of the
                 laboratory investigations by which these entities were
                 produced. However, much biological insight is gained from the
                 analysis of the data produced from these investigations, and
                 there is a lack of adequate descriptions of the wide range of
                 software that are central to bioinformatics. We need to
                 describe how data are analyzed for discovery, audit trails,
                 provenance and reproducibility. RESULTS: The Software Ontology
                 (SWO) is a description of software used to store, manage and
                 analyze data. Input to the SWO has come from beyond the life
                 sciences, but its main focus is the life sciences. We used
                 agile techniques to gather input for the SWO and keep
                 engagement with our users. The result is an ontology that
                 meets the needs of a broad range of users by describing
                 software, its information processing tasks, data inputs and
                 outputs, data formats versions and so on. Recently, the SWO
                 has incorporated EDAM, a vocabulary for describing data and
                 related concepts in bioinformatics. The SWO is currently being
                 used to describe software used in multiple biomedical
                 applications. CONCLUSION: The SWO is another element of the
                 biomedical ontology landscape that is necessary for the
                 description of biomedical entities and how they were
                 discovered. An ontology of software used to analyze data
                 produced by investigations in the life sciences can be made in
                 such a way that it covers the important features requested and
                 prioritized by its users. The SWO thus fits into the landscape
                 of biomedical ontologies and is produced using techniques
                 designed to keep it in line with user's needs. AVAILABILITY:
                 The Software Ontology is available under an Apache 2.0 license
                 at http://theswo.sourceforge.net/; the Software Ontology blog
                 can be read at http://softwareontology.wordpress.com.",
  journal     = "Journal of biomedical semantics",
  publisher   = "jbiomedsem.biomedcentral.com",
  volume      =  5,
  pages       = "25",
  month       =  jun,
  year        =  2014,
  url         = "http://dx.doi.org/10.1186/2041-1480-5-25",
  keywords    = "APIs \& Semantic ontologies",
  language    = "en",
  issn        = "2041-1480",
  pmid        = "25068035",
  doi         = "10.1186/2041-1480-5-25",
  pmc         = "PMC4098953"
}

@ARTICLE{Katz2020-ej,
  title     = "Recognizing the value of software: a software citation guide",
  author    = "Katz, Daniel S and Chue Hong, Neil P and Clark, Tim and Muench,
               August and Stall, Shelley and Bouquin, Daina and Cannon, Matthew
               and Edmunds, Scott and Faez, Telli and Feeney, Patricia and
               Fenner, Martin and Friedman, Michael and Grenier, Gerry and
               Harrison, Melissa and Heber, Joerg and Leary, Adam and
               MacCallum, Catriona and Murray, Hollydawn and Pastrana, Erika
               and Perry, Katherine and Schuster, Douglas and Stockhause,
               Martina and Yeston, Jake",
  abstract  = "Software is as integral as a research paper, monograph, or
               dataset in terms of facilitating the full understanding and
               dissemination of research. This article provides broadly
               applicable guidance on software citation for the communities and
               institutions publishing academic journals and conference
               proceedings. We expect those communities and institutions to
               produce versions of this document with software examples and
               citation styles that are appropriate for their intended
               audience. This article (and those community-specific versions)
               are aimed at authors citing software, including software
               developed by the authors or by others. We also include brief
               instructions on how software can be made citable, directing
               readers to more comprehensive guidance published elsewhere. The
               guidance presented in this article helps to support proper
               attribution and credit, reproducibility, collaboration and
               reuse, and encourages building on the work of others to further
               research.",
  journal   = "F1000Research",
  publisher = "F1000 Research Limited",
  volume    =  9,
  number    =  1257,
  pages     = "1257",
  month     =  oct,
  year      =  2020,
  url       = "https://f1000research.com/articles/9-1257/v2/pdf",
  keywords  = "Software citation, publishing, scholarly communication,
               guidelines, bibliometrics",
  language  = "en",
  doi       = "10.12688/f1000research.26932.2"
}

@ARTICLE{Dumbill2010-qu,
  title   = "{DOAP}: Description of a Project",
  author  = "Dumbill, Edd",
  journal = "URL http://trac. usefulinc. com/doap",
  year    =  2010
}

@INPROCEEDINGS{Stodden2018-ls,
  title     = "Enabling the Verification of Computational Results: An Empirical
               Evaluation of Computational Reproducibility",
  booktitle = "Proceedings of the First International Workshop on Practical
               Reproducible Evaluation of Computer Systems",
  author    = "Stodden, Victoria and Krafczyk, Matthew S and Bhaskar, Adhithya",
  publisher = "Association for Computing Machinery",
  number    = "Article 3",
  pages     = "1--5",
  series    = "P-RECS'18",
  month     =  jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3214239.3214242",
  address   = "New York, NY, USA",
  keywords  = "workflows, reproducibility policy, data access, code access,
               provenance, reproducible research",
  location  = "Tempe, AZ, USA",
  isbn      = "9781450358613",
  doi       = "10.1145/3214239.3214242"
}

@ARTICLE{Maciocci2019-vc,
  title   = "Introducing eLife's first computationally reproducible article",
  author  = "Maciocci, Giuliano and Aufreiter, Michael and Bentley, Nokome",
  journal = "eLife Labs [Internet]",
  volume  =  20,
  year    =  2019,
  url     = "https://elifesciences.org/labs/ad58f08d/introducing-elife-s-first-computationally-reproducible-article?utm_source=Nature+Briefing&utm_campaign=c5a0b228d4-briefing-dy-20190222&utm_medium=email&utm_term=0_c9dfd39373-c5a0b228d4-43623825"
}

@ARTICLE{Piccolo2016-kd,
  title    = "Tools and techniques for computational reproducibility",
  author   = "Piccolo, Stephen R and Frampton, Michael B",
  abstract = "When reporting research findings, scientists document the steps
              they followed so that others can verify and build upon the
              research. When those steps have been described in sufficient
              detail that others can retrace the steps and obtain similar
              results, the research is said to be reproducible. Computers play
              a vital role in many research disciplines and present both
              opportunities and challenges for reproducibility. Computers can
              be programmed to execute analysis tasks, and those programs can
              be repeated and shared with others. The deterministic nature of
              most computer programs means that the same analysis tasks,
              applied to the same data, will often produce the same outputs.
              However, in practice, computational findings often cannot be
              reproduced because of complexities in how software is packaged,
              installed, and executed---and because of limitations associated
              with how scientists document analysis steps. Many tools and
              techniques are available to help overcome these challenges; here
              we describe seven such strategies. With a broad scientific
              audience in mind, we describe the strengths and limitations of
              each approach, as well as the circumstances under which each
              might be applied. No single strategy is sufficient for every
              scenario; thus we emphasize that it is often useful to combine
              approaches.",
  journal  = "GigaScience",
  volume   =  5,
  number   =  1,
  pages    = "30",
  year     =  2016,
  url      = "http://dx.doi.org/10.1186/s13742-016-0135-4",
  issn     = "2047-217X",
  doi      = "10.1186/s13742-016-0135-4"
}

@ARTICLE{Eales2008-md,
  title       = "Methodology capture: discriminating between the ``best'' and
                 the rest of community practice",
  author      = "Eales, James M and Pinney, John W and Stevens, Robert D and
                 Robertson, David L",
  affiliation = "Faculty of Life Sciences, University of Manchester,
                 Manchester, UK. james.eales@postgrad.manchester.ac.uk",
  abstract    = "BACKGROUND: The methodologies we use both enable and help
                 define our research. However, as experimental complexity has
                 increased the choice of appropriate methodologies has become
                 an increasingly difficult task. This makes it difficult to
                 keep track of available bioinformatics software, let alone the
                 most suitable protocols in a specific research area. To remedy
                 this we present an approach for capturing methodology from
                 literature in order to identify and, thus, define best
                 practice within a field. RESULTS: Our approach is to implement
                 data extraction techniques on the full-text of scientific
                 articles to obtain the set of experimental protocols used by
                 an entire scientific discipline, molecular phylogenetics. Our
                 methodology for identifying methodologies could in principle
                 be applied to any scientific discipline, whether or not
                 computer-based. We find a number of issues related to the
                 nature of best practice, as opposed to community practice. We
                 find that there is much heterogeneity in the use of molecular
                 phylogenetic methods and software, some of which is related to
                 poor specification of protocols. We also find that
                 phylogenetic practice exhibits field-specific tendencies that
                 have increased through time, despite the generic nature of the
                 available software. We used the practice of highly published
                 and widely collaborative researchers (``expert'' researchers)
                 to analyse the influence of authority on community practice.
                 We find expert authors exhibit patterns of practice common to
                 their field and therefore act as useful field-specific
                 practice indicators. CONCLUSION: We have identified a
                 structured community of phylogenetic researchers performing
                 analyses that are customary in their own local community and
                 significantly different from those in other areas. Best
                 practice information can help to bridge such subtle
                 differences by increasing communication of protocols to a
                 wider audience. We propose that the practice of expert authors
                 from the field of evolutionary biology is the closest to
                 contemporary best practice in phylogenetic experimental
                 design. Capturing best practice is, however, a complex task
                 and should also acknowledge the differences between fields
                 such as the specific context of the analysis.",
  journal     = "BMC bioinformatics",
  volume      =  9,
  pages       = "359",
  month       =  sep,
  year        =  2008,
  url         = "http://dx.doi.org/10.1186/1471-2105-9-359",
  language    = "en",
  issn        = "1471-2105",
  pmid        = "18761740",
  doi         = "10.1186/1471-2105-9-359",
  pmc         = "PMC2553348"
}

@MISC{noauthor_undated-or,
  title        = "New Changes to Badging Terminology",
  abstract     = "New Changes to Badging Terminology",
  url          = "https://www.acm.org/publications/badging-terms",
  howpublished = "\url{https://www.acm.org/publications/badging-terms}",
  note         = "Accessed: 2021-6-17"
}

@MISC{Nust_undated-jg,
  title  = "Guerrilla Badges for Reproducible Geospatial Data Science ({AGILE}
            2019 Short Paper)",
  author = "N{\"u}st, Daniel and Lohoff, Lukas and Einfeldt, Lasse and Gavish,
            Nimrod and G{\"o}tza, Marlena and Jaswal, Shahzeib and Khalid,
            Salman and Meierkort, Laura and Mohr, Matthias and Rendel, Clara",
  url    = "http://dx.doi.org/10.31223/osf.io/xtsqh",
  doi    = "10.31223/osf.io/xtsqh"
}

@ARTICLE{Graham2005-rg,
  title       = "{DICOM} demystified: a review of digital file formats and
                 their use in radiological practice",
  author      = "Graham, R N J and Perriss, R W and Scarsbrook, A F",
  affiliation = "Department of Radiology, John Radcliffe Hospital, Headington,
                 Oxford, UK.",
  abstract    = "Digital imaging and communications in medicine (DICOM) is the
                 standard image file format used by radiological hardware
                 devices. This article will provide an overview of DICOM and
                 attempt to demystify the bewildering number of image formats
                 that are commonly encountered. The characteristics and
                 usefulness of different image file types will be explored and
                 a variety of freely available web-based resources to aid
                 viewing and manipulation of digital images will be reviewed.
                 How best to harness DICOM technology before the introduction
                 of picture archiving and communication systems (PACS) will
                 also be described.",
  journal     = "Clinical radiology",
  publisher   = "Elsevier",
  volume      =  60,
  number      =  11,
  pages       = "1133--1140",
  month       =  nov,
  year        =  2005,
  url         = "http://dx.doi.org/10.1016/j.crad.2005.07.003",
  language    = "en",
  issn        = "0009-9260",
  pmid        = "16223609",
  doi         = "10.1016/j.crad.2005.07.003"
}

@ARTICLE{Smith2018-ah,
  title     = "Journal of Open Source Software ({JOSS)}: design and first-year
               review",
  author    = "Smith, Arfon M and Niemeyer, Kyle E and Katz, Daniel S and
               Barba, Lorena A and Githinji, George and Gymrek, Melissa and
               Huff, Kathryn D and Madan, Christopher R and Mayes, Abigail
               Cabunoc and Moerman, Kevin M and Prins, Pjotr and Ram, Karthik
               and Rokem, Ariel and Teal, Tracy K and Guimera, Roman Valls and
               Vanderplas, Jacob T",
  abstract  = "This article describes the motivation, design, and progress of
               the Journal of Open Source Software (JOSS). JOSS is a free and
               open-access journal that publishes articles describing research
               software. It has the dual goals of improving the quality of the
               software submitted and providing a mechanism for research
               software developers to receive credit. While designed to work
               within the current merit system of science, JOSS addresses the
               dearth of rewards for key contributions to science made in the
               form of software. JOSS publishes articles that encapsulate
               scholarship contained in the software itself, and its rigorous
               peer review targets the software components: functionality,
               documentation, tests, continuous integration, and the license. A
               JOSS article contains an abstract describing the purpose and
               functionality of the software, references, and a link to the
               software archive. The article is the entry point of a JOSS
               submission, which encompasses the full set of software
               artifacts. Submission and review proceed in the open, on GitHub.
               Editors, reviewers, and authors work collaboratively and openly.
               Unlike other journals, JOSS does not reject articles requiring
               major revision; while not yet accepted, articles remain visible
               and under review until the authors make adequate changes (or
               withdraw, if unable to meet requirements). Once an article is
               accepted, JOSS gives it a digital object identifier (DOI),
               deposits its metadata in Crossref, and the article can begin
               collecting citations on indexers like Google Scholar and other
               services. Authors retain copyright of their JOSS article,
               releasing it under a Creative Commons Attribution 4.0
               International License. In its first year, starting in May 2016,
               JOSS published 111 articles, with more than 40 additional
               articles under review. JOSS is a sponsored project of the
               nonprofit organization NumFOCUS and is an affiliate of the Open
               Source Initiative (OSI).",
  journal   = "PeerJ Computer Science",
  publisher = "PeerJ Inc.",
  volume    =  4,
  pages     = "e147",
  month     =  feb,
  year      =  2018,
  url       = "https://peerj.com/articles/cs-147",
  keywords  = "Research software; Code review; Computational research; Software
               citation; Open-source software; Scholarly publishing",
  language  = "en",
  issn      = "2376-5992",
  doi       = "10.7717/peerj-cs.147"
}

@ARTICLE{Needleman2001-aq,
  title     = "{ONIX} (Online Information Exchange)",
  author    = "Needleman, Mark H",
  journal   = "Serials Review",
  publisher = "Routledge",
  volume    =  27,
  number    = "3-4",
  pages     = "102--104",
  month     =  dec,
  year      =  2001,
  url       = "https://www.tandfonline.com/doi/abs/10.1080/00987913.2001.10764686",
  annote    = "doi: 10.1080/00987913.2001.10764686",
  issn      = "0098-7913",
  doi       = "10.1080/00987913.2001.10764686"
}

@ARTICLE{McDonald2000-lz,
  title    = "{Issues in the Representation of Real Texts: The Design of Krisp}",
  author   = "McDonald, D D",
  journal  = "Natural Language Processing and Knowledge Representation",
  pages    = "77",
  year     =  2000,
  keywords = "charlie\_rcr.bib"
}

@ARTICLE{Anzt2021-iz,
  title     = "An environment for sustainable research software in Germany and
               beyond: current state, open challenges, and call for action",
  author    = "Anzt, Hartwig and Bach, Felix and Druskat, Stephan and
               L{\"o}ffler, Frank and Loewe, Axel and Renard, Bernhard Y and
               Seemann, Gunnar and Struck, Alexander and Achhammer, Elke and
               Aggarwal, Piush and Appel, Franziska and Bader, Michael and
               Brusch, Lutz and Busse, Christian and Chourdakis, Gerasimos and
               Dabrowski, Piotr Wojciech and Ebert, Peter and Flemisch, Bernd
               and Friedl, Sven and Fritzsch, Bernadette and Funk, Maximilian D
               and Gast, Volker and Goth, Florian and Grad, Jean-No{\"e}l and
               Hegewald, Jan and Hermann, Sibylle and Hohmann, Florian and
               Janosch, Stephan and Kutra, Dominik and Linxweiler, Jan and
               Muth, Thilo and Peters-Kottig, Wolfgang and Rack, Fabian and
               Raters, Fabian H C and Rave, Stephan and Reina, Guido and
               Rei{\ss}ig, Malte and Ropinski, Timo and Schaarschmidt, Joerg
               and Seibold, Heidi and Thiele, Jan P and Uekermann, Benjamin and
               Unger, Stefan and Weeber, Rudolf",
  abstract  = "Research software has become a central asset in academic
               research. It optimizes existing and enables new research
               methods, implements and embeds research knowledge, and
               constitutes an essential research product in itself. Research
               software must be sustainable in order to understand, replicate,
               reproduce, and build upon existing research or conduct new
               research effectively. In other words, software must be
               available, discoverable, usable, and adaptable to new needs,
               both now and in the future. Research software therefore requires
               an environment that supports sustainability. Hence, a change is
               needed in the way research software development and maintenance
               are currently motivated, incentivized, funded, structurally and
               infrastructurally supported, and legally treated. Failing to do
               so will threaten the quality and validity of research. In this
               paper, we identify challenges for research software
               sustainability in Germany and beyond, in terms of motivation,
               selection, research software engineering personnel, funding,
               infrastructure, and legal aspects. Besides researchers, we
               specifically address political and academic decision-makers to
               increase awareness of the importance and needs of sustainable
               research software practices. In particular, we recommend
               strategies and measures to create an environment for sustainable
               research software, with the ultimate goal to ensure that
               software-driven research is valid, reproducible and sustainable,
               and that software is recognized as a first class citizen in
               research. This paper is the outcome of two workshops run in
               Germany in 2019, at deRSE19 - the first International Conference
               of Research Software Engineers in Germany - and a dedicated
               DFG-supported follow-up workshop in Berlin.",
  journal   = "F1000Research",
  publisher = "F1000 Research Limited",
  volume    =  9,
  number    =  295,
  pages     = "295",
  month     =  jan,
  year      =  2021,
  url       = "https://f1000research.com/articles/9-295/v2/pdf",
  keywords  = "Sustainable Software Development, Academic Software, Software
               Infrastructure, Software Training, Software Licensing, Research
               Software",
  language  = "en",
  doi       = "10.12688/f1000research.23224.2"
}

@ARTICLE{Margolis2014-fi,
  title       = "The National Institutes of Health's Big Data to Knowledge
                 ({BD2K}) initiative: capitalizing on biomedical big data",
  author      = "Margolis, Ronald and Derr, Leslie and Dunn, Michelle and
                 Huerta, Michael and Larkin, Jennie and Sheehan, Jerry and
                 Guyer, Mark and Green, Eric D",
  affiliation = "National Institute of Diabetes and Digestive and Kidney
                 Diseases, NIH, Bethesda, Maryland, USA. Office of the
                 Director, NIH, Bethesda, Maryland, USA. National Cancer
                 Institute, NIH, Bethesda, Maryland, USA. National Library of
                 Medicine, NIH, Bethesda, Maryland, USA. National Heart, Lung
                 and Blood Institute, NIH, Bethesda, Maryland, USA. National
                 Human Genome Research Institute, NIH, Bethesda, Maryland, USA.",
  abstract    = "Biomedical research has and will continue to generate large
                 amounts of data (termed 'big data') in many formats and at all
                 levels. Consequently, there is an increasing need to better
                 understand and mine the data to further knowledge and foster
                 new discovery. The National Institutes of Health (NIH) has
                 initiated a Big Data to Knowledge (BD2K) initiative to
                 maximize the use of biomedical big data. BD2K seeks to better
                 define how to extract value from the data, both for the
                 individual investigator and the overall research community,
                 create the analytic tools needed to enhance utility of the
                 data, provide the next generation of trained personnel, and
                 develop data science concepts and tools that can be made
                 available to all stakeholders.",
  journal     = "Journal of the American Medical Informatics Association: JAMIA",
  volume      =  21,
  number      =  6,
  pages       = "957--958",
  month       =  nov,
  year        =  2014,
  url         = "http://dx.doi.org/10.1136/amiajnl-2014-002974",
  keywords    = "BD2K; NIH; biomedical big data",
  language    = "en",
  issn        = "1067-5027, 1527-974X",
  pmid        = "25008006",
  doi         = "10.1136/amiajnl-2014-002974",
  pmc         = "PMC4215061"
}

@MISC{noauthor_undated-ur,
  title       = "{DUO}",
  abstract    = "Ontology for consent codes and data use requirements -
                 EBISPOT/DUO",
  institution = "Github",
  url         = "https://github.com/EBISPOT/DUO"
}

@ARTICLE{Clark2014-ag,
  title    = "Micropublications: a semantic model for claims, evidence,
              arguments and annotations in biomedical communications",
  author   = "Clark, Tim and Ciccarese, Paolo N and Goble, Carole A",
  abstract = "Scientific publications are documentary representations of
              defeasible arguments, supported by data and repeatable methods.
              They are the essential mediating artifacts in the ecosystem of
              scientific communications. The institutional ``goal'' of science
              is publishing results. The linear document publication format,
              dating from 1665, has survived transition to the Web.",
  journal  = "Journal of biomedical semantics",
  volume   =  5,
  number   =  1,
  pages    = "28",
  month    =  jul,
  year     =  2014,
  url      = "https://doi.org/10.1186/2041-1480-5-28",
  keywords = "APIs \& Semantic ontologies",
  issn     = "2041-1480",
  doi      = "10.1186/2041-1480-5-28"
}

@ARTICLE{Slenter2018-vy,
  title    = "{{WikiPathways}: a multifaceted pathway database bridging
              metabolomics to other omics research.}",
  author   = "Slenter, D N and Kutmon, M and Hanspers, K and Riutta, A and
              Windsor, J and Nunes, N and {Others}",
  journal  = "Nucleic acids research",
  volume   =  46,
  pages    = "D661",
  year     =  2018,
  keywords = "charlie\_rcr.bib",
  issn     = "0305-1048"
}

@ARTICLE{Ison2013-gm,
  title       = "{EDAM}: an ontology of bioinformatics operations, types of
                 data and identifiers, topics and formats",
  author      = "Ison, Jon and Kalas, Mat{\'u}s and Jonassen, Inge and Bolser,
                 Dan and Uludag, Mahmut and McWilliam, Hamish and Malone, James
                 and Lopez, Rodrigo and Pettifer, Steve and Rice, Peter",
  affiliation = "EMBL European Bioinformatics Institute, Hinxton, Cambridge
                 CB10 1SD, UK. jison@ebi.ac.uk",
  abstract    = "MOTIVATION: Advancing the search, publication and integration
                 of bioinformatics tools and resources demands consistent
                 machine-understandable descriptions. A comprehensive ontology
                 allowing such descriptions is therefore required. RESULTS:
                 EDAM is an ontology of bioinformatics operations (tool or
                 workflow functions), types of data and identifiers,
                 application domains and data formats. EDAM supports semantic
                 annotation of diverse entities such as Web services,
                 databases, programmatic libraries, standalone tools,
                 interactive applications, data schemas, datasets and
                 publications within bioinformatics. EDAM applies to organizing
                 and finding suitable tools and data and to automating their
                 integration into complex applications or workflows. It
                 includes over 2200 defined concepts and has successfully been
                 used for annotations and implementations. AVAILABILITY: The
                 latest stable version of EDAM is available in OWL format from
                 http://edamontology.org/EDAM.owl and in OBO format from
                 http://edamontology.org/EDAM.obo. It can be viewed online at
                 the NCBO BioPortal and the EBI Ontology Lookup Service. For
                 documentation and license please refer to
                 http://edamontology.org. This article describes version 1.2
                 available at http://edamontology.org/EDAM\_1.2.owl. CONTACT:
                 jison@ebi.ac.uk.",
  journal     = "Bioinformatics",
  publisher   = "academic.oup.com",
  volume      =  29,
  number      =  10,
  pages       = "1325--1332",
  month       =  may,
  year        =  2013,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btt113",
  keywords    = "prospectus III;in prospectus;APIs \& Semantic ontologies",
  language    = "en",
  issn        = "1367-4803, 1367-4811",
  pmid        = "23479348",
  doi         = "10.1093/bioinformatics/btt113",
  pmc         = "PMC3654706"
}

@ARTICLE{Hoyt2019-wr,
  title    = "{Re-curation and rational enrichment of knowledge graphs in
              Biological Expression Language}",
  author   = "Hoyt, C T and Domingo-Fern{\'a}ndez, D and Aldisi, R and Xu, L
              and Kolpeja, K and Spalek, S and Wollert, E and Bachman, J and
              Gyori, B M and Greene, P and Hofmann-Apitius, M",
  journal  = "Database",
  volume   =  2019,
  year     =  2019,
  keywords = "charlie\_rcr.bib",
  issn     = "0162-4105"
}

@ARTICLE{Qin2016-tj,
  title    = "Metadata and Reproducibility: A Case Study of Gravitational Wave
              Research Data Management",
  author   = "Qin, Jian and Dobreski, Brian and Brown, Duncan",
  abstract = "The complexity of computationally-intensive scientific research
              poses great challenges for both research data management and
              research reproducibility. What metadata needs to be captured for
              tracking, reproducing, and reusing computational results is the
              starting point in developing metadata models to fulfil these
              functions of data management. This paper reports the findings
              from interviews with gravitational wave (GW) researchers, which
              were designed to gather user requirements to develop a metadata
              model. Motivations for keeping documentation of data and analysis
              results include trust, accountability and continuity of work.
              Research reproducibility relies on metadata that represents code
              dependencies and versions and has good documentation for
              verification. Metadata specific to GW data, workflows and outputs
              tend to differ from those currently available in metadata
              standards. The paper also discusses the challenges in
              representing code dependencies and workflows. .",
  journal  = "International Journal of Digital Curation",
  volume   =  11,
  number   =  1,
  pages    = "218--231",
  month    =  oct,
  year     =  2016,
  url      = "http://ijdc.net/index.php/ijdc/article/view/399",
  language = "en",
  issn     = "1746-8256, 1746-8256",
  doi      = "10.2218/ijdc.v11i1.399"
}

@ARTICLE{Simonyan2017-st,
  title       = "Biocompute {Objects-A} Step towards Evaluation and Validation
                 of Biomedical Scientific Computations",
  author      = "Simonyan, Vahan and Goecks, Jeremy and Mazumder, Raja",
  affiliation = "Center for Biologics Evaluation and Research, Food and Drug
                 Administration, Silver Spring, MD, USA;
                 vahan.simonyan@fda.hhs.gov jgoecks@gwu.edu mazumder@gwu.edu.
                 Computational Biology Institute, George Washington University,
                 Ashburn, VA, USA; and vahan.simonyan@fda.hhs.gov
                 jgoecks@gwu.edu mazumder@gwu.edu. Department of Biochemistry
                 and Molecular Medicine, George Washington University,
                 Washington, DC, USA vahan.simonyan@fda.hhs.gov jgoecks@gwu.edu
                 mazumder@gwu.edu.",
  abstract    = "The unpredictability of actual physical, chemical, and
                 biological experiments due to the multitude of environmental
                 and procedural factors is well documented. What is
                 systematically overlooked, however, is that computational
                 biology algorithms are also affected by multiplicity of
                 parameters and have no lesser volatility. The complexities of
                 computation protocols and interpretation of outcomes is only a
                 part of the challenge: There are also virtually no
                 standardized and industry-accepted metadata schemas for
                 reporting the computational objects that record the parameters
                 used for computations together with the results of
                 computations. Thus, it is often impossible to reproduce the
                 results of a previously performed computation due to missing
                 information on parameters, versions, arguments, conditions,
                 and procedures of application launch. In this article we
                 describe the concept of biocompute objects developed
                 specifically to satisfy regulatory research needs for
                 evaluation, validation, and verification of bioinformatics
                 pipelines. We envision generalized versions of biocompute
                 objects called biocompute templates that support a single
                 class of analyses but can be adapted to meet unique needs. To
                 make these templates widely usable, we outline a simple but
                 powerful cross-platform implementation. We also discuss the
                 reasoning and potential usability for such concept within the
                 larger scientific community through the creation of a
                 biocompute object database initially consisting of records
                 relevant to the U.S. Food and Drug Administration. A
                 biocompute object database record will be similar to a GenBank
                 record in form; the difference being that instead of
                 describing a sequence, the biocompute record will include
                 information related to parameters, dependencies, usage, and
                 other information related to specific computational instance.
                 This mechanism will extend similar efforts and also serve as a
                 collaborative ground to ensure interoperability between
                 different platforms, industries, scientists, regulators, and
                 other stakeholders interested in biocomputing.",
  journal     = "PDA journal of pharmaceutical science and technology / PDA",
  volume      =  71,
  number      =  2,
  pages       = "136--146",
  month       =  mar,
  year        =  2017,
  url         = "http://dx.doi.org/10.5731/pdajpst.2016.006734",
  keywords    = "Biocompute object; Computation reproducibility; FDA; NGS
                 standardization; Regulatory research",
  language    = "en",
  issn        = "1079-7440, 1948-2124",
  pmid        = "27974626",
  doi         = "10.5731/pdajpst.2016.006734",
  pmc         = "PMC5510742"
}

@ARTICLE{LeVeque2012-xx,
  title   = "Reproducible research for scientific computing: Tools and
             strategies for changing the culture",
  author  = "LeVeque, Randall J and Mitchell, Ian M and Stodden, Victoria",
  journal = "Computing in Science and Engineering",
  volume  =  14,
  number  =  4,
  pages   = "13",
  year    =  2012,
  url     = "http://scitation.aip.org/content/aip/journal/cise/14/4/10.1109/MCSE.2012.38?crawler=true"
}
